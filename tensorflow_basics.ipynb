{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c309969",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80a6c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Basic ANN\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if logs['accuracy'] >0.90:\n",
    "            print(\"Accuracy greater than 90%. Stopping Training.\")\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "def val_dnn_model(epochs, X_train, Y_train, X_val, Y_val, callbacks=None):\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f2324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 15ms/step - loss: 1.4392 - accuracy: 0.6114 - val_loss: 0.7750 - val_accuracy: 0.6233\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 1.1206 - accuracy: 0.6157 - val_loss: 0.6447 - val_accuracy: 0.7033\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8553 - accuracy: 0.6257 - val_loss: 0.6501 - val_accuracy: 0.6900\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7674 - accuracy: 0.6686 - val_loss: 0.7009 - val_accuracy: 0.7067\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7329 - accuracy: 0.6914 - val_loss: 0.6520 - val_accuracy: 0.7067\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6660 - accuracy: 0.6971 - val_loss: 0.5500 - val_accuracy: 0.7133\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6634 - accuracy: 0.7043 - val_loss: 0.5326 - val_accuracy: 0.7267\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6450 - accuracy: 0.7129 - val_loss: 0.5418 - val_accuracy: 0.7400\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6165 - accuracy: 0.7000 - val_loss: 0.5371 - val_accuracy: 0.7433\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6055 - accuracy: 0.7186 - val_loss: 0.5474 - val_accuracy: 0.7400\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6062 - accuracy: 0.7371 - val_loss: 0.5392 - val_accuracy: 0.7400\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6180 - accuracy: 0.7071 - val_loss: 0.5260 - val_accuracy: 0.7433\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6034 - accuracy: 0.7200 - val_loss: 0.5164 - val_accuracy: 0.7600\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5607 - accuracy: 0.7200 - val_loss: 0.5201 - val_accuracy: 0.7567\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5718 - accuracy: 0.7343 - val_loss: 0.5166 - val_accuracy: 0.7467\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5430 - accuracy: 0.7271 - val_loss: 0.5070 - val_accuracy: 0.7567\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5696 - accuracy: 0.7200 - val_loss: 0.5098 - val_accuracy: 0.7633\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5631 - accuracy: 0.7229 - val_loss: 0.5047 - val_accuracy: 0.7700\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5531 - accuracy: 0.7514 - val_loss: 0.5150 - val_accuracy: 0.7700\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5388 - accuracy: 0.7400 - val_loss: 0.5281 - val_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5621 - accuracy: 0.7371 - val_loss: 0.5080 - val_accuracy: 0.7600\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5140 - accuracy: 0.7571 - val_loss: 0.5162 - val_accuracy: 0.7567\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5327 - accuracy: 0.7300 - val_loss: 0.5128 - val_accuracy: 0.7500\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5317 - accuracy: 0.7386 - val_loss: 0.5103 - val_accuracy: 0.7567\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5244 - accuracy: 0.7557 - val_loss: 0.5045 - val_accuracy: 0.7467\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5090 - accuracy: 0.7614 - val_loss: 0.5027 - val_accuracy: 0.7567\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7629 - val_loss: 0.5076 - val_accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5386 - accuracy: 0.7500 - val_loss: 0.5084 - val_accuracy: 0.7633\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5311 - accuracy: 0.7386 - val_loss: 0.5080 - val_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5170 - accuracy: 0.7500 - val_loss: 0.5034 - val_accuracy: 0.7467\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5123 - accuracy: 0.7643 - val_loss: 0.5002 - val_accuracy: 0.7600\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5178 - accuracy: 0.7586 - val_loss: 0.4937 - val_accuracy: 0.7600\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4829 - accuracy: 0.7771 - val_loss: 0.4932 - val_accuracy: 0.7767\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4962 - accuracy: 0.7657 - val_loss: 0.4841 - val_accuracy: 0.7767\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5112 - accuracy: 0.7529 - val_loss: 0.4925 - val_accuracy: 0.7667\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4965 - accuracy: 0.7614 - val_loss: 0.4956 - val_accuracy: 0.7700\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5147 - accuracy: 0.7614 - val_loss: 0.5170 - val_accuracy: 0.7433\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5018 - accuracy: 0.7557 - val_loss: 0.4951 - val_accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4988 - accuracy: 0.7600 - val_loss: 0.4952 - val_accuracy: 0.7767\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5125 - accuracy: 0.7629 - val_loss: 0.4929 - val_accuracy: 0.7700\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4973 - accuracy: 0.7643 - val_loss: 0.5019 - val_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4959 - accuracy: 0.7657 - val_loss: 0.4953 - val_accuracy: 0.7767\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4985 - accuracy: 0.7514 - val_loss: 0.4994 - val_accuracy: 0.7733\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4775 - accuracy: 0.7743 - val_loss: 0.4933 - val_accuracy: 0.7733\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4774 - accuracy: 0.7771 - val_loss: 0.4879 - val_accuracy: 0.7800\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4756 - accuracy: 0.7814 - val_loss: 0.4895 - val_accuracy: 0.7867\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4733 - accuracy: 0.7843 - val_loss: 0.4879 - val_accuracy: 0.7800\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4704 - accuracy: 0.7986 - val_loss: 0.4933 - val_accuracy: 0.7933\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4884 - accuracy: 0.7671 - val_loss: 0.4971 - val_accuracy: 0.7700\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4669 - accuracy: 0.7700 - val_loss: 0.5045 - val_accuracy: 0.7433\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4759 - accuracy: 0.7814 - val_loss: 0.4930 - val_accuracy: 0.7733\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4819 - accuracy: 0.7729 - val_loss: 0.4969 - val_accuracy: 0.7667\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4856 - accuracy: 0.7771 - val_loss: 0.5038 - val_accuracy: 0.7700\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4755 - accuracy: 0.7786 - val_loss: 0.4993 - val_accuracy: 0.7767\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4727 - accuracy: 0.7743 - val_loss: 0.4932 - val_accuracy: 0.7733\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4714 - accuracy: 0.7729 - val_loss: 0.4924 - val_accuracy: 0.7700\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4762 - accuracy: 0.7771 - val_loss: 0.4886 - val_accuracy: 0.7767\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4688 - accuracy: 0.7871 - val_loss: 0.4980 - val_accuracy: 0.7733\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4617 - accuracy: 0.7943 - val_loss: 0.4944 - val_accuracy: 0.7867\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4764 - accuracy: 0.7843 - val_loss: 0.4901 - val_accuracy: 0.7700\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4545 - accuracy: 0.7986 - val_loss: 0.4895 - val_accuracy: 0.7800\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4538 - accuracy: 0.7943 - val_loss: 0.4936 - val_accuracy: 0.7667\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4496 - accuracy: 0.8014 - val_loss: 0.4911 - val_accuracy: 0.7833\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4285 - accuracy: 0.7929 - val_loss: 0.4946 - val_accuracy: 0.7767\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4500 - accuracy: 0.7800 - val_loss: 0.4944 - val_accuracy: 0.7567\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4362 - accuracy: 0.8000 - val_loss: 0.4925 - val_accuracy: 0.7833\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4386 - accuracy: 0.8086 - val_loss: 0.4906 - val_accuracy: 0.7833\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4480 - accuracy: 0.8000 - val_loss: 0.4908 - val_accuracy: 0.7633\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4337 - accuracy: 0.8029 - val_loss: 0.4994 - val_accuracy: 0.7800\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4320 - accuracy: 0.8100 - val_loss: 0.4890 - val_accuracy: 0.7767\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4308 - accuracy: 0.8100 - val_loss: 0.4900 - val_accuracy: 0.7700\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4490 - accuracy: 0.7843 - val_loss: 0.4934 - val_accuracy: 0.7667\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4401 - accuracy: 0.8000 - val_loss: 0.4965 - val_accuracy: 0.7600\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4360 - accuracy: 0.7914 - val_loss: 0.4915 - val_accuracy: 0.7700\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4238 - accuracy: 0.8157 - val_loss: 0.5013 - val_accuracy: 0.7933\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4361 - accuracy: 0.7957 - val_loss: 0.5054 - val_accuracy: 0.7767\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4349 - accuracy: 0.8157 - val_loss: 0.5010 - val_accuracy: 0.7833\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4184 - accuracy: 0.8057 - val_loss: 0.4960 - val_accuracy: 0.7667\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4292 - accuracy: 0.8057 - val_loss: 0.4958 - val_accuracy: 0.7700\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4254 - accuracy: 0.8143 - val_loss: 0.4926 - val_accuracy: 0.7967\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4165 - accuracy: 0.8100 - val_loss: 0.4930 - val_accuracy: 0.7767\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4202 - accuracy: 0.8057 - val_loss: 0.5021 - val_accuracy: 0.7767\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4198 - accuracy: 0.8171 - val_loss: 0.5082 - val_accuracy: 0.7567\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4296 - accuracy: 0.8057 - val_loss: 0.5010 - val_accuracy: 0.7633\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4299 - accuracy: 0.7871 - val_loss: 0.4905 - val_accuracy: 0.7700\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4287 - accuracy: 0.8157 - val_loss: 0.5199 - val_accuracy: 0.7900\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4313 - accuracy: 0.8143 - val_loss: 0.5003 - val_accuracy: 0.7767\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4376 - accuracy: 0.8029 - val_loss: 0.4983 - val_accuracy: 0.7767\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4232 - accuracy: 0.8114 - val_loss: 0.5006 - val_accuracy: 0.7767\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4028 - accuracy: 0.8086 - val_loss: 0.4963 - val_accuracy: 0.7667\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4128 - accuracy: 0.8129 - val_loss: 0.4987 - val_accuracy: 0.7767\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4316 - accuracy: 0.7971 - val_loss: 0.4888 - val_accuracy: 0.7933\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4338 - accuracy: 0.8100 - val_loss: 0.4997 - val_accuracy: 0.7833\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4052 - accuracy: 0.8171 - val_loss: 0.5043 - val_accuracy: 0.7767\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4024 - accuracy: 0.8200 - val_loss: 0.5006 - val_accuracy: 0.7800\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4149 - accuracy: 0.8157 - val_loss: 0.5057 - val_accuracy: 0.7833\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4215 - accuracy: 0.8014 - val_loss: 0.5060 - val_accuracy: 0.7700\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4150 - accuracy: 0.8229 - val_loss: 0.5052 - val_accuracy: 0.7633\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4180 - accuracy: 0.8057 - val_loss: 0.5054 - val_accuracy: 0.7767\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4100 - accuracy: 0.8200 - val_loss: 0.5086 - val_accuracy: 0.7733\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8429\n",
      "Training Set:   [0.3707040846347809, 0.8428571224212646]\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5086 - accuracy: 0.7733\n",
      "Validation Set: [0.5085697174072266, 0.7733333110809326]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = tfds.load('german_credit_numeric', split=['train'], batch_size=-1, as_supervised=True)\n",
    "X=tfds.as_numpy(train[0][0])\n",
    "Y=tfds.as_numpy(train[0][1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "hist_regularized, model_regularized = val_dnn_model(100, X_train, Y_train, X_test, Y_test, callbacks=[EarlyStoppingCallback()])\n",
    "print(f\"Training Set:   {model_regularized.evaluate(X_train, Y_train)}\")\n",
    "print(f\"Validation Set: {model_regularized.evaluate(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75d585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "def vtoc_regression_model(norm, model_type):\n",
    "    if model_type=='linear':\n",
    "        model = Sequential()\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_regression_data(dataset, target):\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset.isna().sum()\n",
    "    dataset=dataset.dropna()\n",
    "    #dataset['Origin'] = dataset['Origin'].map({1.0: 'USA', 2.0: 'Europe', 3.0: 'Japan'})\n",
    "    Y=dataset[target]\n",
    "    X=dataset.loc[:, dataset.columns != target]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "    horsepower = np.array(X_train['Horsepower']).astype('float32')\n",
    "\n",
    "    horsepower_normalizer = Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "    \n",
    "    linear_model=vtoc_regression_model(horsepower_normalizer, model_type='linear')\n",
    "    history = linear_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=2)\n",
    "    test_results = linear_model.predict(X_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7480131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 - 0s - loss: 231.0550 - val_loss: 290.7013 - 349ms/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 164.8466 - val_loss: 26.5685 - 119ms/epoch - 13ms/step\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 89.5644 - val_loss: 49.4564 - 100ms/epoch - 11ms/step\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 32.7282 - val_loss: 28.5107 - 103ms/epoch - 11ms/step\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 22.5977 - val_loss: 8.0420 - 79ms/epoch - 9ms/step\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 8.5712 - val_loss: 13.5126 - 81ms/epoch - 9ms/step\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 16.3905 - val_loss: 9.2972 - 67ms/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 35.3781 - val_loss: 11.4659 - 80ms/epoch - 9ms/step\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 14.0517 - val_loss: 18.2754 - 77ms/epoch - 9ms/step\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 16.9186 - val_loss: 32.8455 - 84ms/epoch - 9ms/step\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 11.7090 - val_loss: 9.9018 - 142ms/epoch - 16ms/step\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 10.5167 - val_loss: 43.4549 - 136ms/epoch - 15ms/step\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 35.9725 - val_loss: 43.8688 - 70ms/epoch - 8ms/step\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 52.5922 - val_loss: 91.5081 - 68ms/epoch - 8ms/step\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 59.7495 - val_loss: 25.4189 - 71ms/epoch - 8ms/step\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 50.3082 - val_loss: 13.7712 - 67ms/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 9.6793 - val_loss: 11.8424 - 62ms/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 8.4244 - val_loss: 6.2219 - 58ms/epoch - 6ms/step\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 14.3290 - val_loss: 47.5983 - 59ms/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 47.1582 - val_loss: 60.4549 - 58ms/epoch - 6ms/step\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 59.8589 - val_loss: 36.6870 - 65ms/epoch - 7ms/step\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 57.8566 - val_loss: 34.4942 - 65ms/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 39.9671 - val_loss: 20.7316 - 60ms/epoch - 7ms/step\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 35.9354 - val_loss: 63.6661 - 59ms/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 56.7610 - val_loss: 10.4825 - 55ms/epoch - 6ms/step\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 17.5762 - val_loss: 18.2828 - 60ms/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 18.8054 - val_loss: 28.4866 - 60ms/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 19.3754 - val_loss: 13.4766 - 59ms/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 19.5100 - val_loss: 15.7004 - 59ms/epoch - 7ms/step\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 18.3483 - val_loss: 11.0659 - 59ms/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 19.2071 - val_loss: 16.9530 - 63ms/epoch - 7ms/step\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 24.2225 - val_loss: 30.0491 - 58ms/epoch - 6ms/step\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 34.4608 - val_loss: 3.1661 - 71ms/epoch - 8ms/step\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 8.5103 - val_loss: 17.0906 - 68ms/epoch - 8ms/step\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 29.9465 - val_loss: 67.7897 - 67ms/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 55.8791 - val_loss: 11.8340 - 70ms/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 17.6040 - val_loss: 16.0264 - 75ms/epoch - 8ms/step\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 18.4623 - val_loss: 29.1145 - 67ms/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 20.3347 - val_loss: 38.3329 - 87ms/epoch - 10ms/step\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 49.4726 - val_loss: 114.7535 - 76ms/epoch - 8ms/step\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 91.8438 - val_loss: 75.4737 - 71ms/epoch - 8ms/step\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 42.0845 - val_loss: 36.1096 - 73ms/epoch - 8ms/step\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 36.6106 - val_loss: 19.0706 - 70ms/epoch - 8ms/step\n",
      "Epoch 44/100\n",
      "9/9 - 0s - loss: 28.2720 - val_loss: 4.2670 - 73ms/epoch - 8ms/step\n",
      "Epoch 45/100\n",
      "9/9 - 0s - loss: 17.9850 - val_loss: 43.4785 - 67ms/epoch - 7ms/step\n",
      "Epoch 46/100\n",
      "9/9 - 0s - loss: 37.2763 - val_loss: 30.2308 - 60ms/epoch - 7ms/step\n",
      "Epoch 47/100\n",
      "9/9 - 0s - loss: 36.0434 - val_loss: 14.8247 - 74ms/epoch - 8ms/step\n",
      "Epoch 48/100\n",
      "9/9 - 0s - loss: 32.7237 - val_loss: 23.0098 - 68ms/epoch - 8ms/step\n",
      "Epoch 49/100\n",
      "9/9 - 0s - loss: 34.7945 - val_loss: 17.1758 - 72ms/epoch - 8ms/step\n",
      "Epoch 50/100\n",
      "9/9 - 0s - loss: 18.3651 - val_loss: 21.8865 - 71ms/epoch - 8ms/step\n",
      "Epoch 51/100\n",
      "9/9 - 0s - loss: 17.2057 - val_loss: 17.7007 - 79ms/epoch - 9ms/step\n",
      "Epoch 52/100\n",
      "9/9 - 0s - loss: 18.9188 - val_loss: 5.7622 - 76ms/epoch - 8ms/step\n",
      "Epoch 53/100\n",
      "9/9 - 0s - loss: 15.0085 - val_loss: 31.2784 - 73ms/epoch - 8ms/step\n",
      "Epoch 54/100\n",
      "9/9 - 0s - loss: 17.7970 - val_loss: 30.5717 - 75ms/epoch - 8ms/step\n",
      "Epoch 55/100\n",
      "9/9 - 0s - loss: 20.1554 - val_loss: 30.6217 - 70ms/epoch - 8ms/step\n",
      "Epoch 56/100\n",
      "9/9 - 0s - loss: 19.6728 - val_loss: 25.1946 - 68ms/epoch - 8ms/step\n",
      "Epoch 57/100\n",
      "9/9 - 0s - loss: 21.2872 - val_loss: 67.1470 - 66ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "9/9 - 0s - loss: 23.7287 - val_loss: 20.5163 - 61ms/epoch - 7ms/step\n",
      "Epoch 59/100\n",
      "9/9 - 0s - loss: 19.4008 - val_loss: 9.2367 - 71ms/epoch - 8ms/step\n",
      "Epoch 60/100\n",
      "9/9 - 0s - loss: 24.7478 - val_loss: 51.1190 - 58ms/epoch - 6ms/step\n",
      "Epoch 61/100\n",
      "9/9 - 0s - loss: 21.2939 - val_loss: 3.1549 - 57ms/epoch - 6ms/step\n",
      "Epoch 62/100\n",
      "9/9 - 0s - loss: 10.3272 - val_loss: 46.9247 - 53ms/epoch - 6ms/step\n",
      "Epoch 63/100\n",
      "9/9 - 0s - loss: 36.5932 - val_loss: 52.1956 - 55ms/epoch - 6ms/step\n",
      "Epoch 64/100\n",
      "9/9 - 0s - loss: 47.7434 - val_loss: 102.8872 - 62ms/epoch - 7ms/step\n",
      "Epoch 65/100\n",
      "9/9 - 0s - loss: 44.3543 - val_loss: 4.7107 - 58ms/epoch - 6ms/step\n",
      "Epoch 66/100\n",
      "9/9 - 0s - loss: 29.5823 - val_loss: 16.4534 - 59ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "9/9 - 0s - loss: 32.4985 - val_loss: 28.8167 - 66ms/epoch - 7ms/step\n",
      "Epoch 68/100\n",
      "9/9 - 0s - loss: 33.7966 - val_loss: 32.8490 - 70ms/epoch - 8ms/step\n",
      "Epoch 69/100\n",
      "9/9 - 0s - loss: 31.6030 - val_loss: 9.2226 - 87ms/epoch - 10ms/step\n",
      "Epoch 70/100\n",
      "9/9 - 0s - loss: 17.6679 - val_loss: 21.2345 - 76ms/epoch - 8ms/step\n",
      "Epoch 71/100\n",
      "9/9 - 0s - loss: 18.8371 - val_loss: 49.8182 - 59ms/epoch - 7ms/step\n",
      "Epoch 72/100\n",
      "9/9 - 0s - loss: 37.4482 - val_loss: 38.2868 - 69ms/epoch - 8ms/step\n",
      "Epoch 73/100\n",
      "9/9 - 0s - loss: 36.8873 - val_loss: 17.8581 - 68ms/epoch - 8ms/step\n",
      "Epoch 74/100\n",
      "9/9 - 0s - loss: 27.7476 - val_loss: 49.6260 - 70ms/epoch - 8ms/step\n",
      "Epoch 75/100\n",
      "9/9 - 0s - loss: 35.7549 - val_loss: 40.4830 - 71ms/epoch - 8ms/step\n",
      "Epoch 76/100\n",
      "9/9 - 0s - loss: 26.9945 - val_loss: 54.6800 - 68ms/epoch - 8ms/step\n",
      "Epoch 77/100\n",
      "9/9 - 0s - loss: 34.7609 - val_loss: 41.9603 - 64ms/epoch - 7ms/step\n",
      "Epoch 78/100\n",
      "9/9 - 0s - loss: 49.8772 - val_loss: 38.5500 - 67ms/epoch - 7ms/step\n",
      "Epoch 79/100\n",
      "9/9 - 0s - loss: 50.0076 - val_loss: 60.0998 - 62ms/epoch - 7ms/step\n",
      "Epoch 80/100\n",
      "9/9 - 0s - loss: 55.3288 - val_loss: 83.0530 - 75ms/epoch - 8ms/step\n",
      "Epoch 81/100\n",
      "9/9 - 0s - loss: 56.8725 - val_loss: 94.6463 - 123ms/epoch - 14ms/step\n",
      "Epoch 82/100\n",
      "9/9 - 0s - loss: 54.1468 - val_loss: 29.2230 - 71ms/epoch - 8ms/step\n",
      "Epoch 83/100\n",
      "9/9 - 0s - loss: 50.8579 - val_loss: 58.0240 - 64ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "9/9 - 0s - loss: 64.8713 - val_loss: 45.5247 - 58ms/epoch - 6ms/step\n",
      "Epoch 85/100\n",
      "9/9 - 0s - loss: 48.6255 - val_loss: 97.5019 - 58ms/epoch - 6ms/step\n",
      "Epoch 86/100\n",
      "9/9 - 0s - loss: 55.0300 - val_loss: 34.4709 - 58ms/epoch - 6ms/step\n",
      "Epoch 87/100\n",
      "9/9 - 0s - loss: 54.3597 - val_loss: 41.7874 - 71ms/epoch - 8ms/step\n",
      "Epoch 88/100\n",
      "9/9 - 0s - loss: 57.3993 - val_loss: 31.8896 - 59ms/epoch - 7ms/step\n",
      "Epoch 89/100\n",
      "9/9 - 0s - loss: 32.6342 - val_loss: 36.6163 - 57ms/epoch - 6ms/step\n",
      "Epoch 90/100\n",
      "9/9 - 0s - loss: 32.3820 - val_loss: 51.7041 - 57ms/epoch - 6ms/step\n",
      "Epoch 91/100\n",
      "9/9 - 0s - loss: 18.7867 - val_loss: 37.6886 - 55ms/epoch - 6ms/step\n",
      "Epoch 92/100\n",
      "9/9 - 0s - loss: 37.7455 - val_loss: 3.1943 - 58ms/epoch - 6ms/step\n",
      "Epoch 93/100\n",
      "9/9 - 0s - loss: 15.5021 - val_loss: 35.7002 - 70ms/epoch - 8ms/step\n",
      "Epoch 94/100\n",
      "9/9 - 0s - loss: 33.9451 - val_loss: 44.7141 - 72ms/epoch - 8ms/step\n",
      "Epoch 95/100\n",
      "9/9 - 0s - loss: 38.5440 - val_loss: 17.0859 - 57ms/epoch - 6ms/step\n",
      "Epoch 96/100\n",
      "9/9 - 0s - loss: 34.6955 - val_loss: 2.6448 - 72ms/epoch - 8ms/step\n",
      "Epoch 97/100\n",
      "9/9 - 0s - loss: 19.3012 - val_loss: 4.6197 - 55ms/epoch - 6ms/step\n",
      "Epoch 98/100\n",
      "9/9 - 0s - loss: 19.8220 - val_loss: 25.7089 - 54ms/epoch - 6ms/step\n",
      "Epoch 99/100\n",
      "9/9 - 0s - loss: 33.4909 - val_loss: 21.9347 - 55ms/epoch - 6ms/step\n",
      "Epoch 100/100\n",
      "9/9 - 0s - loss: 29.5453 - val_loss: 39.8524 - 58ms/epoch - 6ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.14522398],\n",
       "       [ -9.3601055 ],\n",
       "       [-29.113241  ],\n",
       "       [ -8.973978  ],\n",
       "       [-41.372612  ],\n",
       "       [-15.517832  ],\n",
       "       [-34.509792  ],\n",
       "       [  2.6275582 ],\n",
       "       [ -4.1425376 ],\n",
       "       [ -8.209433  ],\n",
       "       [-46.33242   ],\n",
       "       [-13.186731  ],\n",
       "       [-46.559914  ],\n",
       "       [-14.423948  ],\n",
       "       [-11.954599  ],\n",
       "       [ -6.548349  ],\n",
       "       [-20.98937   ],\n",
       "       [-43.63129   ],\n",
       "       [-43.856377  ],\n",
       "       [ -3.3977714 ],\n",
       "       [-40.002396  ],\n",
       "       [-38.01322   ],\n",
       "       [-13.034712  ],\n",
       "       [  5.133372  ],\n",
       "       [-57.685776  ],\n",
       "       [ -1.5658737 ],\n",
       "       [-34.57497   ],\n",
       "       [-12.189146  ],\n",
       "       [-29.044123  ],\n",
       "       [ -8.435602  ],\n",
       "       [ -5.9060683 ],\n",
       "       [-39.161034  ],\n",
       "       [  6.5103173 ],\n",
       "       [  3.2081475 ],\n",
       "       [-32.611668  ],\n",
       "       [  3.7958436 ],\n",
       "       [ 10.678658  ],\n",
       "       [ -1.5126663 ],\n",
       "       [-24.098246  ],\n",
       "       [-28.153078  ],\n",
       "       [-10.764357  ],\n",
       "       [-17.608385  ],\n",
       "       [-39.674133  ],\n",
       "       [-23.9512    ],\n",
       "       [  1.5416664 ],\n",
       "       [-21.733248  ],\n",
       "       [-19.90724   ],\n",
       "       [-53.103874  ],\n",
       "       [-16.30805   ],\n",
       "       [  0.44710386],\n",
       "       [  4.7703843 ],\n",
       "       [-29.641771  ],\n",
       "       [ -4.957956  ],\n",
       "       [-62.437706  ],\n",
       "       [-44.249187  ],\n",
       "       [ 10.274672  ],\n",
       "       [-53.97055   ],\n",
       "       [-27.23535   ],\n",
       "       [-20.673437  ],\n",
       "       [  8.1887    ],\n",
       "       [-58.843735  ],\n",
       "       [ -8.035505  ],\n",
       "       [-23.762934  ],\n",
       "       [-21.656034  ],\n",
       "       [-18.579794  ],\n",
       "       [ -0.30193865],\n",
       "       [-29.164145  ],\n",
       "       [ -2.6411338 ],\n",
       "       [-39.754353  ],\n",
       "       [-25.030016  ],\n",
       "       [  4.1873956 ],\n",
       "       [ -0.43158114],\n",
       "       [-18.349173  ],\n",
       "       [-48.129128  ],\n",
       "       [-15.291765  ],\n",
       "       [-11.119848  ],\n",
       "       [-22.607473  ],\n",
       "       [-30.895334  ],\n",
       "       [  6.461619  ],\n",
       "       [ -7.1851707 ],\n",
       "       [-21.714388  ],\n",
       "       [-15.534365  ],\n",
       "       [  0.11648405],\n",
       "       [-23.479406  ],\n",
       "       [-18.912592  ],\n",
       "       [  2.7277932 ],\n",
       "       [-28.445875  ],\n",
       "       [  0.58064497],\n",
       "       [-39.565384  ],\n",
       "       [  8.498163  ],\n",
       "       [-25.76406   ],\n",
       "       [-39.3816    ],\n",
       "       [-25.396112  ],\n",
       "       [  9.164114  ],\n",
       "       [ -5.7145514 ],\n",
       "       [-19.669909  ],\n",
       "       [-35.892937  ],\n",
       "       [-15.418055  ],\n",
       "       [ -2.28301   ],\n",
       "       [ -3.6840148 ],\n",
       "       [-16.858072  ],\n",
       "       [-41.179844  ],\n",
       "       [  7.5403047 ],\n",
       "       [-11.092619  ],\n",
       "       [ -4.1008277 ],\n",
       "       [-14.176073  ],\n",
       "       [  7.090861  ],\n",
       "       [-40.387028  ],\n",
       "       [  8.564764  ],\n",
       "       [-32.780968  ],\n",
       "       [ -0.93877566],\n",
       "       [  7.516318  ],\n",
       "       [-11.508898  ],\n",
       "       [  0.60615766],\n",
       "       [-24.61508   ],\n",
       "       [  9.656111  ],\n",
       "       [ -4.8267264 ],\n",
       "       [  3.267119  ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "eval_regression_data(raw_dataset, 'MPG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5370c9",
   "metadata": {},
   "source": [
    "# Build Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "100bd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Neural Network\n",
    "import tensorflow as tf\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "    xs = tf.cast(xs, tf.float32) / 255.0\n",
    "    ys = tf.cast(ys, tf.float32)\n",
    "    ys = tf.one_hot(ys, depth=n_classes)\n",
    "    return tf.data.Dataset.from_tensor_slices((xs, ys)).map(preprocess).shuffle(len(ys)).batch(128)\n",
    "\n",
    "def val_nn(training_inputs_data, training_outputs_data, test_inputs):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    training_inputs = tensorflow.compat.v1.placeholder(shape=[None, 3], dtype=tensorflow.float32)  \n",
    "    training_outputs = tensorflow.compat.v1.placeholder(shape=[None, 1], dtype=tensorflow.float32) #Desired outputs for each input  \n",
    "    weights = tensorflow.Variable(initial_value=[[.3], [.1], [.8]], dtype=tensorflow.float32)  \n",
    "    bias = tensorflow.Variable(initial_value=[[1]], dtype=tensorflow.float32)  \n",
    "\n",
    "    af_input = tensorflow.matmul(training_inputs, weights) + bias  \n",
    "  \n",
    "    # Activation function of the output layer neuron  \n",
    "    predictions = tensorflow.nn.sigmoid(af_input)  \n",
    "    # Measuring the prediction error of the network after being trained  \n",
    "    prediction_error = tensorflow.reduce_sum(training_outputs - predictions)  \n",
    "    # Minimizing the prediction error using gradient descent optimizer  \n",
    "    \n",
    "    train_op = tensorflow.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(prediction_error) \n",
    "    # Creating a TensorFlow Session  \n",
    "    sess = tensorflow.compat.v1.Session()  \n",
    "    # Initializing the TensorFlow Variables (weights and bias)  \n",
    "    sess.run(tensorflow.compat.v1.global_variables_initializer())  \n",
    "    \n",
    "    # Training loop of the neural network  \n",
    "    for step in range(10000):  \n",
    "        sess.run(fetches=train_op, feed_dict={training_inputs: training_inputs_data, training_outputs: training_outputs_data})  \n",
    "        # Class scores of some testing data  \n",
    "    score= sess.run(fetches=predictions, feed_dict={training_inputs: [[248, 80, 68], [0, 0, 255]]})\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predictions =  sess.run(fetches=test_inputs)\n",
    "    # Closing the TensorFlow Session to free resources  \n",
    "    sess.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "213165e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2., 13.],\n",
       "       [ 7.,  9.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "X_test=tensorflow.convert_to_tensor(value=X_test, dtype=tensorflow.float32)\n",
    "val_nn(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa10de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop Neural Network\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = 1.0 / (1.0 + exp(-activation))\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            transfer_derivative = neuron['output'] * (1.0 - neuron['output'])\n",
    "            neuron['delta'] = errors[j] * transfer_derivative\n",
    "\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    "\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10c141a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': 0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': -0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': 0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': -0.03803132596437354}]\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836, 2.550537003, 0],\n",
    "           [1.465489372, 2.362125076, 0],\n",
    "           [3.396561688, 4.400293529, 0],\n",
    "           [1.38807019, 1.850220317, 0],\n",
    "           [3.06407232, 3.005305973, 0],\n",
    "           [7.627531214, 2.759262235, 1],\n",
    "           [5.332441248, 2.088626775, 1],\n",
    "           [6.922596716, 1.77106367, 1],\n",
    "           [8.675418651, -0.242068655, 1],\n",
    "           [7.673756466, 3.508563011, 1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "           [{'weights': [0.2550690257394217, 0.49543508709194095]},\n",
    "            {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b263d-5053-4a06-a25b-1270d80d7ef4",
   "metadata": {},
   "source": [
    "# Build CNN network using Pure Python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bff641f-fae4-4b59-a056-76ab2ba6ebb5",
   "metadata": {},
   "source": [
    "The following is an uncompleted project. I used ChatGPT to write a code to create and train a CNN neural network in just Python, without using Tensorflow or PyTorch. This was an experiment to see how capable the coding algorithm was. It did not solve the backward pass updating for the convolution. I'm putting this project on hold and will return to it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8231196-a91b-42bf-99c1-e80d0c312272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 32, 32)\n",
      "(64, 32, 32, 32)\n",
      "(64, 32, 32, 32)\n",
      "(64, 16, 16, 32)\n",
      "(64, 16, 16, 64)\n",
      "(64, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train.astype('float32')# / 255.0\n",
    "X_test = X_test.astype('float32')# / 255.0\n",
    "\n",
    "# Flatten labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train.flatten())\n",
    "y_test = label_encoder.transform(y_test.flatten())\n",
    "\n",
    "class SimpleCNNCifar:\n",
    "    def __init__(self):\n",
    "        self.weights = {\n",
    "            'conv1': np.random.randn(3, 3, 3, 32) / (3 * 3 * 3),\n",
    "            'batch_norm1_gamma': np.ones((1, 1, 1, 32)),\n",
    "            'batch_norm1_beta': np.zeros((1, 1, 1, 32)),\n",
    "            'conv2': np.random.randn(3, 3, 32, 32) / (3 * 3 * 32),\n",
    "            'batch_norm2_gamma': np.ones((1, 1, 1, 32)),\n",
    "            'batch_norm2_beta': np.zeros((1, 1, 1, 32)),\n",
    "            'conv3': np.random.randn(3, 3, 32, 64) / (3 * 3 * 32),\n",
    "            'batch_norm3_gamma': np.ones((1, 1, 1, 64)),\n",
    "            'batch_norm3_beta': np.zeros((1, 1, 1, 64)),\n",
    "            'conv4': np.random.randn(3, 3, 64, 64) / (3 * 3 * 64),\n",
    "            'batch_norm4_gamma': np.ones((1, 1, 1, 64)),\n",
    "            'batch_norm4_beta': np.zeros((1, 1, 1, 64)),\n",
    "            'conv5': np.random.randn(3, 3, 64, 128) / (3 * 3 * 64),\n",
    "            'batch_norm5_gamma': np.ones((1, 1, 1, 128)),\n",
    "            'batch_norm5_beta': np.zeros((1, 1, 1, 128)),\n",
    "            'conv6': np.random.randn(3, 3, 128, 128) / (3 * 3 * 128),\n",
    "            'batch_norm6_gamma': np.ones((1, 1, 1, 128)),\n",
    "            'batch_norm6_beta': np.zeros((1, 1, 1, 128)),\n",
    "            'dense1': np.random.randn(2048, 10) / 2048,\n",
    "            'dense1_bias': np.zeros((1, 10)),\n",
    "            'fc': np.random.randn(2048, 10) / 2048,\n",
    "            'fc_bias': np.zeros((1, 10)),\n",
    "        }\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def forward_pass(self, images):\n",
    "        conv1 = self.convolution(images, self.weights['conv1'])\n",
    "        print(conv1.shape)\n",
    "        activation1 = self.relu_activation(conv1)\n",
    "        print(activation1.shape)\n",
    "        batch_norm1 = self.batch_normalization(activation1, self.weights['batch_norm1_gamma'], self.weights['batch_norm1_beta'])\n",
    "        print(batch_norm1.shape)\n",
    "        conv2 = self.convolution(activation1, self.weights['conv2'])\n",
    "        \n",
    "        activation2 = self.relu_activation(conv2)\n",
    "        batch_norm2 = self.batch_normalization(activation2, self.weights['batch_norm2_gamma'], self.weights['batch_norm2_beta'])\n",
    "        pool1 = self.max_pooling(batch_norm2)\n",
    "        print(pool1.shape)\n",
    "        conv3 = self.convolution(pool1, self.weights['conv3'])\n",
    "        batch_norm3 = self.batch_normalization(conv3, self.weights['batch_norm3_gamma'], self.weights['batch_norm3_beta'])\n",
    "        activation3 = self.relu_activation(batch_norm3)\n",
    "        print(activation3.shape)\n",
    "        conv4 = self.convolution(activation3, self.weights['conv4'])\n",
    "        batch_norm4 = self.batch_normalization(conv4, self.weights['batch_norm4_gamma'], self.weights['batch_norm4_beta'])\n",
    "        activation4 = self.relu_activation(batch_norm4)\n",
    "        pool2 = self.max_pooling(activation4)\n",
    "        print(pool2.shape)\n",
    "        conv5 = self.convolution(pool2, self.weights['conv5'])\n",
    "        batch_norm5 = self.batch_normalization(conv5, self.weights['batch_norm5_gamma'], self.weights['batch_norm5_beta'])\n",
    "        activation5 = self.relu_activation(batch_norm5)\n",
    "\n",
    "        conv6 = self.convolution(activation5, self.weights['conv6'])\n",
    "        batch_norm6 = self.batch_normalization(conv6, self.weights['batch_norm6_gamma'], self.weights['batch_norm6_beta'])\n",
    "        activation6 = self.relu_activation(batch_norm6)\n",
    "        pool3 = self.max_pooling(activation6)\n",
    "        print(pool3.shape)\n",
    "        flatten = pool3.reshape((pool3.shape[0], -1))\n",
    "        dense1 = np.dot(flatten, self.weights['dense1']) + self.weights['dense1_bias']\n",
    "        print(dense1.shape)\n",
    "        return dense1\n",
    "\n",
    "    def batch_normalization(self, x, gamma, beta):\n",
    "        mean = np.mean(x, axis=(0, 1, 2), keepdims=True)\n",
    "        variance = np.var(x, axis=(0, 1, 2), keepdims=True)\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        return gamma * x_normalized + beta\n",
    "\n",
    "\n",
    "    def max_pooling(self, image, pool_size=(2, 2)):\n",
    "        return np.max(image.reshape((image.shape[0], image.shape[1] // pool_size[0], pool_size[0], image.shape[2] // pool_size[1], pool_size[1], image.shape[3])), axis=(2, 4))\n",
    "\n",
    "    def relu_activation(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "    def convolution(self, image, kernel):\n",
    "        result = [\n",
    "            np.sum(\n",
    "                [convolve2d(image[i, :, :, c], kernel[:, :, c, j], mode='same', boundary='symm')[:, :, np.newaxis]\n",
    "                 for c in range(image.shape[-1])]\n",
    "            , axis=0)\n",
    "            for i in range(len(image))\n",
    "            for j in range(kernel.shape[-1])\n",
    "        ]\n",
    "        result = np.array(result)\n",
    "        result = result.reshape((len(image), image.shape[1], image.shape[2], -1))\n",
    "        return result\n",
    "\n",
    "    def softmax_activation(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y, epochs=5, learning_rate=0.01, batch_size=64):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                end = min(i + batch_size, len(X))\n",
    "                batch_images = X[i:end]\n",
    "                batch_labels = y[i:end].astype(int)  # Convert labels to integers\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward_pass(batch_images)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(predictions, batch_labels)\n",
    "\n",
    "                # Backward pass\n",
    "                gradients = self.backward_pass(batch_images, predictions, batch_labels)\n",
    "\n",
    "                # Update weights\n",
    "                self.update_weights(gradients, learning_rate)\n",
    "\n",
    "    def compute_loss(self, predictions, labels):\n",
    "        return np.mean(-np.log(predictions[np.arange(len(predictions)), labels]))\n",
    "\n",
    "\n",
    "\n",
    "    def backward_pass(self, images, predictions, labels):\n",
    "        gradients = {}\n",
    "        num_samples = len(images)\n",
    "    \n",
    "        output_gradients = predictions.copy()\n",
    "        output_gradients[np.arange(num_samples), labels] -= 1\n",
    "        output_gradients /= num_samples\n",
    "    \n",
    "        gradients['fc'] = np.dot(images.reshape((len(images), -1)).T, output_gradients)\n",
    "        gradients['fc_bias'] = np.sum(output_gradients, axis=0, keepdims=True)\n",
    "    \n",
    "        fc_gradients = np.dot(output_gradients, self.weights['fc'].T)\n",
    "        fc_gradients_reshaped = fc_gradients.reshape((len(images), 4, 4, 128))\n",
    "    \n",
    "        pool3_gradients = fc_gradients_reshaped.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        \n",
    "        conv6_gradients = np.zeros_like(images)\n",
    "        for i in range(3):\n",
    "            for j in range(128):\n",
    "                conv6_gradients[:, :, :, i:i+3, :, :] += np.sum(images[:, np.newaxis, :, i:i+3, :, :] * pool3_gradients[:, j, :, np.newaxis, np.newaxis, :], axis=(2, 3, 4))\n",
    "\n",
    "        conv6_gradients = np.sum(conv6_gradients, axis=(1, 2, 3))\n",
    "        conv6_gradients = conv6_gradients.reshape((3, 128, len(pool3_gradients), pool3_gradients.shape[2], pool3_gradients.shape[3]))\n",
    "        conv6_gradients = np.sum(conv6_gradients, axis=0)\n",
    "        conv6_gradients = conv6_gradients[:, :, np.newaxis, :, :]\n",
    "\n",
    "    \n",
    "        gradients['conv6'] = np.zeros_like(self.weights['conv6'])\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                gradients['conv6'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv6_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        pool3_gradients = conv6_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv5_gradients = pool3_gradients * (self.weights['conv5'] > 0)\n",
    "    \n",
    "        gradients['conv5'] = np.zeros_like(self.weights['conv5'])\n",
    "        for i in range(64):\n",
    "            for j in range(128):\n",
    "                gradients['conv5'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv5_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "\n",
    "        pool2_gradients = conv5_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv4_gradients = pool2_gradients * (self.weights['conv4'] > 0)\n",
    "    \n",
    "        gradients['conv4'] = np.zeros_like(self.weights['conv4'])\n",
    "        for i in range(64):\n",
    "            for j in range(64):\n",
    "                gradients['conv4'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv4_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        pool2_gradients = conv4_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv3_gradients = pool2_gradients * (self.weights['conv3'] > 0)\n",
    "    \n",
    "        gradients['conv3'] = np.zeros_like(self.weights['conv3'])\n",
    "        for i in range(32):\n",
    "            for j in range(64):\n",
    "                gradients['conv3'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv3_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        pool1_gradients = conv3_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv2_gradients = pool1_gradients * (self.weights['conv2'] > 0)\n",
    "    \n",
    "        gradients['conv2'] = np.zeros_like(self.weights['conv2'])\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                gradients['conv2'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv2_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "\n",
    "        pool1_gradients = conv2_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv1_gradients = pool1_gradients * (self.weights['conv1'] > 0)\n",
    "    \n",
    "        gradients['conv1'] = np.zeros_like(self.weights['conv1'])\n",
    "        for i in range(3):\n",
    "            for j in range(32):\n",
    "                gradients['conv1'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv1_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        return gradients\n",
    "\n",
    "\n",
    "# Convert tensors to Pandas DataFrame using threading\n",
    "def convert_to_dataframe(X, y, start, end, result, cnn_model):\n",
    "    data = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        image = X[i].reshape(-1, 32, 32, 3)\n",
    "        label = y[i]\n",
    "\n",
    "        # Forward pass through the CNN to get features\n",
    "        features = cnn_model.forward_pass(np.expand_dims(image, axis=0))\n",
    "\n",
    "        data.append(np.concatenate([features.flatten(), [int(label)]]))\n",
    "\n",
    "    result.extend(data)\n",
    "\n",
    "# Function to use threading effectively\n",
    "def tensors_to_dataframe(X, y, cnn_model, num_threads=4, batch_size=64):\n",
    "    data = []\n",
    "    threads = []\n",
    "\n",
    "    for i in range(0, len(X), batch_size * num_threads):\n",
    "        for j in range(num_threads):\n",
    "            start = i + j * batch_size\n",
    "            end = min(i + (j + 1) * batch_size, len(X))\n",
    "            thread_result = []\n",
    "            thread = threading.Thread(target=convert_to_dataframe, args=(X[start:end], y[start:end], start, end, thread_result, cnn_model))\n",
    "            thread.daemon = True  # Set daemon attribute to True\n",
    "            thread.start()\n",
    "            threads.append((thread, thread_result))\n",
    "\n",
    "    for thread, thread_result in threads:\n",
    "        try:\n",
    "            thread.join()\n",
    "        except Exception as e:\n",
    "            pass  # Do nothing on exception\n",
    "\n",
    "        data.extend(thread_result)\n",
    "\n",
    "    # Create Pandas DataFrame\n",
    "    columns = [f'feature_{i}' for i in range(data[0].shape[0] - 1)] + ['label']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model = SimpleCNNCifar()\n",
    "cnn_model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# Convert tensors to Pandas DataFrame using threading\n",
    "df_train_cnn = tensors_to_dataframe(X_train, y_train, cnn_model, batch_size=64)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "df_train_cnn, df_test_cnn = train_test_split(df_train_cnn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train_cnn, y_train_cnn = df_train_cnn.iloc[:, :-1], df_train_cnn['label']\n",
    "X_test_cnn, y_test_cnn = df_test_cnn.iloc[:, :-1], df_test_cnn['label']\n",
    "\n",
    "# Continue with your desired classification model or further processing using the CNN features.\n",
    "# For example, if you want to use the SimpleCNNCifar model for classification:\n",
    "\n",
    "# Initialize and train SimpleCNNCifar model\n",
    "cnn_model_cifar = SimpleCNNCifar()\n",
    "cnn_model_cifar.fit(X_train_cnn, y_train_cnn, epochs=5, learning_rate=0.01)\n",
    "\n",
    "# Ensure the number of samples in X_test_cnn matches y_test_cnn\n",
    "X_test_cnn = df_test_cnn.iloc[:, :-1].values.reshape(-1, 32, 32, 3)\n",
    "y_test_cnn = df_test_cnn['label']\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_cnn_cifar = np.argmax(cnn_model_cifar.forward_pass(X_test_cnn), axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_cnn_cifar = accuracy_score(y_test_cnn, y_pred_cnn_cifar)\n",
    "print(f'SimpleCNNCifar Accuracy: {accuracy_cnn_cifar:.4f}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b23a46e-8d53-45d5-acf1-0680f5afd1c2",
   "metadata": {},
   "source": [
    "THis trains a CNN neural network in Tensorflow without using Keras Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388504a7-b6b8-4b40-86b8-46578de49169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1422.8845, Accuracy: 0.3310\n",
      "Epoch 2/5, Loss: 818.0657, Accuracy: 0.3730\n",
      "Epoch 3/5, Loss: 544.9580, Accuracy: 0.3941\n",
      "Epoch 4/5, Loss: 376.3708, Accuracy: 0.4138\n",
      "Epoch 5/5, Loss: 278.3134, Accuracy: 0.4148\n",
      "Test Accuracy: 0.3905\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build the CNN model using subclassed layers\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = tf.Variable(tf.random.normal([3, 3, 3, 32]))\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = tf.Variable(tf.random.normal([3, 3, 32, 64]))\n",
    "        # Fully connected layer\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.Variable(tf.random.normal([8 * 8 * 64, 256]))\n",
    "        # Output layer\n",
    "        self.output_layer = tf.Variable(tf.random.normal([256, 10]))\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Convolutional layer 1\n",
    "        conv1 = tf.nn.relu(tf.nn.conv2d(x, self.conv1, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.nn.relu(tf.nn.conv2d(pool1, self.conv2, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # Flatten\n",
    "        flattened = self.flatten(pool2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc = tf.nn.relu(tf.matmul(flattened, self.fc))\n",
    "\n",
    "        # Output layer\n",
    "        output = tf.matmul(fc, self.output_layer)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "# Define training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Use tf.data.Dataset for input pipeline parallelization\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.device('/device:GPU:0'):  # Specify the GPU device\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                current_loss = loss_fn(y_batch, logits)\n",
    "\n",
    "            gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss = loss_fn(y_train, model(x_train, training=False))\n",
    "    train_accuracy = np.mean(np.argmax(model(x_train, training=False), axis=1) == np.argmax(y_train, axis=1))\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = np.mean(np.argmax(model(x_test, training=False), axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1c775",
   "metadata": {},
   "source": [
    "# Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290c624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "xt=tf.constant(X_train)\n",
    "xt.shape, tf.rank(xt)\n",
    "y1 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "y2 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob4=tf.matmul(y1, tf.transpose(y2))\n",
    "prob5=tf.tensordot(y1, tf.transpose(y2), axes=1)\n",
    "y6 = tf.random.uniform(shape=[224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob6=tf.math.reduce_max(y6, axis=0)\n",
    "y7 = tf.random.uniform(shape=[1, 224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob7 = tf.squeeze(y7, axis=0)\n",
    "y8 = tf.random.uniform(shape=[10], minval=0, maxval=10, dtype=tf.int64)\n",
    "prob9=tf.math.argmax(y8)\n",
    "prob10=tf.one_hot(y8, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14aa121-df37-4304-9b37-b51afe532078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       " array([[1, 9, 3, 2],\n",
       "        [1, 3, 9, 7],\n",
       "        [8, 6, 9, 4]])>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[-1.5040163 , -0.8377314 ,  0.5743124 , -0.833165  ],\n",
       "        [-0.693856  , -1.2710524 , -0.69161797, -0.3762566 ],\n",
       "        [-0.0787261 ,  0.33580709,  0.59934694, -0.15102491]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[5.2, 5.2, 5.2, 5.2],\n",
       "        [5.2, 5.2, 5.2, 5.2],\n",
       "        [5.2, 5.2, 5.2, 5.2]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]], dtype=float32)>,\n",
       " TensorShape([3, 4]),\n",
       " tf.float32,\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 4])>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [4]])>,\n",
       " <tf.Tensor: shape=(25,), dtype=float32, numpy=\n",
       " array([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 10), dtype=float64, numpy=\n",
       " array([[-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ])>,\n",
       " <tf.Tensor: shape=(35, 1), dtype=float32, numpy=\n",
       " array([[-1.        ],\n",
       "        [-0.7777778 ],\n",
       "        [-0.5555556 ],\n",
       "        [-0.33333334],\n",
       "        [-0.11111111],\n",
       "        [ 0.11111111],\n",
       "        [ 0.33333334],\n",
       "        [ 0.5555556 ],\n",
       "        [ 0.7777778 ],\n",
       "        [ 1.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[20]])>,\n",
       " <tf.Tensor: shape=(4,), dtype=float64, numpy=array([10., 11., 12., 13.])>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 0, 4])>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(X_train)\n",
    "a = tf.linspace(-1, 1, 10)\n",
    "a_new = tf.expand_dims(a, axis=1)\n",
    "a_new\n",
    "a_transpose=tf.transpose(a_new)\n",
    "a_randint = tf.random.uniform(shape=[3,4], minval=1, maxval=10, dtype=tf.int32)\n",
    "a_randn = tf.random.normal(shape=[3,4])\n",
    "a_zeros = tf.zeros(shape=[3,4])\n",
    "a_ones = tf.ones(shape=[3,4])\n",
    "a_fives = tf.fill([3,4], 5.2)\n",
    "a_eye = tf.eye(5)\n",
    "x = tf.expand_dims(tf.Variable([1, 2, 0, 4]), axis=1)\n",
    "mask = x >= 2\n",
    "a_slice = tf.boolean_mask(x, mask)\n",
    "masked = tf.greater(x,1)\n",
    "zeros=tf.zeros_like(x)\n",
    "a_masked = tf.where(masked, x, zeros)\n",
    "a_sq_eye = tf.reshape(a_eye, [25])\n",
    "a_new_transpose = tf.squeeze(a_transpose)\n",
    "a_concat = tf.concat((tf.cast(a_new, tf.float32), tf.cast(tf.expand_dims(a_sq_eye, axis=1), tf.float32)), axis=0)\n",
    "a_matmul = tf.matmul(tf.transpose(a_masked), x)\n",
    "\n",
    "numpy_arr = np.array([10.0, 11.0, 12.0, 13.0])\n",
    "from_numpy_to_tensor = tf.convert_to_tensor(numpy_arr)\n",
    "a_min = tf.reduce_min(x)\n",
    "a_max = tf.reduce_max(x, axis=1)\n",
    "a_randint, a_randn, a_zeros, a_fives, a_ones, a_eye, a_randint.shape, a_randn.dtype, a_slice, a_masked, a_sq_eye, a_transpose, a_new_transpose, a_concat, a_matmul, from_numpy_to_tensor, a_min, a_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94968d-902c-4f2b-b846-7c3b0a3cf64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744cd7c-1f08-4746-81fe-312a3550b2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
