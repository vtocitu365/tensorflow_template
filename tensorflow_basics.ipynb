{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5370c9",
   "metadata": {},
   "source": [
    "# Build Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "100bd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Neural Network\n",
    "import tensorflow as tf\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "    xs = tf.cast(xs, tf.float32) / 255.0\n",
    "    ys = tf.cast(ys, tf.float32)\n",
    "    ys = tf.one_hot(ys, depth=n_classes)\n",
    "    return tf.data.Dataset.from_tensor_slices((xs, ys)).map(preprocess).shuffle(len(ys)).batch(128)\n",
    "\n",
    "def val_nn(training_inputs_data, training_outputs_data, test_inputs):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    training_inputs = tensorflow.compat.v1.placeholder(shape=[None, 3], dtype=tensorflow.float32)  \n",
    "    training_outputs = tensorflow.compat.v1.placeholder(shape=[None, 1], dtype=tensorflow.float32) #Desired outputs for each input  \n",
    "    weights = tensorflow.Variable(initial_value=[[.3], [.1], [.8]], dtype=tensorflow.float32)  \n",
    "    bias = tensorflow.Variable(initial_value=[[1]], dtype=tensorflow.float32)  \n",
    "\n",
    "    af_input = tensorflow.matmul(training_inputs, weights) + bias  \n",
    "  \n",
    "    # Activation function of the output layer neuron  \n",
    "    predictions = tensorflow.nn.sigmoid(af_input)  \n",
    "    # Measuring the prediction error of the network after being trained  \n",
    "    prediction_error = tensorflow.reduce_sum(training_outputs - predictions)  \n",
    "    # Minimizing the prediction error using gradient descent optimizer  \n",
    "    \n",
    "    train_op = tensorflow.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(prediction_error) \n",
    "    # Creating a TensorFlow Session  \n",
    "    sess = tensorflow.compat.v1.Session()  \n",
    "    # Initializing the TensorFlow Variables (weights and bias)  \n",
    "    sess.run(tensorflow.compat.v1.global_variables_initializer())  \n",
    "    \n",
    "    # Training loop of the neural network  \n",
    "    for step in range(10000):  \n",
    "        sess.run(fetches=train_op, feed_dict={training_inputs: training_inputs_data, training_outputs: training_outputs_data})  \n",
    "        # Class scores of some testing data  \n",
    "    score= sess.run(fetches=predictions, feed_dict={training_inputs: [[248, 80, 68], [0, 0, 255]]})\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predictions =  sess.run(fetches=test_inputs)\n",
    "    # Closing the TensorFlow Session to free resources  \n",
    "    sess.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "213165e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2., 13.],\n",
       "       [ 7.,  9.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "X_test=tensorflow.convert_to_tensor(value=X_test, dtype=tensorflow.float32)\n",
    "val_nn(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa10de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop Neural Network\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = 1.0 / (1.0 + exp(-activation))\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            transfer_derivative = neuron['output'] * (1.0 - neuron['output'])\n",
    "            neuron['delta'] = errors[j] * transfer_derivative\n",
    "\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    "\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10c141a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': 0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': -0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': 0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': -0.03803132596437354}]\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836, 2.550537003, 0],\n",
    "           [1.465489372, 2.362125076, 0],\n",
    "           [3.396561688, 4.400293529, 0],\n",
    "           [1.38807019, 1.850220317, 0],\n",
    "           [3.06407232, 3.005305973, 0],\n",
    "           [7.627531214, 2.759262235, 1],\n",
    "           [5.332441248, 2.088626775, 1],\n",
    "           [6.922596716, 1.77106367, 1],\n",
    "           [8.675418651, -0.242068655, 1],\n",
    "           [7.673756466, 3.508563011, 1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "           [{'weights': [0.2550690257394217, 0.49543508709194095]},\n",
    "            {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b263d-5053-4a06-a25b-1270d80d7ef4",
   "metadata": {},
   "source": [
    "# Build CNN network using Pure Python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bff641f-fae4-4b59-a056-76ab2ba6ebb5",
   "metadata": {},
   "source": [
    "The following is an uncompleted project. I used ChatGPT to write a code to create and train a CNN neural network in just Python, without using Tensorflow or PyTorch. This was an experiment to see how capable the coding algorithm was. It did not solve the backward pass updating for the convolution. I'm putting this project on hold and will return to it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8231196-a91b-42bf-99c1-e80d0c312272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 32, 32)\n",
      "(64, 32, 32, 32)\n",
      "(64, 32, 32, 32)\n",
      "(64, 16, 16, 32)\n",
      "(64, 16, 16, 64)\n",
      "(64, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train.astype('float32')# / 255.0\n",
    "X_test = X_test.astype('float32')# / 255.0\n",
    "\n",
    "# Flatten labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train.flatten())\n",
    "y_test = label_encoder.transform(y_test.flatten())\n",
    "\n",
    "class SimpleCNNCifar:\n",
    "    def __init__(self):\n",
    "        self.weights = {\n",
    "            'conv1': np.random.randn(3, 3, 3, 32) / (3 * 3 * 3),\n",
    "            'batch_norm1_gamma': np.ones((1, 1, 1, 32)),\n",
    "            'batch_norm1_beta': np.zeros((1, 1, 1, 32)),\n",
    "            'conv2': np.random.randn(3, 3, 32, 32) / (3 * 3 * 32),\n",
    "            'batch_norm2_gamma': np.ones((1, 1, 1, 32)),\n",
    "            'batch_norm2_beta': np.zeros((1, 1, 1, 32)),\n",
    "            'conv3': np.random.randn(3, 3, 32, 64) / (3 * 3 * 32),\n",
    "            'batch_norm3_gamma': np.ones((1, 1, 1, 64)),\n",
    "            'batch_norm3_beta': np.zeros((1, 1, 1, 64)),\n",
    "            'conv4': np.random.randn(3, 3, 64, 64) / (3 * 3 * 64),\n",
    "            'batch_norm4_gamma': np.ones((1, 1, 1, 64)),\n",
    "            'batch_norm4_beta': np.zeros((1, 1, 1, 64)),\n",
    "            'conv5': np.random.randn(3, 3, 64, 128) / (3 * 3 * 64),\n",
    "            'batch_norm5_gamma': np.ones((1, 1, 1, 128)),\n",
    "            'batch_norm5_beta': np.zeros((1, 1, 1, 128)),\n",
    "            'conv6': np.random.randn(3, 3, 128, 128) / (3 * 3 * 128),\n",
    "            'batch_norm6_gamma': np.ones((1, 1, 1, 128)),\n",
    "            'batch_norm6_beta': np.zeros((1, 1, 1, 128)),\n",
    "            'dense1': np.random.randn(2048, 10) / 2048,\n",
    "            'dense1_bias': np.zeros((1, 10)),\n",
    "            'fc': np.random.randn(2048, 10) / 2048,\n",
    "            'fc_bias': np.zeros((1, 10)),\n",
    "        }\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def forward_pass(self, images):\n",
    "        conv1 = self.convolution(images, self.weights['conv1'])\n",
    "        print(conv1.shape)\n",
    "        activation1 = self.relu_activation(conv1)\n",
    "        print(activation1.shape)\n",
    "        batch_norm1 = self.batch_normalization(activation1, self.weights['batch_norm1_gamma'], self.weights['batch_norm1_beta'])\n",
    "        print(batch_norm1.shape)\n",
    "        conv2 = self.convolution(activation1, self.weights['conv2'])\n",
    "        \n",
    "        activation2 = self.relu_activation(conv2)\n",
    "        batch_norm2 = self.batch_normalization(activation2, self.weights['batch_norm2_gamma'], self.weights['batch_norm2_beta'])\n",
    "        pool1 = self.max_pooling(batch_norm2)\n",
    "        print(pool1.shape)\n",
    "        conv3 = self.convolution(pool1, self.weights['conv3'])\n",
    "        batch_norm3 = self.batch_normalization(conv3, self.weights['batch_norm3_gamma'], self.weights['batch_norm3_beta'])\n",
    "        activation3 = self.relu_activation(batch_norm3)\n",
    "        print(activation3.shape)\n",
    "        conv4 = self.convolution(activation3, self.weights['conv4'])\n",
    "        batch_norm4 = self.batch_normalization(conv4, self.weights['batch_norm4_gamma'], self.weights['batch_norm4_beta'])\n",
    "        activation4 = self.relu_activation(batch_norm4)\n",
    "        pool2 = self.max_pooling(activation4)\n",
    "        print(pool2.shape)\n",
    "        conv5 = self.convolution(pool2, self.weights['conv5'])\n",
    "        batch_norm5 = self.batch_normalization(conv5, self.weights['batch_norm5_gamma'], self.weights['batch_norm5_beta'])\n",
    "        activation5 = self.relu_activation(batch_norm5)\n",
    "\n",
    "        conv6 = self.convolution(activation5, self.weights['conv6'])\n",
    "        batch_norm6 = self.batch_normalization(conv6, self.weights['batch_norm6_gamma'], self.weights['batch_norm6_beta'])\n",
    "        activation6 = self.relu_activation(batch_norm6)\n",
    "        pool3 = self.max_pooling(activation6)\n",
    "        print(pool3.shape)\n",
    "        flatten = pool3.reshape((pool3.shape[0], -1))\n",
    "        dense1 = np.dot(flatten, self.weights['dense1']) + self.weights['dense1_bias']\n",
    "        print(dense1.shape)\n",
    "        return dense1\n",
    "\n",
    "    def batch_normalization(self, x, gamma, beta):\n",
    "        mean = np.mean(x, axis=(0, 1, 2), keepdims=True)\n",
    "        variance = np.var(x, axis=(0, 1, 2), keepdims=True)\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        return gamma * x_normalized + beta\n",
    "\n",
    "\n",
    "    def max_pooling(self, image, pool_size=(2, 2)):\n",
    "        return np.max(image.reshape((image.shape[0], image.shape[1] // pool_size[0], pool_size[0], image.shape[2] // pool_size[1], pool_size[1], image.shape[3])), axis=(2, 4))\n",
    "\n",
    "    def relu_activation(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "    def convolution(self, image, kernel):\n",
    "        result = [\n",
    "            np.sum(\n",
    "                [convolve2d(image[i, :, :, c], kernel[:, :, c, j], mode='same', boundary='symm')[:, :, np.newaxis]\n",
    "                 for c in range(image.shape[-1])]\n",
    "            , axis=0)\n",
    "            for i in range(len(image))\n",
    "            for j in range(kernel.shape[-1])\n",
    "        ]\n",
    "        result = np.array(result)\n",
    "        result = result.reshape((len(image), image.shape[1], image.shape[2], -1))\n",
    "        return result\n",
    "\n",
    "    def softmax_activation(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y, epochs=5, learning_rate=0.01, batch_size=64):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                end = min(i + batch_size, len(X))\n",
    "                batch_images = X[i:end]\n",
    "                batch_labels = y[i:end].astype(int)  # Convert labels to integers\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward_pass(batch_images)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(predictions, batch_labels)\n",
    "\n",
    "                # Backward pass\n",
    "                gradients = self.backward_pass(batch_images, predictions, batch_labels)\n",
    "\n",
    "                # Update weights\n",
    "                self.update_weights(gradients, learning_rate)\n",
    "\n",
    "    def compute_loss(self, predictions, labels):\n",
    "        return np.mean(-np.log(predictions[np.arange(len(predictions)), labels]))\n",
    "\n",
    "\n",
    "\n",
    "    def backward_pass(self, images, predictions, labels):\n",
    "        gradients = {}\n",
    "        num_samples = len(images)\n",
    "    \n",
    "        output_gradients = predictions.copy()\n",
    "        output_gradients[np.arange(num_samples), labels] -= 1\n",
    "        output_gradients /= num_samples\n",
    "    \n",
    "        gradients['fc'] = np.dot(images.reshape((len(images), -1)).T, output_gradients)\n",
    "        gradients['fc_bias'] = np.sum(output_gradients, axis=0, keepdims=True)\n",
    "    \n",
    "        fc_gradients = np.dot(output_gradients, self.weights['fc'].T)\n",
    "        fc_gradients_reshaped = fc_gradients.reshape((len(images), 4, 4, 128))\n",
    "    \n",
    "        pool3_gradients = fc_gradients_reshaped.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        \n",
    "        conv6_gradients = np.zeros_like(images)\n",
    "        for i in range(3):\n",
    "            for j in range(128):\n",
    "                conv6_gradients[:, :, :, i:i+3, :, :] += np.sum(images[:, np.newaxis, :, i:i+3, :, :] * pool3_gradients[:, j, :, np.newaxis, np.newaxis, :], axis=(2, 3, 4))\n",
    "\n",
    "        conv6_gradients = np.sum(conv6_gradients, axis=(1, 2, 3))\n",
    "        conv6_gradients = conv6_gradients.reshape((3, 128, len(pool3_gradients), pool3_gradients.shape[2], pool3_gradients.shape[3]))\n",
    "        conv6_gradients = np.sum(conv6_gradients, axis=0)\n",
    "        conv6_gradients = conv6_gradients[:, :, np.newaxis, :, :]\n",
    "\n",
    "    \n",
    "        gradients['conv6'] = np.zeros_like(self.weights['conv6'])\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                gradients['conv6'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv6_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        pool3_gradients = conv6_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv5_gradients = pool3_gradients * (self.weights['conv5'] > 0)\n",
    "    \n",
    "        gradients['conv5'] = np.zeros_like(self.weights['conv5'])\n",
    "        for i in range(64):\n",
    "            for j in range(128):\n",
    "                gradients['conv5'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv5_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "\n",
    "        pool2_gradients = conv5_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv4_gradients = pool2_gradients * (self.weights['conv4'] > 0)\n",
    "    \n",
    "        gradients['conv4'] = np.zeros_like(self.weights['conv4'])\n",
    "        for i in range(64):\n",
    "            for j in range(64):\n",
    "                gradients['conv4'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv4_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        pool2_gradients = conv4_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv3_gradients = pool2_gradients * (self.weights['conv3'] > 0)\n",
    "    \n",
    "        gradients['conv3'] = np.zeros_like(self.weights['conv3'])\n",
    "        for i in range(32):\n",
    "            for j in range(64):\n",
    "                gradients['conv3'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv3_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        pool1_gradients = conv3_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv2_gradients = pool1_gradients * (self.weights['conv2'] > 0)\n",
    "    \n",
    "        gradients['conv2'] = np.zeros_like(self.weights['conv2'])\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                gradients['conv2'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv2_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "\n",
    "        pool1_gradients = conv2_gradients.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        conv1_gradients = pool1_gradients * (self.weights['conv1'] > 0)\n",
    "    \n",
    "        gradients['conv1'] = np.zeros_like(self.weights['conv1'])\n",
    "        for i in range(3):\n",
    "            for j in range(32):\n",
    "                gradients['conv1'][:, :, :, j] += np.sum(\n",
    "                    images[:, i:i+3, :, :] * conv1_gradients[:, j, :, :], axis=(0, 2, 3)\n",
    "                )\n",
    "    \n",
    "        return gradients\n",
    "\n",
    "\n",
    "# Convert tensors to Pandas DataFrame using threading\n",
    "def convert_to_dataframe(X, y, start, end, result, cnn_model):\n",
    "    data = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        image = X[i].reshape(-1, 32, 32, 3)\n",
    "        label = y[i]\n",
    "\n",
    "        # Forward pass through the CNN to get features\n",
    "        features = cnn_model.forward_pass(np.expand_dims(image, axis=0))\n",
    "\n",
    "        data.append(np.concatenate([features.flatten(), [int(label)]]))\n",
    "\n",
    "    result.extend(data)\n",
    "\n",
    "# Function to use threading effectively\n",
    "def tensors_to_dataframe(X, y, cnn_model, num_threads=4, batch_size=64):\n",
    "    data = []\n",
    "    threads = []\n",
    "\n",
    "    for i in range(0, len(X), batch_size * num_threads):\n",
    "        for j in range(num_threads):\n",
    "            start = i + j * batch_size\n",
    "            end = min(i + (j + 1) * batch_size, len(X))\n",
    "            thread_result = []\n",
    "            thread = threading.Thread(target=convert_to_dataframe, args=(X[start:end], y[start:end], start, end, thread_result, cnn_model))\n",
    "            thread.daemon = True  # Set daemon attribute to True\n",
    "            thread.start()\n",
    "            threads.append((thread, thread_result))\n",
    "\n",
    "    for thread, thread_result in threads:\n",
    "        try:\n",
    "            thread.join()\n",
    "        except Exception as e:\n",
    "            pass  # Do nothing on exception\n",
    "\n",
    "        data.extend(thread_result)\n",
    "\n",
    "    # Create Pandas DataFrame\n",
    "    columns = [f'feature_{i}' for i in range(data[0].shape[0] - 1)] + ['label']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model = SimpleCNNCifar()\n",
    "cnn_model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# Convert tensors to Pandas DataFrame using threading\n",
    "df_train_cnn = tensors_to_dataframe(X_train, y_train, cnn_model, batch_size=64)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "df_train_cnn, df_test_cnn = train_test_split(df_train_cnn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train_cnn, y_train_cnn = df_train_cnn.iloc[:, :-1], df_train_cnn['label']\n",
    "X_test_cnn, y_test_cnn = df_test_cnn.iloc[:, :-1], df_test_cnn['label']\n",
    "\n",
    "# Continue with your desired classification model or further processing using the CNN features.\n",
    "# For example, if you want to use the SimpleCNNCifar model for classification:\n",
    "\n",
    "# Initialize and train SimpleCNNCifar model\n",
    "cnn_model_cifar = SimpleCNNCifar()\n",
    "cnn_model_cifar.fit(X_train_cnn, y_train_cnn, epochs=5, learning_rate=0.01)\n",
    "\n",
    "# Ensure the number of samples in X_test_cnn matches y_test_cnn\n",
    "X_test_cnn = df_test_cnn.iloc[:, :-1].values.reshape(-1, 32, 32, 3)\n",
    "y_test_cnn = df_test_cnn['label']\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_cnn_cifar = np.argmax(cnn_model_cifar.forward_pass(X_test_cnn), axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_cnn_cifar = accuracy_score(y_test_cnn, y_pred_cnn_cifar)\n",
    "print(f'SimpleCNNCifar Accuracy: {accuracy_cnn_cifar:.4f}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b23a46e-8d53-45d5-acf1-0680f5afd1c2",
   "metadata": {},
   "source": [
    "THis trains a CNN neural network in Tensorflow without using Keras Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388504a7-b6b8-4b40-86b8-46578de49169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1422.8845, Accuracy: 0.3310\n",
      "Epoch 2/5, Loss: 818.0657, Accuracy: 0.3730\n",
      "Epoch 3/5, Loss: 544.9580, Accuracy: 0.3941\n",
      "Epoch 4/5, Loss: 376.3708, Accuracy: 0.4138\n",
      "Epoch 5/5, Loss: 278.3134, Accuracy: 0.4148\n",
      "Test Accuracy: 0.3905\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build the CNN model using subclassed layers\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = tf.Variable(tf.random.normal([3, 3, 3, 32]))\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = tf.Variable(tf.random.normal([3, 3, 32, 64]))\n",
    "        # Fully connected layer\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.Variable(tf.random.normal([8 * 8 * 64, 256]))\n",
    "        # Output layer\n",
    "        self.output_layer = tf.Variable(tf.random.normal([256, 10]))\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Convolutional layer 1\n",
    "        conv1 = tf.nn.relu(tf.nn.conv2d(x, self.conv1, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.nn.relu(tf.nn.conv2d(pool1, self.conv2, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # Flatten\n",
    "        flattened = self.flatten(pool2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc = tf.nn.relu(tf.matmul(flattened, self.fc))\n",
    "\n",
    "        # Output layer\n",
    "        output = tf.matmul(fc, self.output_layer)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "# Define training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Use tf.data.Dataset for input pipeline parallelization\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.device('/device:GPU:0'):  # Specify the GPU device\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                current_loss = loss_fn(y_batch, logits)\n",
    "\n",
    "            gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss = loss_fn(y_train, model(x_train, training=False))\n",
    "    train_accuracy = np.mean(np.argmax(model(x_train, training=False), axis=1) == np.argmax(y_train, axis=1))\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = np.mean(np.argmax(model(x_test, training=False), axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1c775",
   "metadata": {},
   "source": [
    "# Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290c624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "xt=tf.constant(X_train)\n",
    "xt.shape, tf.rank(xt)\n",
    "y1 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "y2 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob4=tf.matmul(y1, tf.transpose(y2))\n",
    "prob5=tf.tensordot(y1, tf.transpose(y2), axes=1)\n",
    "y6 = tf.random.uniform(shape=[224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob6=tf.math.reduce_max(y6, axis=0)\n",
    "y7 = tf.random.uniform(shape=[1, 224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob7 = tf.squeeze(y7, axis=0)\n",
    "y8 = tf.random.uniform(shape=[10], minval=0, maxval=10, dtype=tf.int64)\n",
    "prob9=tf.math.argmax(y8)\n",
    "prob10=tf.one_hot(y8, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14aa121-df37-4304-9b37-b51afe532078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       " array([[1, 9, 3, 2],\n",
       "        [1, 3, 9, 7],\n",
       "        [8, 6, 9, 4]])>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[-1.5040163 , -0.8377314 ,  0.5743124 , -0.833165  ],\n",
       "        [-0.693856  , -1.2710524 , -0.69161797, -0.3762566 ],\n",
       "        [-0.0787261 ,  0.33580709,  0.59934694, -0.15102491]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[5.2, 5.2, 5.2, 5.2],\n",
       "        [5.2, 5.2, 5.2, 5.2],\n",
       "        [5.2, 5.2, 5.2, 5.2]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]], dtype=float32)>,\n",
       " TensorShape([3, 4]),\n",
       " tf.float32,\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 4])>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [4]])>,\n",
       " <tf.Tensor: shape=(25,), dtype=float32, numpy=\n",
       " array([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 10), dtype=float64, numpy=\n",
       " array([[-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "         0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ])>,\n",
       " <tf.Tensor: shape=(35, 1), dtype=float32, numpy=\n",
       " array([[-1.        ],\n",
       "        [-0.7777778 ],\n",
       "        [-0.5555556 ],\n",
       "        [-0.33333334],\n",
       "        [-0.11111111],\n",
       "        [ 0.11111111],\n",
       "        [ 0.33333334],\n",
       "        [ 0.5555556 ],\n",
       "        [ 0.7777778 ],\n",
       "        [ 1.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.        ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[20]])>,\n",
       " <tf.Tensor: shape=(4,), dtype=float64, numpy=array([10., 11., 12., 13.])>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 0, 4])>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(X_train)\n",
    "a = tf.linspace(-1, 1, 10)\n",
    "a_new = tf.expand_dims(a, axis=1)\n",
    "a_new\n",
    "a_transpose=tf.transpose(a_new)\n",
    "a_randint = tf.random.uniform(shape=[3,4], minval=1, maxval=10, dtype=tf.int32)\n",
    "a_randn = tf.random.normal(shape=[3,4])\n",
    "a_zeros = tf.zeros(shape=[3,4])\n",
    "a_ones = tf.ones(shape=[3,4])\n",
    "a_fives = tf.fill([3,4], 5.2)\n",
    "a_eye = tf.eye(5)\n",
    "x = tf.expand_dims(tf.Variable([1, 2, 0, 4]), axis=1)\n",
    "mask = x >= 2\n",
    "a_slice = tf.boolean_mask(x, mask)\n",
    "masked = tf.greater(x,1)\n",
    "zeros=tf.zeros_like(x)\n",
    "a_masked = tf.where(masked, x, zeros)\n",
    "a_sq_eye = tf.reshape(a_eye, [25])\n",
    "a_new_transpose = tf.squeeze(a_transpose)\n",
    "a_concat = tf.concat((tf.cast(a_new, tf.float32), tf.cast(tf.expand_dims(a_sq_eye, axis=1), tf.float32)), axis=0)\n",
    "a_matmul = tf.matmul(tf.transpose(a_masked), x)\n",
    "\n",
    "numpy_arr = np.array([10.0, 11.0, 12.0, 13.0])\n",
    "from_numpy_to_tensor = tf.convert_to_tensor(numpy_arr)\n",
    "a_min = tf.reduce_min(x)\n",
    "a_max = tf.reduce_max(x, axis=1)\n",
    "a_randint, a_randn, a_zeros, a_fives, a_ones, a_eye, a_randint.shape, a_randn.dtype, a_slice, a_masked, a_sq_eye, a_transpose, a_new_transpose, a_concat, a_matmul, from_numpy_to_tensor, a_min, a_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94968d-902c-4f2b-b846-7c3b0a3cf64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744cd7c-1f08-4746-81fe-312a3550b2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
