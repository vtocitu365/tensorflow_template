{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12242ac5",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3167760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Timeseries LSTM Autoencoder\n",
    "def vtoc_lstm_autoencoder(n_steps, n_horizon, n_features, lr):\n",
    "    serie_size=n_steps\n",
    "    encoder_decoder = Sequential()\n",
    "    encoder_decoder.add(LSTM(n_steps, activation='relu', input_shape=(n_steps, n_features), return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(1, activation='relu'))\n",
    "    encoder_decoder.add(RepeatVector(serie_size))\n",
    "    encoder_decoder.add(LSTM(serie_size, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(TimeDistributed(Dense(1)))\n",
    "    encoder_decoder.summary()\n",
    "\n",
    "    adam = Adam(lr)\n",
    "    encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5c33ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "#Timeseries LSTM Model\n",
    "def vtoc_lstm_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(72, activation='relu', input_shape=(n_steps, n_features), return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=True)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128, activation='relu')),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries CNN Model\n",
    "def vtoc_cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    #tf.keras.layers.Input(shape=(n_steps, n_features)),\n",
    "    #tf.keras.layers.Flatten(input_shape=(n_steps, n_features)),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Flatten()),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss= Huber()\n",
    "    optimizer =Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries DNN Model\n",
    "def vtoc_dnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(n_steps, n_features)), #Use with evaluate_timeseries_model\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='dnn')\n",
    "    \n",
    "    loss=tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Model\n",
    "def lstm_cnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_steps,n_features))),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(LSTM(72, activation='relu', return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=False)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Skip Model\n",
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(72, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(48, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, skip_flatten])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a260d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Prepare timeseries data\n",
    "def multi_baseline_eror(data, pred_cols):\n",
    "    df = data[pred_cols]\n",
    "    #fill nans with linear interpolation because this is how we will fill when using the data in the models.\n",
    "    df_filled = df.interpolate(\"linear\")\n",
    "    mm = MinMaxScaler()\n",
    "    df_scaled = mm.fit_transform(df_filled)\n",
    "    df_prep = pd.DataFrame(df_scaled, columns=pred_cols)\n",
    "    y_true = df_prep[pred_cols[0]]\n",
    "    y_pred_forecast = df_prep[pred_cols[1]]\n",
    "\n",
    "    ### persistence 1 day\n",
    "    #shift series by 24 hours\n",
    "    # realign y_true to have the same length and time samples\n",
    "    y_preds_persistance_1_day = y_true.shift(24).dropna()\n",
    "    persistence_1_day_mae = tf.keras.losses.MAE(y_true[y_preds_persistance_1_day.index], y_preds_persistance_1_day).numpy()\n",
    "    persistence_1_day_mape = tf.keras.losses.MAPE(np.maximum(y_true[y_preds_persistance_1_day.index], 1e-5), np.maximum(y_preds_persistance_1_day, 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ### persistence 3 day average\n",
    "    #shift by 1, 2, 3 days. Realign to have same lengths. Average days and calcualte MAE.\n",
    "\n",
    "    shift_dfs = list()\n",
    "    for i in range(1, 4):\n",
    "        shift_dfs.append(pd.Series(y_true.shift(24 * i), name=f\"d{i}\"))\n",
    "\n",
    "    y_persistance_3d = pd.concat(shift_dfs, axis=1).dropna()\n",
    "    y_persistance_3d[\"avg\"] = (y_persistance_3d[\"d1\"] + y_persistance_3d[\"d2\"] + y_persistance_3d[\"d3\"])/3\n",
    "    d3_idx = y_persistance_3d.index\n",
    "    persistence_3day_avg_mae = tf.keras.losses.MAE(y_true[d3_idx], y_persistance_3d['avg']).numpy()\n",
    "    persistence_3day_avg_mape = tf.keras.losses.MAPE(np.maximum(y_true[d3_idx], 1e-5), np.maximum(y_persistance_3d['avg'], 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ref_error = pd.DataFrame({\n",
    "        \"Method\": [\"TSO Forecast\", \"Persistence 1 Day\", \"Persitence 3 Day Avg\"],\n",
    "        \"MAE\": [tf.keras.losses.MAE(y_true, y_pred_forecast).numpy(),\n",
    "                persistence_1_day_mae,\n",
    "                persistence_3day_avg_mae],\n",
    "        \"MAPE\":[tf.keras.losses.MAPE(np.maximum(y_true, 1e-5), np.maximum(y_pred_forecast, 1e-5)).numpy(),\n",
    "                persistence_1_day_mape,\n",
    "                persistence_3day_avg_mape]}, \n",
    "        index=[i for i in range(3)])\n",
    "    return ref_error\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "def split_data(series, train_fraq, test_len=8760):\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "\n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_window_data(dataset, look_back=1, n_features=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :n_features]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :n_features])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "    \n",
    "    #expand dimensions to 3D to fit with LSTM inputs\n",
    "    #creat the inital tensor dataset\n",
    "    if expand_dims:\n",
    "        ds = tf.expand_dims(data, axis=-1)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    \n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_multi_dataset(data, pred_cols, multivar):\n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    for column in pred_cols:\n",
    "        # Getting rid of outliers\n",
    "        data.loc[data[column] == -9999.0, column] = 0.0\n",
    "    ref_error = multi_baseline_eror(timeseries_df, pred_cols)\n",
    "    data=MinMaxScaler().fit_transform(data)\n",
    "    tf.random.set_seed(42)\n",
    "    train_fraq=0.65\n",
    "    lr = 3e-4\n",
    "    n_steps = 14#24*30\n",
    "    n_horizon = 14\n",
    "    batch_size = 64#256\n",
    "    shuffle_buffer = 100\n",
    "    if multivar:\n",
    "        n_features=len(data[0])\n",
    "    else:\n",
    "        n_features=1\n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=8760)\n",
    "    \n",
    "    (X_train, Y_train)=create_window_data(train_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_test, Y_test)=create_window_data(test_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_val, Y_val)=create_window_data(val_data, look_back=n_horizon, n_features=n_features)\n",
    "    #train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #test_ds = window_dataset(test_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #val_ds = window_dataset(val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #split_sequences(train_ds, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False)\n",
    "    #print(f\"Train Data Shape: {train_ds.shape}\")\n",
    "    #print(f\"Val Data Shape: {val_ds.shape}\")\n",
    "    #kfold = KFold(n_folds=5, shuffle=True, random_state=1)\n",
    "    model=vtoc_dnn_model(n_steps, n_horizon, n_features, lr)\n",
    "    model.summary()\n",
    "    model_hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, verbose=3)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    model.evaluate(X_train, Y_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    #_, acc = model.evaluate(test, verbose=2)\n",
    "    #scores.append(acc)\n",
    "    return model_hist.history\n",
    "\n",
    "def evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=False):\n",
    "    df_processed = create_multi_dataset(timeseries_df, pred_cols = ['wv (m/s)', 'max. wv (m/s)'], multivar=multivar)\n",
    "    print(df_processed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b94ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path) #We load the dataset in a csv_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16fb1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 196)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25216     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,534\n",
      "Trainable params: 43,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "8365/8365 [==============================] - 29s 3ms/step - loss: 0.0016 - mae: 0.0342\n",
      "4504/4504 [==============================] - 7s 2ms/step\n",
      "{'loss': [0.002608983078971505, 0.0016388605581596494, 0.0015648044645786285, 0.0015329904854297638, 0.0015151449479162693], 'mae': [0.044424545019865036, 0.03305155038833618, 0.03175380825996399, 0.031170841306447983, 0.030865579843521118], 'val_loss': [0.001654260908253491, 0.001841593300923705, 0.0017942589474841952, 0.0014471292961388826, 0.0016615190543234348], 'val_mae': [0.03435081988573074, 0.03647252917289734, 0.03513265773653984, 0.029650719836354256, 0.03391486033797264]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa18b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import datetime\n",
    "\n",
    "def create_dataset(X, y, delay=24, lookback=48):\n",
    "    window_length = lookback + delay\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)-delay):\n",
    "        v = X.iloc[i-lookback:i].to_numpy() # every one hour, we take the past 48 hours of features\n",
    "        Xs.append(v)\n",
    "        w = y.iloc[i+delay] # Every timestep, we take the temperature the next delay (here one day)\n",
    "        ys.append(w)\n",
    "    return(np.array(Xs), np.array(ys))\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    # Getting rid of outliers\n",
    "    data.loc[data['wv (m/s)'] == -9999.0, 'wv (m/s)'] = 0.0\n",
    "    data.loc[data['max. wv (m/s)'] == -9999.0, 'max. wv (m/s)'] = 0.0\n",
    "    \n",
    "    # Taking values every hours\n",
    "    data = data[5::6]# df[start,stop,step]\n",
    "    \n",
    "    wv = data.pop('wv (m/s)')\n",
    "    max_wv = data.pop('max. wv (m/s)')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = data.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    data['Wx'] = wv*np.cos(wd_rad)\n",
    "    data['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "    # Calculate the max wind x and y components.\n",
    "    data['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "    data['max Wy'] = max_wv*np.sin(wd_rad)\n",
    "    \n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    \n",
    "    day = 24*60*60 # Time is second within a single day\n",
    "    year = 365.2425*day # Time in second withon a year\n",
    "\n",
    "    data['Day sin'] = np.sin(timestamp_s * (2*np.pi / day))\n",
    "    data['Day cos'] = np.cos(timestamp_s * (2*np.pi / day))\n",
    "    data['Year sin'] = np.sin(timestamp_s * (2*np.pi / year))\n",
    "    data['Year cos'] = np.cos(timestamp_s * (2*np.pi / year))\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def split(data):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    train_df = data.iloc[0: n * 70 //100] # \"iloc\" because we have to select the lines at the indicies 0 to int(n*0.7) compared to \"loc\"\n",
    "    val_df = data.iloc[n * 70 //100 : n * 90 //100]\n",
    "    test_df = data.iloc[n * 90 //100:]\n",
    "    \n",
    "    return(train_df, val_df, test_df)\n",
    "\n",
    "def naive_eval_arr(X, y, lookback, delay):\n",
    "    batch_maes = []\n",
    "    for i in range(0, len(X)):\n",
    "        preds = X[i, -1, 1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n",
    "        mae = np.mean(np.abs(preds - y[i]))\n",
    "        batch_maes.append(mae)\n",
    "    return(np.mean(batch_maes))\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=True, batch_size=32, step=1):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while True:\n",
    "        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n",
    "            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n",
    "        else:\n",
    "            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n",
    "                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n",
    "            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n",
    "            i+=len(rows) # rows represents the number of sample in one batch\n",
    "            \n",
    "        samples = np.zeros((len(rows), lookback//step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n",
    "        targets = np.zeros((len(rows),)) #Shape = (batch_size,)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1] #We only want to predict the temperature for now,since [1], the second column\n",
    "        return samples, targets # The yield that replace the return to create a generator and not a regular function.\n",
    "        \n",
    "def evaluate_timeseries_model(df, n_folds=5, multivar=False):\n",
    "    scores, histories = list(), list()\n",
    "    df_processed = preprocessing(df)\n",
    "    train_df, val_df, test_df = split(df_processed)\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean)/train_std # As simple as that !\n",
    "    val_df = (val_df - train_mean)/train_std\n",
    "    test_df = (test_df - train_mean)/train_std\n",
    "    lookback = 48 # Looking at all features for the past 2 days\n",
    "    delay = 24 # Trying to predict the temperature for the next day\n",
    "    batch_size = 64 # Features will be batched 32 by 32.\n",
    "    X_train, Y_train = create_dataset(train_df, train_df['T (degC)'], delay = delay, lookback = lookback)\n",
    "    X_val, Y_val = create_dataset(val_df, val_df['T (degC)'], delay = delay)\n",
    "    naive_loss_arr = naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay)\n",
    "\n",
    "    naive_loss_arr = round(naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay),2) # Round the value\n",
    "    \n",
    "\n",
    "    data_train = train_df.to_numpy()\n",
    "    (X_train, Y_train) = generator(data = data_train, lookback = lookback, delay =delay, min_index = 0, \n",
    "                                   max_index = len(data_train), shuffle = True, batch_size = batch_size)\n",
    "\n",
    "    data_val = val_df.to_numpy()\n",
    "    (X_val, Y_val) = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, \n",
    "                               max_index = len(data_val), batch_size = batch_size)\n",
    "\n",
    "    data_test = test_df.to_numpy()\n",
    "    test_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0,\n",
    "                         max_index = len(data_test), batch_size = batch_size)\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=2)\n",
    "    \n",
    "    '''for train_index, test_index in tscv.split(X_train):\n",
    "        X_train, X_test = X_train[train_index], X_train[test_index]\n",
    "        Y_train, Y_test = Y_train[train_index], Y_train[test_index]'''\n",
    "    print(naive_loss_arr)\n",
    "    lr = 3e-4\n",
    "    n_steps=48#24*30\n",
    "    n_horizon=24\n",
    "    #batch_size=64\n",
    "    if multivar:\n",
    "        n_features=5\n",
    "    else:\n",
    "        n_features=1\n",
    "    model=vtoc_cnn_model(n_steps, n_horizon, n_features, lr)\n",
    "\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, verbose=3, shuffle=True)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    train_encoded = model.predict(X_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    print('Encoded time-series shape', train_encoded.shape)\n",
    "    print('Encoded time-series sample', train_encoded[0])\n",
    "    return model.evaluate(X_test, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
