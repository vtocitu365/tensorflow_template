{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae887a50",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e8c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb4eee2-587d-4041-8145-6dc44d8a3cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.3\n",
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b1aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels2(train, test, target_train, target_test):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    X_train = train.reshape(train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = test.reshape(test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    train_norm = X_train.astype('float32')\n",
    "    test_norm = X_test.astype('float32')\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    target_train = to_categorical(target_train)\n",
    "    target_test =  to_categorical(target_test)\n",
    "    return train_norm, test_norm, target_train, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed36d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels(image, label, depth=10):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.divide(image, 255)\n",
    "    train_norm = tf.image.resize(image, (32, 32))\n",
    "    target = tf.one_hot(label, depth=depth)\n",
    "    return train_norm, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc2ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "def val_cnn_model(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(320, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c4c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN optimized for MNIST\n",
    "def val_cnn_mnist(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.1, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eee238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Optimized for CIFAR10\n",
    "def val_cnn_cifar(n_depth, n_channels=3):\n",
    "    weight_decay = 1e-4\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, n_channels)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_depth, activation='softmax'))\n",
    "    opt=RMSprop(learning_rate=0.001,decay=1e-5)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71556d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN network for images\n",
    "def val_rnn_model(x_train):\n",
    "    '''i = Input(shape=x_train[0].shape)\n",
    "    x = LSTM(128)(i)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model=Model(i, x)'''\n",
    "    \n",
    "    model=Sequential()\n",
    "    #model.add(Input(shape=x_train[0].shape))\n",
    "    model.add(LSTM(128, input_shape=x_train[0].shape))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ee9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained MobileNet network for image recognition\n",
    "def val_mn_model(depth, n_channels=3):\n",
    "    bottom_model = MobileNet(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)#top_model = Dense(1024, activation='relu')(bottom_model.output)\n",
    "    top_model = Dense(depth, activation='relu')(top_model)\n",
    "    top_model = Dense(depth, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    opt = Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f3892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained VGG Network for image recognition\n",
    "def val_vgg_model(depth, n_channels=3):\n",
    "    bottom_model = VGG16(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(depth, activation='relu')(top_model)\n",
    "    top_model = Dense(depth, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained ResNet Network for image recognition\n",
    "def val_resnet_model(depth, n_channels=3):\n",
    "    bottom_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(depth, activation='relu')(top_model)\n",
    "    top_model = Dense(depth, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    opt = Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80756af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(inputs, targets, model, loss_fn, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs)\n",
    "        loss_value = loss_fn(targets, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update the accuracy metric\n",
    "    accuracy_metric.update_state(targets, logits)\n",
    "    \n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def train_step(dataset, model, loss_fn, optimizer):\n",
    "    training_loss = tf.constant(0.0)\n",
    "    num_batches = tf.constant(0)\n",
    "    \n",
    "    for batch in dataset:\n",
    "        inputs, targets = batch\n",
    "        loss = training_step(inputs, targets, model, loss_fn, optimizer)\n",
    "        training_loss += loss\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Calculate the mean loss over all batches\n",
    "    mean_loss = training_loss / tf.cast(num_batches, dtype=tf.float32)\n",
    "    \n",
    "    # Get the current accuracy from the accuracy metric\n",
    "    current_accuracy = accuracy_metric.result()\n",
    "    \n",
    "    # Reset the accuracy metric for the next epoch\n",
    "    accuracy_metric.reset_states()\n",
    "    \n",
    "    return mean_loss, current_accuracy\n",
    "  \n",
    "def evaluate_image_model(train_dataset, val_dataset, num_epochs, n_channels=3, depth=10, model_name='cifar'):\n",
    "    batch_size=64\n",
    "    if model_name=='cifar':\n",
    "        \n",
    "        model=val_cnn_cifar(depth, n_channels) #Needs 80pct accuracy Eurosat gets > 80 at epoch 2 and overfits afterwards #Needs 80pct accuracy CIFAR10 gets > 80 at epoch 5 for train and epoch 6 for validation. Each epoch is 12 min\n",
    "        print(model.summary())\n",
    "    elif model_name=='vgg':\n",
    "        model=val_vgg_model(depth, n_channels) # Reached 80pct at epoch 6. 10 minutes per epoch\n",
    "    elif model_name=='resnet':\n",
    "        model=val_resnet_model(depth, n_channels)\n",
    "    else:\n",
    "        model=val_mn_model(depth, n_channels)\n",
    "    model.fit(train_dataset, epochs=num_epochs, steps_per_epoch=60000 // 64, validation_data=val_dataset, verbose=2)\n",
    "    _, acc = model.evaluate(val_dataset, verbose=2)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92af20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_rnn(x_train, y_train, x_test, y_test):\n",
    "    # Expand dimensions to include a channel (grayscale)\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "    model=val_cnn_mnist()\n",
    "    print(model.summary())\n",
    "    train_generator=ImageDataGenerator(rotation_range=7, width_shift_range=0.05, shear_range=0, height_shift_range=0.07, zoom_range=0.05)\n",
    "    test_generator=ImageDataGenerator()\n",
    "    train_generator=train_generator.flow(x_train, y_train, batch_size=64)\n",
    "    test_generator = test_generator.flow(x_test, y_test, batch_size=64)\n",
    "    model.fit(train_generator, validation_data=test_generator, epochs=5, verbose=2)\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3a9fd2-66e2-4249-acc6-793c874369b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "937/937 - 34s - loss: 1.6528 - accuracy: 0.4963 - val_loss: 1.2268 - val_accuracy: 0.6020 - 34s/epoch - 36ms/step\n",
      "Epoch 2/10\n",
      "937/937 - 17s - loss: 1.0058 - accuracy: 0.6771 - val_loss: 1.0221 - val_accuracy: 0.6785 - 17s/epoch - 18ms/step\n",
      "Epoch 3/10\n",
      "937/937 - 17s - loss: 0.8337 - accuracy: 0.7398 - val_loss: 0.7757 - val_accuracy: 0.7602 - 17s/epoch - 18ms/step\n",
      "Epoch 4/10\n",
      "937/937 - 18s - loss: 0.7305 - accuracy: 0.7775 - val_loss: 0.7373 - val_accuracy: 0.7798 - 18s/epoch - 19ms/step\n",
      "Epoch 5/10\n",
      "937/937 - 18s - loss: 0.6626 - accuracy: 0.8055 - val_loss: 0.7705 - val_accuracy: 0.7801 - 18s/epoch - 19ms/step\n",
      "Epoch 6/10\n",
      "937/937 - 18s - loss: 0.6110 - accuracy: 0.8251 - val_loss: 0.6754 - val_accuracy: 0.8094 - 18s/epoch - 19ms/step\n",
      "Epoch 7/10\n",
      "937/937 - 18s - loss: 0.5689 - accuracy: 0.8436 - val_loss: 0.7100 - val_accuracy: 0.8091 - 18s/epoch - 19ms/step\n",
      "Epoch 8/10\n",
      "937/937 - 18s - loss: 0.5435 - accuracy: 0.8546 - val_loss: 0.7819 - val_accuracy: 0.7928 - 18s/epoch - 20ms/step\n",
      "Epoch 9/10\n",
      "937/937 - 18s - loss: 0.5179 - accuracy: 0.8637 - val_loss: 0.7147 - val_accuracy: 0.8050 - 18s/epoch - 20ms/step\n",
      "Epoch 10/10\n",
      "937/937 - 18s - loss: 0.4991 - accuracy: 0.8725 - val_loss: 0.8576 - val_accuracy: 0.7776 - 18s/epoch - 20ms/step\n",
      "196/196 - 1s - loss: 0.8576 - accuracy: 0.7776 - 1s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "#CNN optimized for CIFAR10\n",
    "batch_size=64\n",
    "train_ds, test_ds = tfds.load('cifar10', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(partial(prep_pixels, depth=10)).cache().shuffle(100).batch(64).prefetch(tf.data.experimental.AUTOTUNE).repeat()\n",
    "test = test_ds.map(partial(prep_pixels, depth=10)).cache().batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "epochs=10\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model(train, test, epochs, n_channels=3, depth=10, model_name='cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce19beea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "937/937 - 17s - loss: 1.1099 - accuracy: 0.6323 - val_loss: 0.7921 - val_accuracy: 0.7391 - 17s/epoch - 18ms/step\n",
      "Epoch 2/20\n",
      "937/937 - 14s - loss: 0.7169 - accuracy: 0.7573 - val_loss: 0.6911 - val_accuracy: 0.7711 - 14s/epoch - 15ms/step\n",
      "Epoch 3/20\n",
      "937/937 - 15s - loss: 0.6442 - accuracy: 0.7773 - val_loss: 0.6569 - val_accuracy: 0.7747 - 15s/epoch - 16ms/step\n",
      "Epoch 4/20\n",
      "937/937 - 15s - loss: 0.6060 - accuracy: 0.7904 - val_loss: 0.6238 - val_accuracy: 0.7868 - 15s/epoch - 16ms/step\n",
      "Epoch 5/20\n",
      "937/937 - 16s - loss: 0.5812 - accuracy: 0.7977 - val_loss: 0.6081 - val_accuracy: 0.7910 - 16s/epoch - 17ms/step\n",
      "Epoch 6/20\n",
      "937/937 - 18s - loss: 0.5614 - accuracy: 0.8047 - val_loss: 0.6007 - val_accuracy: 0.7942 - 18s/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "937/937 - 18s - loss: 0.5474 - accuracy: 0.8097 - val_loss: 0.5913 - val_accuracy: 0.7970 - 18s/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "937/937 - 17s - loss: 0.5360 - accuracy: 0.8129 - val_loss: 0.5859 - val_accuracy: 0.7984 - 17s/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "937/937 - 19s - loss: 0.5263 - accuracy: 0.8164 - val_loss: 0.5883 - val_accuracy: 0.7970 - 19s/epoch - 20ms/step\n",
      "Epoch 10/20\n",
      "937/937 - 18s - loss: 0.5173 - accuracy: 0.8195 - val_loss: 0.5879 - val_accuracy: 0.7930 - 18s/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "937/937 - 18s - loss: 0.5100 - accuracy: 0.8222 - val_loss: 0.5734 - val_accuracy: 0.8009 - 18s/epoch - 20ms/step\n",
      "Epoch 12/20\n",
      "937/937 - 18s - loss: 0.5033 - accuracy: 0.8237 - val_loss: 0.5716 - val_accuracy: 0.8044 - 18s/epoch - 20ms/step\n",
      "Epoch 13/20\n",
      "937/937 - 19s - loss: 0.4987 - accuracy: 0.8258 - val_loss: 0.5735 - val_accuracy: 0.8043 - 19s/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "937/937 - 19s - loss: 0.4927 - accuracy: 0.8274 - val_loss: 0.5677 - val_accuracy: 0.8067 - 19s/epoch - 20ms/step\n",
      "Epoch 15/20\n",
      "937/937 - 19s - loss: 0.4873 - accuracy: 0.8293 - val_loss: 0.5649 - val_accuracy: 0.8068 - 19s/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "937/937 - 19s - loss: 0.4833 - accuracy: 0.8302 - val_loss: 0.5700 - val_accuracy: 0.8030 - 19s/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "937/937 - 17s - loss: 0.4798 - accuracy: 0.8319 - val_loss: 0.5751 - val_accuracy: 0.8016 - 17s/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "937/937 - 16s - loss: 0.4757 - accuracy: 0.8328 - val_loss: 0.5691 - val_accuracy: 0.8090 - 16s/epoch - 17ms/step\n",
      "Epoch 19/20\n",
      "937/937 - 17s - loss: 0.4710 - accuracy: 0.8350 - val_loss: 0.5649 - val_accuracy: 0.8077 - 17s/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "937/937 - 19s - loss: 0.4691 - accuracy: 0.8353 - val_loss: 0.5629 - val_accuracy: 0.8105 - 19s/epoch - 20ms/step\n",
      "106/106 - 2s - loss: 0.5629 - accuracy: 0.8105 - 2s/epoch - 16ms/step\n"
     ]
    }
   ],
   "source": [
    "#Eurosat - VGG16\n",
    "train_ds, test_ds = tfds.load('eurosat', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(partial(prep_pixels, depth=10)).cache().shuffle(100).batch(64).prefetch(tf.data.experimental.AUTOTUNE).repeat()\n",
    "test = test_ds.map(partial(prep_pixels, depth=10)).cache().prefetch(tf.data.experimental.AUTOTUNE).batch(64)\n",
    "epochs=20\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model(train, test, epochs, n_channels=3, depth=10, model_name='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70c19377-dff3-4d54-b25d-ef6d0409f79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "937/937 - 122s - loss: 2.2969 - accuracy: 0.1107 - val_loss: 2.2951 - val_accuracy: 0.1123 - 122s/epoch - 130ms/step\n",
      "Epoch 2/20\n",
      "937/937 - 102s - loss: 2.2960 - accuracy: 0.1101 - val_loss: 2.2947 - val_accuracy: 0.1073 - 102s/epoch - 109ms/step\n",
      "Epoch 3/20\n",
      "937/937 - 92s - loss: 2.2960 - accuracy: 0.1099 - val_loss: 2.2950 - val_accuracy: 0.1073 - 92s/epoch - 99ms/step\n",
      "Epoch 4/20\n",
      "937/937 - 99s - loss: 2.2960 - accuracy: 0.1103 - val_loss: 2.2956 - val_accuracy: 0.1142 - 99s/epoch - 106ms/step\n",
      "Epoch 5/20\n",
      "937/937 - 102s - loss: 2.2959 - accuracy: 0.1096 - val_loss: 2.2959 - val_accuracy: 0.1142 - 102s/epoch - 109ms/step\n",
      "Epoch 6/20\n",
      "937/937 - 99s - loss: 2.2959 - accuracy: 0.1100 - val_loss: 2.2954 - val_accuracy: 0.1108 - 99s/epoch - 106ms/step\n",
      "Epoch 7/20\n",
      "937/937 - 92s - loss: 2.2959 - accuracy: 0.1105 - val_loss: 2.2958 - val_accuracy: 0.1108 - 92s/epoch - 98ms/step\n",
      "Epoch 8/20\n",
      "937/937 - 95s - loss: 2.2960 - accuracy: 0.1102 - val_loss: 2.2966 - val_accuracy: 0.1108 - 95s/epoch - 102ms/step\n",
      "Epoch 9/20\n",
      "937/937 - 96s - loss: 2.2960 - accuracy: 0.1102 - val_loss: 2.2968 - val_accuracy: 0.1108 - 96s/epoch - 102ms/step\n",
      "Epoch 10/20\n",
      "937/937 - 101s - loss: 2.2959 - accuracy: 0.1100 - val_loss: 2.2965 - val_accuracy: 0.1108 - 101s/epoch - 108ms/step\n",
      "Epoch 11/20\n",
      "937/937 - 53s - loss: 2.2961 - accuracy: 0.1098 - val_loss: 2.2954 - val_accuracy: 0.1108 - 53s/epoch - 57ms/step\n",
      "Epoch 12/20\n",
      "937/937 - 24s - loss: 2.2961 - accuracy: 0.1097 - val_loss: 2.2952 - val_accuracy: 0.1123 - 24s/epoch - 26ms/step\n",
      "Epoch 13/20\n",
      "937/937 - 25s - loss: 2.2961 - accuracy: 0.1095 - val_loss: 2.2953 - val_accuracy: 0.1123 - 25s/epoch - 27ms/step\n",
      "Epoch 14/20\n",
      "937/937 - 26s - loss: 2.2960 - accuracy: 0.1100 - val_loss: 2.2956 - val_accuracy: 0.1073 - 26s/epoch - 28ms/step\n",
      "Epoch 15/20\n",
      "937/937 - 27s - loss: 2.2959 - accuracy: 0.1101 - val_loss: 2.2953 - val_accuracy: 0.1073 - 27s/epoch - 28ms/step\n",
      "Epoch 16/20\n",
      "937/937 - 27s - loss: 2.2960 - accuracy: 0.1097 - val_loss: 2.2947 - val_accuracy: 0.1073 - 27s/epoch - 28ms/step\n",
      "Epoch 17/20\n",
      "937/937 - 27s - loss: 2.2960 - accuracy: 0.1103 - val_loss: 2.2944 - val_accuracy: 0.1123 - 27s/epoch - 29ms/step\n",
      "Epoch 18/20\n",
      "937/937 - 27s - loss: 2.2961 - accuracy: 0.1101 - val_loss: 2.2947 - val_accuracy: 0.1123 - 27s/epoch - 29ms/step\n",
      "Epoch 19/20\n",
      "937/937 - 27s - loss: 2.2959 - accuracy: 0.1101 - val_loss: 2.2955 - val_accuracy: 0.1073 - 27s/epoch - 29ms/step\n",
      "Epoch 20/20\n",
      "937/937 - 27s - loss: 2.2960 - accuracy: 0.1102 - val_loss: 2.2959 - val_accuracy: 0.1120 - 27s/epoch - 29ms/step\n",
      "106/106 - 3s - loss: 2.2959 - accuracy: 0.1120 - 3s/epoch - 27ms/step\n"
     ]
    }
   ],
   "source": [
    "#Eurosat - ResNet50\n",
    "train_ds, test_ds = tfds.load('eurosat', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(partial(prep_pixels, depth=10)).cache().shuffle(100).batch(64).prefetch(tf.data.experimental.AUTOTUNE).repeat()\n",
    "test = test_ds.map(partial(prep_pixels, depth=10)).cache().prefetch(tf.data.experimental.AUTOTUNE).batch(64)\n",
    "epochs=20\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model(train, test, epochs, n_channels=3, depth=10, model_name='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c7ea49-7e8d-4cd8-8c83-bebe7d420f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "937/937 - 35s - loss: 1.6672 - accuracy: 0.3930 - val_loss: 1.6172 - val_accuracy: 0.4095 - 35s/epoch - 38ms/step\n",
      "Epoch 2/20\n",
      "937/937 - 12s - loss: 1.5955 - accuracy: 0.4172 - val_loss: 1.6074 - val_accuracy: 0.4071 - 12s/epoch - 13ms/step\n",
      "Epoch 3/20\n",
      "937/937 - 12s - loss: 1.5887 - accuracy: 0.4199 - val_loss: 1.5989 - val_accuracy: 0.4151 - 12s/epoch - 13ms/step\n",
      "Epoch 4/20\n",
      "937/937 - 12s - loss: 1.5861 - accuracy: 0.4223 - val_loss: 1.6088 - val_accuracy: 0.4076 - 12s/epoch - 13ms/step\n",
      "Epoch 5/20\n",
      "937/937 - 12s - loss: 1.5861 - accuracy: 0.4209 - val_loss: 1.5935 - val_accuracy: 0.4150 - 12s/epoch - 13ms/step\n",
      "Epoch 6/20\n",
      "937/937 - 13s - loss: 1.5832 - accuracy: 0.4217 - val_loss: 1.5974 - val_accuracy: 0.4175 - 13s/epoch - 13ms/step\n",
      "Epoch 7/20\n",
      "937/937 - 13s - loss: 1.5814 - accuracy: 0.4209 - val_loss: 1.6104 - val_accuracy: 0.4052 - 13s/epoch - 14ms/step\n",
      "Epoch 8/20\n",
      "937/937 - 12s - loss: 1.5807 - accuracy: 0.4221 - val_loss: 1.5968 - val_accuracy: 0.4200 - 12s/epoch - 13ms/step\n",
      "Epoch 9/20\n",
      "937/937 - 12s - loss: 1.5801 - accuracy: 0.4222 - val_loss: 1.5980 - val_accuracy: 0.4215 - 12s/epoch - 13ms/step\n",
      "Epoch 10/20\n",
      "937/937 - 12s - loss: 1.5795 - accuracy: 0.4213 - val_loss: 1.6017 - val_accuracy: 0.4142 - 12s/epoch - 13ms/step\n",
      "Epoch 11/20\n",
      "937/937 - 13s - loss: 1.5773 - accuracy: 0.4224 - val_loss: 1.5950 - val_accuracy: 0.4213 - 13s/epoch - 14ms/step\n",
      "Epoch 12/20\n",
      "937/937 - 13s - loss: 1.5751 - accuracy: 0.4244 - val_loss: 1.5844 - val_accuracy: 0.4151 - 13s/epoch - 13ms/step\n",
      "Epoch 13/20\n",
      "937/937 - 12s - loss: 1.5734 - accuracy: 0.4257 - val_loss: 1.5967 - val_accuracy: 0.4110 - 12s/epoch - 13ms/step\n",
      "Epoch 14/20\n",
      "937/937 - 12s - loss: 1.5709 - accuracy: 0.4249 - val_loss: 1.5906 - val_accuracy: 0.4164 - 12s/epoch - 13ms/step\n",
      "Epoch 15/20\n",
      "937/937 - 12s - loss: 1.5697 - accuracy: 0.4255 - val_loss: 1.5911 - val_accuracy: 0.4185 - 12s/epoch - 13ms/step\n",
      "Epoch 16/20\n",
      "937/937 - 13s - loss: 1.5704 - accuracy: 0.4259 - val_loss: 1.5956 - val_accuracy: 0.4170 - 13s/epoch - 14ms/step\n",
      "Epoch 17/20\n",
      "937/937 - 13s - loss: 1.5698 - accuracy: 0.4258 - val_loss: 1.5959 - val_accuracy: 0.4179 - 13s/epoch - 13ms/step\n",
      "Epoch 18/20\n",
      "937/937 - 13s - loss: 1.5689 - accuracy: 0.4263 - val_loss: 1.6032 - val_accuracy: 0.4117 - 13s/epoch - 14ms/step\n",
      "Epoch 19/20\n",
      "937/937 - 12s - loss: 1.5689 - accuracy: 0.4266 - val_loss: 1.5949 - val_accuracy: 0.4201 - 12s/epoch - 13ms/step\n",
      "Epoch 20/20\n",
      "937/937 - 13s - loss: 1.5673 - accuracy: 0.4266 - val_loss: 1.5889 - val_accuracy: 0.4191 - 13s/epoch - 13ms/step\n",
      "106/106 - 1s - loss: 1.5889 - accuracy: 0.4191 - 1s/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "#Eurosat - MobileNet\n",
    "train_ds, test_ds = tfds.load('eurosat', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(partial(prep_pixels, depth=10)).cache().shuffle(100).batch(64).prefetch(tf.data.experimental.AUTOTUNE).repeat()\n",
    "test = test_ds.map(partial(prep_pixels, depth=10)).cache().prefetch(tf.data.experimental.AUTOTUNE).batch(64)\n",
    "epochs=20\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model(train, test, epochs, n_channels=3, depth=10, model_name='mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc0e7cd-de9e-4fc5-b84e-4207ddbe7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               204900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 493,700\n",
      "Trainable params: 492,804\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "937/937 - 26s - loss: 4.0075 - accuracy: 0.1649 - val_loss: 3.1353 - val_accuracy: 0.2700 - 26s/epoch - 28ms/step\n",
      "Epoch 2/20\n",
      "937/937 - 19s - loss: 2.7564 - accuracy: 0.3329 - val_loss: 2.4071 - val_accuracy: 0.4035 - 19s/epoch - 20ms/step\n",
      "Epoch 3/20\n",
      "937/937 - 19s - loss: 2.3339 - accuracy: 0.4185 - val_loss: 2.2518 - val_accuracy: 0.4366 - 19s/epoch - 21ms/step\n",
      "Epoch 4/20\n",
      "937/937 - 19s - loss: 2.0953 - accuracy: 0.4747 - val_loss: 2.2552 - val_accuracy: 0.4477 - 19s/epoch - 21ms/step\n",
      "Epoch 5/20\n",
      "937/937 - 19s - loss: 1.9269 - accuracy: 0.5174 - val_loss: 2.1668 - val_accuracy: 0.4774 - 19s/epoch - 21ms/step\n",
      "Epoch 6/20\n",
      "937/937 - 21s - loss: 1.7931 - accuracy: 0.5508 - val_loss: 2.1528 - val_accuracy: 0.4814 - 21s/epoch - 23ms/step\n",
      "Epoch 7/20\n",
      "937/937 - 21s - loss: 1.6766 - accuracy: 0.5815 - val_loss: 2.0586 - val_accuracy: 0.5064 - 21s/epoch - 23ms/step\n",
      "Epoch 8/20\n",
      "937/937 - 20s - loss: 1.6006 - accuracy: 0.5993 - val_loss: 2.0811 - val_accuracy: 0.5034 - 20s/epoch - 21ms/step\n",
      "Epoch 9/20\n",
      "937/937 - 21s - loss: 1.5160 - accuracy: 0.6233 - val_loss: 2.0916 - val_accuracy: 0.5118 - 21s/epoch - 22ms/step\n",
      "Epoch 10/20\n",
      "937/937 - 20s - loss: 1.4734 - accuracy: 0.6340 - val_loss: 2.0670 - val_accuracy: 0.5212 - 20s/epoch - 22ms/step\n",
      "Epoch 11/20\n",
      "937/937 - 21s - loss: 1.4002 - accuracy: 0.6543 - val_loss: 2.1353 - val_accuracy: 0.5198 - 21s/epoch - 22ms/step\n",
      "Epoch 12/20\n",
      "937/937 - 20s - loss: 1.3517 - accuracy: 0.6677 - val_loss: 2.1894 - val_accuracy: 0.5084 - 20s/epoch - 22ms/step\n",
      "Epoch 13/20\n",
      "937/937 - 20s - loss: 1.3170 - accuracy: 0.6789 - val_loss: 2.1277 - val_accuracy: 0.5258 - 20s/epoch - 21ms/step\n",
      "Epoch 14/20\n",
      "937/937 - 20s - loss: 1.2767 - accuracy: 0.6893 - val_loss: 2.1951 - val_accuracy: 0.5205 - 20s/epoch - 22ms/step\n",
      "Epoch 15/20\n",
      "937/937 - 21s - loss: 1.2553 - accuracy: 0.6969 - val_loss: 2.1578 - val_accuracy: 0.5322 - 21s/epoch - 22ms/step\n",
      "Epoch 16/20\n",
      "937/937 - 21s - loss: 1.2234 - accuracy: 0.7076 - val_loss: 2.1857 - val_accuracy: 0.5254 - 21s/epoch - 23ms/step\n",
      "Epoch 17/20\n",
      "937/937 - 21s - loss: 1.1891 - accuracy: 0.7164 - val_loss: 2.2352 - val_accuracy: 0.5197 - 21s/epoch - 22ms/step\n",
      "Epoch 18/20\n",
      "937/937 - 21s - loss: 1.1696 - accuracy: 0.7211 - val_loss: 2.2134 - val_accuracy: 0.5284 - 21s/epoch - 22ms/step\n",
      "Epoch 19/20\n",
      "937/937 - 20s - loss: 1.1411 - accuracy: 0.7317 - val_loss: 2.2593 - val_accuracy: 0.5221 - 20s/epoch - 21ms/step\n",
      "Epoch 20/20\n",
      "937/937 - 20s - loss: 1.1289 - accuracy: 0.7354 - val_loss: 2.2404 - val_accuracy: 0.5349 - 20s/epoch - 21ms/step\n",
      "196/196 - 1s - loss: 2.2404 - accuracy: 0.5349 - 1s/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# CIFAR100 with a CNN netowork optimized for CIFAR10\n",
    "train_ds, test_ds = tfds.load('cifar100', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(partial(prep_pixels, depth=100)).cache().shuffle(100).batch(64).prefetch(tf.data.experimental.AUTOTUNE).repeat()\n",
    "test = test_ds.map(partial(prep_pixels, depth=100)).cache().batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "epochs=20\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model(train, test, epochs, n_channels=3, depth=100, model_name='cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae153b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 6)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 4, 4, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "938/938 - 18s - loss: 0.4472 - accuracy: 0.8696 - val_loss: 0.1566 - val_accuracy: 0.9575 - 18s/epoch - 19ms/step\n",
      "Epoch 2/5\n",
      "938/938 - 17s - loss: 0.2614 - accuracy: 0.9335 - val_loss: 0.2283 - val_accuracy: 0.9534 - 17s/epoch - 18ms/step\n",
      "Epoch 3/5\n",
      "938/938 - 15s - loss: 0.2919 - accuracy: 0.9313 - val_loss: 0.1523 - val_accuracy: 0.9624 - 15s/epoch - 16ms/step\n",
      "Epoch 4/5\n",
      "938/938 - 17s - loss: 0.3191 - accuracy: 0.9297 - val_loss: 0.1706 - val_accuracy: 0.9644 - 17s/epoch - 18ms/step\n",
      "Epoch 5/5\n",
      "938/938 - 16s - loss: 0.4282 - accuracy: 0.9047 - val_loss: 0.3678 - val_accuracy: 0.9126 - 16s/epoch - 17ms/step\n",
      "313/313 - 2s - loss: 0.3678 - accuracy: 0.9126 - 2s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#MNIST Dataset using tf.kersas.dataset instead of tfds\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "print(get_available_devices())\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model_rnn(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910b0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def val_load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=10) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=10)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1cee1",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "957575ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import Model, Input\n",
    "def val_vae(input_encoder):\n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(1, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    encoded = Dense(2, activation='relu')(x)\n",
    "\n",
    "    encoder = Model(inputs=inputs, outputs=encoded)\n",
    "    \n",
    "    encoded_inputs = Input(shape=(2,))\n",
    "\n",
    "    x = Dense(4, activation='relu')(encoded_inputs)\n",
    "    x = Reshape((2, 2, 1))(x)\n",
    "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((7, 7))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(inputs=encoded_inputs, outputs=decoded)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = decoder(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=Adam(0.01), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    clr = ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_delta=0.01,\n",
    "        cooldown=0,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        batch_size=256,\n",
    "        epochs=10,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=[clr])\n",
    "\n",
    "    return model, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e111061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_vae(x_train, y_train, x_test, y_test):\n",
    "    scores, histories = list(), list()\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        model, encoder, decoder=val_vae(x_train)\n",
    "        model.fit(x_train, x_train, validation_data=(x_test, x_test),  epochs=3, verbose=2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54995f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " model_4 (Functional)        (None, 2)                 34889     \n",
      "                                                                 \n",
      " model_5 (Functional)        (None, 28, 28, 1)         42417     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,306\n",
      "Trainable params: 77,082\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 10s 38ms/step - loss: 0.2815 - accuracy: 0.8059 - mse: 0.0711 - val_loss: 0.3340 - val_accuracy: 0.8072 - val_mse: 0.0869 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.2673 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2704 - val_accuracy: 0.8072 - val_mse: 0.0696 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2674 - val_accuracy: 0.8072 - val_mse: 0.0694 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2670 - val_accuracy: 0.8072 - val_mse: 0.0693 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.8088 - mse: 0.0689\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2673 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2676 - val_accuracy: 0.8072 - val_mse: 0.0692 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2671 - val_accuracy: 0.8072 - val_mse: 0.0692 - lr: 0.0050\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2668 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0050\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.2669 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2680 - val_accuracy: 0.8072 - val_mse: 0.0693 - lr: 0.0050\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2668 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0025\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2665 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0025\n",
      "Epoch 1/3\n",
      "1875/1875 - 34s - loss: 0.2672 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2672 - val_accuracy: 0.8072 - val_mse: 0.0693 - 34s/epoch - 18ms/step\n",
      "Epoch 2/3\n",
      "1875/1875 - 32s - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2671 - val_accuracy: 0.8072 - val_mse: 0.0692 - 32s/epoch - 17ms/step\n",
      "Epoch 3/3\n",
      "1875/1875 - 31s - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2666 - val_accuracy: 0.8072 - val_mse: 0.0691 - 31s/epoch - 17ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test, y_train, y_test = prep_pixels2(x_train, x_test, y_train, y_test)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model_vae(x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
