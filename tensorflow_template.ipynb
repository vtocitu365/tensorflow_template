{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae887a50",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b1aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels2(train, test, target_train, target_test):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    #if k.image_data_format() == 'channels_first':\n",
    "    #X_train = train.reshape(train.shape[0], 1, img_rows, img_cols)\n",
    "    #X_test = test.reshape(test.shape[0], 1, img_rows, img_cols)\n",
    "    #input_shape = (1, img_rows, img_cols)\n",
    "    #else:\n",
    "    X_train = train.reshape(train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = test.reshape(test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    train_norm = X_train.astype('float32')\n",
    "    test_norm = X_test.astype('float32')\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    target_train = to_categorical(target_train)\n",
    "    target_test =  to_categorical(target_test)\n",
    "    return train_norm, test_norm, ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e8c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed36d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels(image, label):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.divide(image, 255)\n",
    "    train_norm = tf.image.resize(image, (32, 32))\n",
    "    target = tf.one_hot(label, depth=10)\n",
    "    return train_norm, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc2ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "\n",
    "# CNN model\n",
    "def val_cnn_model(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(320, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03c4c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN optimized for MNIST\n",
    "def val_cnn_mnist(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.1, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    '''model=Sequential()\n",
    "\n",
    "    #model.add(Lambda(standardize,input_shape=(28,28,1)))    \n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,n_channels)))\n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())    \n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])'''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6eee238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "#data augmentation\n",
    "'''\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)'''\n",
    "\n",
    "# CNN Optimized for CIFAR10\n",
    "def val_cnn_cifar(n_channels=3):\n",
    "    weight_decay = 1e-4\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, n_channels)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt=RMSprop(lr=0.001,decay=1e-6)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71556d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN network for images\n",
    "def val_rnn_model(x_train):\n",
    "    '''i = Input(shape=x_train[0].shape)\n",
    "    x = LSTM(128)(i)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model=Model(i, x)'''\n",
    "    \n",
    "    model=Sequential()\n",
    "    #model.add(Input(shape=x_train[0].shape))\n",
    "    model.add(LSTM(128, input_shape=x_train[0].shape))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ee9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained MobileNet network for image recognition\n",
    "def val_mn_model(n_channels=3):\n",
    "    bottom_model = MobileNet(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)#top_model = Dense(1024, activation='relu')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95f3892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained VGG Network for image recognition\n",
    "def val_vgg_model(n_channels=3):\n",
    "    bottom_model = VGG16(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f8dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained ResNet Network for image recognition\n",
    "def val_resnet_model(n_channels=3):\n",
    "    bottom_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80756af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_model(train, test, n_channels=3, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model=val_cnn_mnist(n_channels) # 3 epochs\n",
    "    #model=val_cnn_cifar(n_channels) #Needs 80pct accuracy Eurosat gets > 80 at epoch 2 and overfits afterwards\n",
    "    #Needs 80pct accuracy CIFAR10 gets > 80 at epoch 5 for train and epoch 6 for validation. Each epoch is 12 min\n",
    "    #model=val_vgg_model(n_channels) # Reached 80pct at epoch 6. 10 minutes per epoch\n",
    "    model.fit(train, batch_size=128, epochs=10, steps_per_epoch=60000 // 64, validation_data=test, verbose=2)\n",
    "    _, acc = model.evaluate(test, verbose=2)\n",
    "    scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92af20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_rnn(x_train, y_train, x_test, y_test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model=val_rnn_model(x_train)\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, verbose=2)\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56da8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, Y_train) = train_ds\n",
    "#(X_test, Y_test) = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce19beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 - 17s - loss: 0.2407 - accuracy: 0.9301 - val_loss: 0.1004 - val_accuracy: 0.9719\n",
      "Epoch 2/10\n",
      "937/937 - 12s - loss: 0.1135 - accuracy: 0.9704 - val_loss: 0.1080 - val_accuracy: 0.9742\n",
      "Epoch 3/10\n",
      "937/937 - 13s - loss: 0.1119 - accuracy: 0.9724 - val_loss: 0.0990 - val_accuracy: 0.9761\n",
      "Epoch 4/10\n",
      "937/937 - 12s - loss: 0.0902 - accuracy: 0.9775 - val_loss: 0.1166 - val_accuracy: 0.9717\n",
      "Epoch 5/10\n",
      "937/937 - 13s - loss: 0.0947 - accuracy: 0.9775 - val_loss: 0.1372 - val_accuracy: 0.9640\n",
      "Epoch 6/10\n",
      "937/937 - 12s - loss: 0.1013 - accuracy: 0.9774 - val_loss: 0.1409 - val_accuracy: 0.9685\n",
      "Epoch 7/10\n",
      "937/937 - 13s - loss: 0.1277 - accuracy: 0.9737 - val_loss: 0.1764 - val_accuracy: 0.9721\n",
      "Epoch 8/10\n",
      "937/937 - 12s - loss: 0.4645 - accuracy: 0.8570 - val_loss: 2.1209 - val_accuracy: 0.1808\n",
      "Epoch 9/10\n",
      "937/937 - 12s - loss: 2.3037 - accuracy: 0.1067 - val_loss: 2.3036 - val_accuracy: 0.1009\n",
      "Epoch 10/10\n",
      "937/937 - 13s - loss: 2.3050 - accuracy: 0.1054 - val_loss: 2.3032 - val_accuracy: 0.1009\n",
      "157/157 - 1s - loss: 2.3032 - accuracy: 0.1009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10090000182390213]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load datasets\n",
    "#train_ds, test_ds=tf.keras.datasets.mnist.load_data()\n",
    "#train_ds, test_ds = tfds.load('eurosat', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train_ds, test_ds = tfds.load('mnist', split=['train','test'], as_supervised=True) # 3 Epochs\n",
    "#train_ds, test_ds = tfds.load('cifar10', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "evaluate_image_model(train, test, n_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae153b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 59s - loss: 0.5603 - accuracy: 0.8167 - val_loss: 0.1987 - val_accuracy: 0.9404\n",
      "Epoch 2/10\n",
      "1875/1875 - 68s - loss: 0.1446 - accuracy: 0.9565 - val_loss: 0.1134 - val_accuracy: 0.9670\n",
      "Epoch 3/10\n",
      "1875/1875 - 63s - loss: 0.0965 - accuracy: 0.9707 - val_loss: 0.0931 - val_accuracy: 0.9722\n",
      "Epoch 4/10\n",
      "1875/1875 - 82s - loss: 0.0745 - accuracy: 0.9782 - val_loss: 0.0747 - val_accuracy: 0.9785\n",
      "Epoch 5/10\n",
      "1875/1875 - 92s - loss: 0.0607 - accuracy: 0.9819 - val_loss: 0.0598 - val_accuracy: 0.9820\n",
      "Epoch 6/10\n",
      "1875/1875 - 96s - loss: 0.0504 - accuracy: 0.9845 - val_loss: 0.0569 - val_accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "1875/1875 - 89s - loss: 0.0439 - accuracy: 0.9872 - val_loss: 0.0537 - val_accuracy: 0.9835\n",
      "Epoch 8/10\n",
      "1875/1875 - 98s - loss: 0.0383 - accuracy: 0.9880 - val_loss: 0.0489 - val_accuracy: 0.9847\n",
      "Epoch 9/10\n",
      "1875/1875 - 74s - loss: 0.0322 - accuracy: 0.9904 - val_loss: 0.0478 - val_accuracy: 0.9847\n",
      "Epoch 10/10\n",
      "1875/1875 - 70s - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.0574 - val_accuracy: 0.9831\n",
      "313/313 - 8s - loss: 0.0574 - accuracy: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9830999970436096]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "#train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "#test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "evaluate_image_model_rnn(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f1040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910b0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def val_load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=10) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=10)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4ecba",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a7db3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# LSTM model for NLP\n",
    "def lstm_sequence_model(maxlen,\n",
    "                   max_features,\n",
    "                   embed_size,\n",
    "                   embedding_matrix,\n",
    "                   metrics):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        embeddings = [embedding_matrix]\n",
    "        output_dim = embedding_matrix.shape[1]\n",
    "    else:\n",
    "        embeddings = None\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True))\n",
    "    model.add(Reshape((maxlen, embed_size)))\n",
    "    model.add(Bidirectional(LSTM(27, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(20, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    '''model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.Input(shape=maxlen),\n",
    "        tf.keras.layers.Embedding(max_features, \n",
    "                                  embed_size, \n",
    "                                  weights=[embedding_matrix], \n",
    "                                  trainable=True),\n",
    "        #tf.keras.layers.Reshape((max_len, embed_size))\n",
    "        #tf.keras.layers.Bidirectional(\n",
    "        #    tf.keras.layers.LSTM(27, activation='relu', return_sequences = True)),\n",
    "        #tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        #tf.keras.layers.Dense(16, activation='relu'),\n",
    "        #tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])'''\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.1, epsilon=0.01)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e1942ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "# Loading Glove Model\n",
    "def load_glove_model(File):\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model\n",
    "\n",
    "# NLP classification code\n",
    "def val_nlp_classification(X_train, X_test, Y_train, Y_test, n_folds):\n",
    "    #set maxlen based on right edge of question length histogram \n",
    "    maxlen = 250\n",
    "\n",
    "    #arbitrary choice of top 25000 words\n",
    "    max_features=25000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, oov_token=\"<oov>\", filters='\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"', split=\" \")\n",
    "\n",
    "    tokenizer.fit_on_texts(np.concatenate([X_train, X_test]))\n",
    "    train_df = tokenizer.texts_to_sequences(X_train)\n",
    "    train_df = pad_sequences(train_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    test_df = tokenizer.texts_to_sequences(X_test)\n",
    "    test_df = pad_sequences(test_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "\n",
    "\n",
    "    EMBEDDING_FILE = 'C:/Users/vtoci/Documents/glove.6B.50d.txt'\n",
    "\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    #open the embedding file\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "    \n",
    "\n",
    "    #glove_embedding_index = load_glove_model('glove.txt')\n",
    "\n",
    "    # get the mean and standard deviation of the embeddings weights\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    #add the missing words to the embeddings and generate the random values\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    METRICS = [\n",
    "        tf.keras.metrics.AUC(name='roc-auc'),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name=\"recall\")\n",
    "              ]\n",
    "    val_metrics=BinaryAccuracy(name='accuracy')\n",
    "    \n",
    "\n",
    "    print(f\"Maximum sequence length: {maxlen}\")\n",
    "    print(f\"Number of words in the embedding: {max_features}\")\n",
    "    print(f\"Number of words in the vocabulary: {len(tokenizer.word_index)}\")\n",
    "    print(f\"Number of features per embedding: {embed_size}\")\n",
    "    #embedding_matrix = tf.convert_to_tensor(embedding_matrix)\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model = lstm_sequence_model(maxlen, max_features, embed_size, embedding_matrix, val_metrics)\n",
    "    model.fit(train_df, Y_train, epochs=10, batch_size=64, validation_data=(test_df, Y_test))\n",
    "    scores = list()\n",
    "    #, acc = model.evaluate(test_df, verbose=2)\n",
    "    #scores.append(acc)\n",
    "    #ss_predictions = ss.copy()\n",
    "    #ss_predictions['prediction'] = preds\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7c0b83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Generate word cloud to embed the model\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embedding(file):\n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "def make_tokenizer(texts, len_voc):\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "def modify_sentence(sentence, synonyms, p=0.5):\n",
    "    for i in range(len(sentence)):\n",
    "        if np.random.random() > p:\n",
    "            try:\n",
    "                syns = synonyms[sentence[i]]\n",
    "                sentence[i] = np.random.choice(syns)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return sentence\n",
    "\n",
    "def val_nlp_we_da(X, Y):\n",
    "    synonyms_number = 5\n",
    "    word_number = 20000\n",
    "    np.random.seed(100)\n",
    "    len_voc = 100000\n",
    "    glove = load_embedding('C:/Users/vtoci/Documents/glove.6B.50d.txt')\n",
    "    tokenizer = make_tokenizer(X, len_voc)\n",
    "    train_df = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(train_df, maxlen=70, padding=\"post\", truncating='post')\n",
    "    #X = pad_sequences(X, 70)\n",
    "    index_word = {0: ''}\n",
    "    for word in tokenizer.word_index.keys():\n",
    "        index_word[tokenizer.word_index[word]] = word\n",
    "    embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)\n",
    "    nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat) \n",
    "    neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "    synonyms = {x[0]: x[1:] for x in neighbours_mat}\n",
    "    for x in np.random.randint(1, word_number, 10):\n",
    "        print(f\"{index_word[x]} : {[index_word[synonyms[x][i]] for i in range(synonyms_number-1)]}\")\n",
    "    X_pos = X[Y==1]\n",
    "    indexes = np.random.randint(0, X_pos.shape[0], 10)\n",
    "    for x in X_pos[indexes]:\n",
    "        sample =  np.trim_zeros(x)\n",
    "        sentence = ' '.join([index_word[x] for x in sample])\n",
    "\n",
    "        modified = modify_sentence(sample, synonyms)\n",
    "        sentence_m = ' '.join([index_word[x] for x in modified])\n",
    "\n",
    "        print(' ')\n",
    "    X_gen = np.array([modify_sentence(x, synonyms) for x in X_pos[indexes]])\n",
    "    y_gen = np.ones(len(Y))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6035870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 250\n",
      "Number of words in the embedding: 25000\n",
      "Number of words in the vocabulary: 124253\n",
      "Number of features per embedding: 50\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 75s 191ms/step - loss: 0.5377 - accuracy: 0.6993 - val_loss: 0.3873 - val_accuracy: 0.8251\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 75s 191ms/step - loss: 0.3707 - accuracy: 0.8373 - val_loss: 0.3620 - val_accuracy: 0.8314\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.3103 - accuracy: 0.8695 - val_loss: 0.3338 - val_accuracy: 0.8526\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 73s 188ms/step - loss: 0.2771 - accuracy: 0.8852 - val_loss: 0.3345 - val_accuracy: 0.8564\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 0.2393 - accuracy: 0.9052 - val_loss: 0.3513 - val_accuracy: 0.8527\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2034 - accuracy: 0.9212 - val_loss: 0.3528 - val_accuracy: 0.8597\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 83s 212ms/step - loss: 0.1733 - accuracy: 0.9331 - val_loss: 0.3575 - val_accuracy: 0.8523\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 76s 194ms/step - loss: 0.1449 - accuracy: 0.9452 - val_loss: 0.3963 - val_accuracy: 0.8542\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1142 - accuracy: 0.9562 - val_loss: 0.3821 - val_accuracy: 0.8547\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 108s 276ms/step - loss: 0.0981 - accuracy: 0.9643 - val_loss: 0.5038 - val_accuracy: 0.8490\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def evaluate_model(train, test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    X_train, Y_train = tfds.as_numpy(train)\n",
    "    X_test, Y_test = tfds.as_numpy(test)\n",
    "    #Y_train = Y_train.astype('str').tolist()\n",
    "    #Y_test = Y_test.astype('str').tolist()\n",
    "    X_train=pd.DataFrame(X_train, columns=['Text'])\n",
    "    X_train = X_train['Text'].str.decode(\"utf-8\")\n",
    "    X_test=pd.DataFrame(X_test, columns=['Text'])\n",
    "    X_test = X_test['Text'].str.decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = np.array(label_encoder.fit_transform(Y_train))\n",
    "    Y_test = np.array(label_encoder.fit_transform(Y_test))\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    #X_train, Y_train = val_nlp_we_da(X_train, Y_train)\n",
    "    scores=val_nlp_classification(X_train, X_test, Y_train, Y_test, n_folds)\n",
    "    return scores\n",
    "\n",
    "def evaluate_data_augment_model(train, test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    X_train, Y_train = tfds.as_numpy(train)\n",
    "    X_test, Y_test = tfds.as_numpy(test)\n",
    "    #Y_train = Y_train.astype('str').tolist()\n",
    "    #Y_test = Y_test.astype('str').tolist()\n",
    "    X_train=pd.DataFrame(X_train, columns=['Text'])\n",
    "    X_train = X_train['Text'].str.decode(\"utf-8\")\n",
    "    X_test=pd.DataFrame(X_test, columns=['Text'])\n",
    "    X_test = X_test['Text'].str.decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = np.array(label_encoder.fit_transform(Y_train))\n",
    "    Y_test = np.array(label_encoder.fit_transform(Y_test))\n",
    "    print(set(Y_test))\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    val_nlp_we_da(X_train, Y_train)\n",
    "    return\n",
    "\n",
    "train_ds, test_ds = tfds.load('imdb_reviews', split=['train', 'test'], batch_size=-1, as_supervised=True)\n",
    "scores = evaluate_model(train_ds, test_ds)\n",
    "#evaluate_data_augment_model(train_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede868c",
   "metadata": {},
   "source": [
    "## Text2Score\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "import tensorflow.keras.layers as layer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "max_len = 30\n",
    "tk = text.Tokenizer(num_words=50000)\n",
    "tk.fit_on_texts(df_all['text'].str.lower().tolist())\n",
    "X = tk.texts_to_sequences(df_all['text'].str.lower().values)\n",
    "X = sequence.pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "df_train = df_all[df_all['deal_probability'].notnull()]\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X[:len(df_train)], df_train['deal_probability'].values, test_size=0.01)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for sentiment analysis\n",
    "def vtoc_sentiment_model(vocab_size, embed_size, max_len):\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "    x = tf.keras.layers.Embedding(input_dim = vocab_size, \n",
    "                              output_dim = embed_size,\n",
    "                              input_length=max_len)(inputs)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
    "    # Add a classifier\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile and Run \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def vectorize_sequences(train_data):\n",
    "    training_sentences = []\n",
    "    training_labels = []\n",
    "    \n",
    "    for sentence in train_data[0]:\n",
    "        training_sentences.append(str(sentence.numpy()))\n",
    "    for label in train_data[1]:\n",
    "        training_labels.append(str(label.numpy()))\n",
    "    return training_sentences, np.asarray(training_labels)\n",
    "\n",
    "def evaluate_sentiment_model(train_ds, test_ds):\n",
    "    X_train, Y_train = vectorize_sequences(train_ds)\n",
    "    X_test, Y_test = vectorize_sequences(test_ds)\n",
    "    #arbitrary choice of top 25000 words\n",
    "    maxlen = 250\n",
    "    max_features=25000\n",
    "    vocab_size = 2000 # The maximum number of words to keep, based on word frequency. \n",
    "    embed_size = 30   # Dimension of the dense embedding.\n",
    "    max_len = 100  \n",
    "    tokenizer = Tokenizer(num_words=max_features, oov_token=\"<oov>\", filters='\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"', split=\" \")\n",
    "    tokenizer.fit_on_texts(np.concatenate([X_train, X_test]))\n",
    "    train_df = tokenizer.texts_to_sequences(X_train)\n",
    "    train_padded = pad_sequences(train_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    test_df = tokenizer.texts_to_sequences(X_test)\n",
    "    test_padded = pad_sequences(test_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    model=vtoc_sentiment_model(vocab_size, embed_size, max_len)\n",
    "    model.summary()\n",
    "    model.fit(train_padded, Y_train, validation_data=(test_padded, Y_test), epochs=5, verbose=3)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487685d",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80a6c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# Basic ANN\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if logs['accuracy'] >0.90:\n",
    "            print(\"Accuracy greater than 90%. Stopping Training.\")\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "def val_dnn_model(epochs, X_train, Y_train, X_val, Y_val, callbacks=None):\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f2324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1.4741 - accuracy: 0.5600 - val_loss: 0.6924 - val_accuracy: 0.7000\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9039 - accuracy: 0.6529 - val_loss: 0.5940 - val_accuracy: 0.7467\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9534 - accuracy: 0.6171 - val_loss: 0.5701 - val_accuracy: 0.7367\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.6943 - val_loss: 0.5851 - val_accuracy: 0.7133\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7493 - accuracy: 0.6571 - val_loss: 0.5599 - val_accuracy: 0.7333\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7379 - accuracy: 0.6814 - val_loss: 0.5614 - val_accuracy: 0.7133\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.6857 - val_loss: 0.5483 - val_accuracy: 0.7433\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6388 - accuracy: 0.7114 - val_loss: 0.5793 - val_accuracy: 0.7033\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.6914 - val_loss: 0.5507 - val_accuracy: 0.7133\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.7157 - val_loss: 0.5417 - val_accuracy: 0.7167\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.7157 - val_loss: 0.5350 - val_accuracy: 0.7333\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5999 - accuracy: 0.7043 - val_loss: 0.5403 - val_accuracy: 0.7267\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.7086 - val_loss: 0.5414 - val_accuracy: 0.7200\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7314 - val_loss: 0.5401 - val_accuracy: 0.7300\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7300 - val_loss: 0.5341 - val_accuracy: 0.7533\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7186 - val_loss: 0.5432 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.6957 - val_loss: 0.5272 - val_accuracy: 0.7533\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.7357 - val_loss: 0.5247 - val_accuracy: 0.7400\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7371 - val_loss: 0.5258 - val_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7429 - val_loss: 0.5261 - val_accuracy: 0.7300\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7300 - val_loss: 0.5281 - val_accuracy: 0.7433\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7329 - val_loss: 0.5211 - val_accuracy: 0.7633\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7371 - val_loss: 0.5100 - val_accuracy: 0.7733\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7400 - val_loss: 0.5161 - val_accuracy: 0.7767\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5400 - accuracy: 0.7243 - val_loss: 0.5126 - val_accuracy: 0.7700\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7643 - val_loss: 0.5221 - val_accuracy: 0.7633\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7500 - val_loss: 0.5047 - val_accuracy: 0.7833\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.7871 - val_loss: 0.5075 - val_accuracy: 0.7833\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7757 - val_loss: 0.5053 - val_accuracy: 0.7800\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7843 - val_loss: 0.5024 - val_accuracy: 0.7733\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7629 - val_loss: 0.5025 - val_accuracy: 0.7767\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7714 - val_loss: 0.5049 - val_accuracy: 0.7933\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7686 - val_loss: 0.4997 - val_accuracy: 0.7700\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7471 - val_loss: 0.5187 - val_accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7957 - val_loss: 0.5019 - val_accuracy: 0.7733\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.7814 - val_loss: 0.5090 - val_accuracy: 0.7567\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7829 - val_loss: 0.4993 - val_accuracy: 0.7867\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.75 - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7557 - val_loss: 0.4992 - val_accuracy: 0.7533\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7929 - val_loss: 0.4967 - val_accuracy: 0.7933\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7757 - val_loss: 0.4972 - val_accuracy: 0.7600\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.8000 - val_loss: 0.4929 - val_accuracy: 0.8033\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7729 - val_loss: 0.5005 - val_accuracy: 0.7667\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7800 - val_loss: 0.4938 - val_accuracy: 0.7733\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.8043 - val_loss: 0.4965 - val_accuracy: 0.7667\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.8029 - val_loss: 0.4930 - val_accuracy: 0.7933\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7886 - val_loss: 0.5248 - val_accuracy: 0.7367\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.8014 - val_loss: 0.4930 - val_accuracy: 0.7833\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.8029 - val_loss: 0.4924 - val_accuracy: 0.7833\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7957 - val_loss: 0.4871 - val_accuracy: 0.7933\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7886 - val_loss: 0.4993 - val_accuracy: 0.7467\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.8043 - val_loss: 0.4924 - val_accuracy: 0.7667\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4728 - accuracy: 0.7943 - val_loss: 0.4962 - val_accuracy: 0.7533\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.8029 - val_loss: 0.4995 - val_accuracy: 0.7667\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8143 - val_loss: 0.4919 - val_accuracy: 0.7833\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8143 - val_loss: 0.4938 - val_accuracy: 0.7967\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.7929 - val_loss: 0.4950 - val_accuracy: 0.7567\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.7929 - val_loss: 0.4932 - val_accuracy: 0.7567\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8157 - val_loss: 0.4952 - val_accuracy: 0.7533\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.8000 - val_loss: 0.5050 - val_accuracy: 0.7567\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.8057 - val_loss: 0.4916 - val_accuracy: 0.8033\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8000 - val_loss: 0.4885 - val_accuracy: 0.7733\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7971 - val_loss: 0.4976 - val_accuracy: 0.7833\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8300 - val_loss: 0.4976 - val_accuracy: 0.7400\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8029 - val_loss: 0.4936 - val_accuracy: 0.8033\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8100 - val_loss: 0.4984 - val_accuracy: 0.7433\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8114 - val_loss: 0.4987 - val_accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8114 - val_loss: 0.5013 - val_accuracy: 0.7767\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.7986 - val_loss: 0.4906 - val_accuracy: 0.7833\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.8200 - val_loss: 0.4953 - val_accuracy: 0.7733\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8114 - val_loss: 0.5053 - val_accuracy: 0.7867\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.8171 - val_loss: 0.4968 - val_accuracy: 0.7933\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8229 - val_loss: 0.5083 - val_accuracy: 0.7467\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8143 - val_loss: 0.4960 - val_accuracy: 0.7700\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8143 - val_loss: 0.4936 - val_accuracy: 0.7933\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8186 - val_loss: 0.4904 - val_accuracy: 0.7800\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8071 - val_loss: 0.4989 - val_accuracy: 0.7533\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8171 - val_loss: 0.4999 - val_accuracy: 0.7600\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8086 - val_loss: 0.4963 - val_accuracy: 0.7467\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8329 - val_loss: 0.5069 - val_accuracy: 0.7967\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8271 - val_loss: 0.5035 - val_accuracy: 0.7433\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8271 - val_loss: 0.5015 - val_accuracy: 0.7533\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8129 - val_loss: 0.4997 - val_accuracy: 0.8067\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8343 - val_loss: 0.5036 - val_accuracy: 0.7767\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8371 - val_loss: 0.5100 - val_accuracy: 0.7967\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8257 - val_loss: 0.4951 - val_accuracy: 0.7500\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8300 - val_loss: 0.5038 - val_accuracy: 0.7700\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8357 - val_loss: 0.4959 - val_accuracy: 0.7567\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8443 - val_loss: 0.5152 - val_accuracy: 0.7600\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8257 - val_loss: 0.5092 - val_accuracy: 0.7867\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8386 - val_loss: 0.5251 - val_accuracy: 0.7667\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8329 - val_loss: 0.5140 - val_accuracy: 0.7633\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8200 - val_loss: 0.5080 - val_accuracy: 0.7667\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8371 - val_loss: 0.5001 - val_accuracy: 0.7767\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8257 - val_loss: 0.5165 - val_accuracy: 0.7967\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8300 - val_loss: 0.5141 - val_accuracy: 0.7600\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8443 - val_loss: 0.5287 - val_accuracy: 0.7600\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8429 - val_loss: 0.5122 - val_accuracy: 0.7533\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8386 - val_loss: 0.5114 - val_accuracy: 0.7800\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8300 - val_loss: 0.5291 - val_accuracy: 0.7767\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8371 - val_loss: 0.5230 - val_accuracy: 0.7467\n",
      "22/22 [==============================] - 0s 640us/step - loss: 0.3517 - accuracy: 0.8614\n",
      "Training Set:   [0.3517096936702728, 0.8614285588264465]\n",
      "10/10 [==============================] - 0s 798us/step - loss: 0.5230 - accuracy: 0.7467\n",
      "Validation Set: [0.5230162143707275, 0.746666669845581]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = tfds.load('german_credit_numeric', split=['train'], batch_size=-1, as_supervised=True)\n",
    "X=tfds.as_numpy(train[0][0])\n",
    "Y=tfds.as_numpy(train[0][1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "hist_regularized, model_regularized = val_dnn_model(100, X_train, Y_train, X_test, Y_test, callbacks=[EarlyStoppingCallback()])\n",
    "print(f\"Training Set:   {model_regularized.evaluate(X_train, Y_train)}\")\n",
    "print(f\"Validation Set: {model_regularized.evaluate(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56da78d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Serialization\\nimport tensorflow as tf\\nimport numpy as np\\n\\nPATH = '/kaggle/working/data.tfrecord'\\n\\nwith tf.io.TFRecordWriter(path=PATH) as f:\\n    f.write(b'123') # write one record\\n    f.write(b'xyz314') # write another record\\n\\nwith open(PATH, 'rb') as f:\\n    print(f.read())\\n    \\nx = tf.constant([[1, 2], [3, 4]], dtype=tf.uint8)\\nprint('x:', x, '\\n')\\n\\nx_bytes = tf.io.serialize_tensor(x)\\nprint('x_bytes:', x_bytes, '\\n')\\n\\nprint('x:', tf.io.parse_tensor(x_bytes, out_type=tf.uint8))\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "PATH = '/kaggle/working/data.tfrecord'\n",
    "\n",
    "with tf.io.TFRecordWriter(path=PATH) as f:\n",
    "    f.write(b'123') # write one record\n",
    "    f.write(b'xyz314') # write another record\n",
    "\n",
    "with open(PATH, 'rb') as f:\n",
    "    print(f.read())\n",
    "    \n",
    "x = tf.constant([[1, 2], [3, 4]], dtype=tf.uint8)\n",
    "print('x:', x, '\\n')\n",
    "\n",
    "x_bytes = tf.io.serialize_tensor(x)\n",
    "print('x_bytes:', x_bytes, '\\n')\n",
    "\n",
    "print('x:', tf.io.parse_tensor(x_bytes, out_type=tf.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242ac5",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3167760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Timeseries LSTM Autoencoder\n",
    "def vtoc_lstm_autoencoder(n_steps, n_horizon, n_features, lr):\n",
    "    serie_size=n_steps\n",
    "    encoder_decoder = Sequential()\n",
    "    encoder_decoder.add(LSTM(n_steps, activation='relu', input_shape=(n_steps, n_features), return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(1, activation='relu'))\n",
    "    encoder_decoder.add(RepeatVector(serie_size))\n",
    "    encoder_decoder.add(LSTM(serie_size, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(TimeDistributed(Dense(1)))\n",
    "    encoder_decoder.summary()\n",
    "\n",
    "    adam = Adam(lr)\n",
    "    encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e5c33ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "#Timeseries LSTM Model\n",
    "def vtoc_lstm_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(72, activation='relu', input_shape=(n_steps, n_features), return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=True)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128, activation='relu')),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries CNN Model\n",
    "def vtoc_cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    #tf.keras.layers.Input(shape=(n_steps, n_features)),\n",
    "    #tf.keras.layers.Flatten(input_shape=(n_steps, n_features)),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Flatten()),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss= Huber()\n",
    "    optimizer =Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries DNN Model\n",
    "def vtoc_dnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(n_steps, n_features)), #Use with evaluate_timeseries_model\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='dnn')\n",
    "    \n",
    "    loss=tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Model\n",
    "def lstm_cnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_steps,n_features))),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(LSTM(72, activation='relu', return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=False)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Skip Model\n",
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(72, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(48, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, skip_flatten])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a260d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Prepare timeseries data\n",
    "def multi_baseline_eror(data, pred_cols):\n",
    "    df = data[pred_cols]\n",
    "    #fill nans with linear interpolation because this is how we will fill when using the data in the models.\n",
    "    df_filled = df.interpolate(\"linear\")\n",
    "    mm = MinMaxScaler()\n",
    "    df_scaled = mm.fit_transform(df_filled)\n",
    "    df_prep = pd.DataFrame(df_scaled, columns=pred_cols)\n",
    "    y_true = df_prep[pred_cols[0]]\n",
    "    y_pred_forecast = df_prep[pred_cols[1]]\n",
    "\n",
    "    ### persistence 1 day\n",
    "    #shift series by 24 hours\n",
    "    # realign y_true to have the same length and time samples\n",
    "    y_preds_persistance_1_day = y_true.shift(24).dropna()\n",
    "    persistence_1_day_mae = tf.keras.losses.MAE(y_true[y_preds_persistance_1_day.index], y_preds_persistance_1_day).numpy()\n",
    "    persistence_1_day_mape = tf.keras.losses.MAPE(np.maximum(y_true[y_preds_persistance_1_day.index], 1e-5), np.maximum(y_preds_persistance_1_day, 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ### persistence 3 day average\n",
    "    #shift by 1, 2, 3 days. Realign to have same lengths. Average days and calcualte MAE.\n",
    "\n",
    "    shift_dfs = list()\n",
    "    for i in range(1, 4):\n",
    "        shift_dfs.append(pd.Series(y_true.shift(24 * i), name=f\"d{i}\"))\n",
    "\n",
    "    y_persistance_3d = pd.concat(shift_dfs, axis=1).dropna()\n",
    "    y_persistance_3d[\"avg\"] = (y_persistance_3d[\"d1\"] + y_persistance_3d[\"d2\"] + y_persistance_3d[\"d3\"])/3\n",
    "    d3_idx = y_persistance_3d.index\n",
    "    persistence_3day_avg_mae = tf.keras.losses.MAE(y_true[d3_idx], y_persistance_3d['avg']).numpy()\n",
    "    persistence_3day_avg_mape = tf.keras.losses.MAPE(np.maximum(y_true[d3_idx], 1e-5), np.maximum(y_persistance_3d['avg'], 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ref_error = pd.DataFrame({\n",
    "        \"Method\": [\"TSO Forecast\", \"Persistence 1 Day\", \"Persitence 3 Day Avg\"],\n",
    "        \"MAE\": [tf.keras.losses.MAE(y_true, y_pred_forecast).numpy(),\n",
    "                persistence_1_day_mae,\n",
    "                persistence_3day_avg_mae],\n",
    "        \"MAPE\":[tf.keras.losses.MAPE(np.maximum(y_true, 1e-5), np.maximum(y_pred_forecast, 1e-5)).numpy(),\n",
    "                persistence_1_day_mape,\n",
    "                persistence_3day_avg_mape]}, \n",
    "        index=[i for i in range(3)])\n",
    "    return ref_error\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "def split_data(series, train_fraq, test_len=8760):\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "\n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_window_data(dataset, look_back=1, n_features=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :n_features]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :n_features])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "    \n",
    "    #expand dimensions to 3D to fit with LSTM inputs\n",
    "    #creat the inital tensor dataset\n",
    "    if expand_dims:\n",
    "        ds = tf.expand_dims(data, axis=-1)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    \n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_multi_dataset(data, pred_cols, multivar):\n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    for column in pred_cols:\n",
    "        # Getting rid of outliers\n",
    "        data.loc[data[column] == -9999.0, column] = 0.0\n",
    "    ref_error = multi_baseline_eror(timeseries_df, pred_cols)\n",
    "    data=MinMaxScaler().fit_transform(data)\n",
    "    tf.random.set_seed(42)\n",
    "    train_fraq=0.65\n",
    "    lr = 3e-4\n",
    "    n_steps = 14#24*30\n",
    "    n_horizon = 14\n",
    "    batch_size = 64#256\n",
    "    shuffle_buffer = 100\n",
    "    if multivar:\n",
    "        n_features=len(data[0])\n",
    "    else:\n",
    "        n_features=1\n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=8760)\n",
    "    \n",
    "    (X_train, Y_train)=create_window_data(train_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_test, Y_test)=create_window_data(test_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_val, Y_val)=create_window_data(val_data, look_back=n_horizon, n_features=n_features)\n",
    "    #train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #test_ds = window_dataset(test_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #val_ds = window_dataset(val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #split_sequences(train_ds, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False)\n",
    "    #print(f\"Train Data Shape: {train_ds.shape}\")\n",
    "    #print(f\"Val Data Shape: {val_ds.shape}\")\n",
    "    #kfold = KFold(n_folds=5, shuffle=True, random_state=1)\n",
    "    model=vtoc_dnn_model(n_steps, n_horizon, n_features, lr)\n",
    "    model.summary()\n",
    "    model_hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, verbose=3)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    model.evaluate(X_train, Y_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    #_, acc = model.evaluate(test, verbose=2)\n",
    "    #scores.append(acc)\n",
    "    return model_hist.history\n",
    "\n",
    "def evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=False):\n",
    "    df_processed = create_multi_dataset(timeseries_df, pred_cols = ['wv (m/s)', 'max. wv (m/s)'], multivar=multivar)\n",
    "    print(df_processed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3b94ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path) #We load the dataset in a csv_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               25216     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 43,534\n",
      "Trainable params: 43,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa18b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import datetime\n",
    "\n",
    "def create_dataset(X, y, delay=24, lookback=48):\n",
    "    window_length = lookback + delay\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)-delay):\n",
    "        v = X.iloc[i-lookback:i].to_numpy() # every one hour, we take the past 48 hours of features\n",
    "        Xs.append(v)\n",
    "        w = y.iloc[i+delay] # Every timestep, we take the temperature the next delay (here one day)\n",
    "        ys.append(w)\n",
    "    return(np.array(Xs), np.array(ys))\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    # Getting rid of outliers\n",
    "    data.loc[data['wv (m/s)'] == -9999.0, 'wv (m/s)'] = 0.0\n",
    "    data.loc[data['max. wv (m/s)'] == -9999.0, 'max. wv (m/s)'] = 0.0\n",
    "    \n",
    "    # Taking values every hours\n",
    "    data = data[5::6]# df[start,stop,step]\n",
    "    \n",
    "    wv = data.pop('wv (m/s)')\n",
    "    max_wv = data.pop('max. wv (m/s)')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = data.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    data['Wx'] = wv*np.cos(wd_rad)\n",
    "    data['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "    # Calculate the max wind x and y components.\n",
    "    data['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "    data['max Wy'] = max_wv*np.sin(wd_rad)\n",
    "    \n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    \n",
    "    day = 24*60*60 # Time is second within a single day\n",
    "    year = 365.2425*day # Time in second withon a year\n",
    "\n",
    "    data['Day sin'] = np.sin(timestamp_s * (2*np.pi / day))\n",
    "    data['Day cos'] = np.cos(timestamp_s * (2*np.pi / day))\n",
    "    data['Year sin'] = np.sin(timestamp_s * (2*np.pi / year))\n",
    "    data['Year cos'] = np.cos(timestamp_s * (2*np.pi / year))\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def split(data):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    train_df = data.iloc[0: n * 70 //100] # \"iloc\" because we have to select the lines at the indicies 0 to int(n*0.7) compared to \"loc\"\n",
    "    val_df = data.iloc[n * 70 //100 : n * 90 //100]\n",
    "    test_df = data.iloc[n * 90 //100:]\n",
    "    \n",
    "    return(train_df, val_df, test_df)\n",
    "\n",
    "def naive_eval_arr(X, y, lookback, delay):\n",
    "    batch_maes = []\n",
    "    for i in range(0, len(X)):\n",
    "        preds = X[i, -1, 1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n",
    "        mae = np.mean(np.abs(preds - y[i]))\n",
    "        batch_maes.append(mae)\n",
    "    return(np.mean(batch_maes))\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=True, batch_size=32, step=1):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while True:\n",
    "        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n",
    "            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n",
    "        else:\n",
    "            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n",
    "                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n",
    "            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n",
    "            i+=len(rows) # rows represents the number of sample in one batch\n",
    "            \n",
    "        samples = np.zeros((len(rows), lookback//step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n",
    "        targets = np.zeros((len(rows),)) #Shape = (batch_size,)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1] #We only want to predict the temperature for now,since [1], the second column\n",
    "        return samples, targets # The yield that replace the return to create a generator and not a regular function.\n",
    "        \n",
    "def evaluate_timeseries_model(df, n_folds=5, multivar=False):\n",
    "    scores, histories = list(), list()\n",
    "    df_processed = preprocessing(df)\n",
    "    train_df, val_df, test_df = split(df_processed)\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean)/train_std # As simple as that !\n",
    "    val_df = (val_df - train_mean)/train_std\n",
    "    test_df = (test_df - train_mean)/train_std\n",
    "    lookback = 48 # Looking at all features for the past 2 days\n",
    "    delay = 24 # Trying to predict the temperature for the next day\n",
    "    batch_size = 64 # Features will be batched 32 by 32.\n",
    "    X_train, Y_train = create_dataset(train_df, train_df['T (degC)'], delay = delay, lookback = lookback)\n",
    "    X_val, Y_val = create_dataset(val_df, val_df['T (degC)'], delay = delay)\n",
    "    naive_loss_arr = naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay)\n",
    "\n",
    "    naive_loss_arr = round(naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay),2) # Round the value\n",
    "    \n",
    "\n",
    "    data_train = train_df.to_numpy()\n",
    "    (X_train, Y_train) = generator(data = data_train, lookback = lookback, delay =delay, min_index = 0, \n",
    "                                   max_index = len(data_train), shuffle = True, batch_size = batch_size)\n",
    "\n",
    "    data_val = val_df.to_numpy()\n",
    "    (X_val, Y_val) = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, \n",
    "                               max_index = len(data_val), batch_size = batch_size)\n",
    "\n",
    "    data_test = test_df.to_numpy()\n",
    "    test_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0,\n",
    "                         max_index = len(data_test), batch_size = batch_size)\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=2)\n",
    "    \n",
    "    '''for train_index, test_index in tscv.split(X_train):\n",
    "        X_train, X_test = X_train[train_index], X_train[test_index]\n",
    "        Y_train, Y_test = Y_train[train_index], Y_train[test_index]'''\n",
    "    print(naive_loss_arr)\n",
    "    lr = 3e-4\n",
    "    n_steps=48#24*30\n",
    "    n_horizon=24\n",
    "    #batch_size=64\n",
    "    if multivar:\n",
    "        n_features=5\n",
    "    else:\n",
    "        n_features=1\n",
    "    model=vtoc_cnn_model(n_steps, n_horizon, n_features, lr)\n",
    "\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, verbose=3, shuffle=True)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    train_encoded = model.predict(X_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    print('Encoded time-series shape', train_encoded.shape)\n",
    "    print('Encoded time-series sample', train_encoded[0])\n",
    "    return model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a6c93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#csv_path = \"/kaggle/input/energy-consumption-generation-prices-and-weather/energy_dataset.csv\"\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "#evaluate_timeseries_model(timeseries_df, n_folds=5, multivar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c309969",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "75d585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "def vtoc_regression_model(norm, model_type):\n",
    "    if model_type=='linear':\n",
    "        model = Sequential()\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_regression_data(dataset, target):\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset.isna().sum()\n",
    "    dataset=dataset.dropna()\n",
    "    #dataset['Origin'] = dataset['Origin'].map({1.0: 'USA', 2.0: 'Europe', 3.0: 'Japan'})\n",
    "    Y=dataset[target]\n",
    "    X=dataset.loc[:, dataset.columns != target]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "    horsepower = np.array(X_train['Horsepower']).astype('float32')\n",
    "\n",
    "    horsepower_normalizer = Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "    \n",
    "    linear_model=vtoc_regression_model(horsepower_normalizer, model_type='linear')\n",
    "    history = linear_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=2)\n",
    "    test_results = linear_model.predict(X_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f7480131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 - 0s - loss: 655.3945 - val_loss: 324.5295\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 177.0524 - val_loss: 34.7252\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 61.0334 - val_loss: 41.9099\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 27.7987 - val_loss: 17.4146\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 19.5658 - val_loss: 30.1306\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 31.2055 - val_loss: 38.6909\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 35.5456 - val_loss: 29.2807\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 35.7468 - val_loss: 31.9856\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 32.8222 - val_loss: 39.2455\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 33.6606 - val_loss: 10.7386\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 34.6217 - val_loss: 25.9623\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 35.2077 - val_loss: 23.6323\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 33.2501 - val_loss: 29.8130\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 31.0524 - val_loss: 11.1152\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 33.3188 - val_loss: 18.9865\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 30.4590 - val_loss: 8.6874\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 8.8491 - val_loss: 9.0524\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 19.1884 - val_loss: 37.1273\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 21.4964 - val_loss: 17.8741\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 18.6284 - val_loss: 17.0258\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 16.1843 - val_loss: 6.2859\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 20.4963 - val_loss: 11.1967\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 17.5334 - val_loss: 14.9765\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 18.4820 - val_loss: 19.6653\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 36.0507 - val_loss: 66.3878\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 56.8899 - val_loss: 83.5776\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 63.7196 - val_loss: 33.9487\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 11.9490 - val_loss: 13.2835\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 8.5848 - val_loss: 5.9575\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 19.9504 - val_loss: 51.3244\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 35.1979 - val_loss: 51.3070\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 32.5077 - val_loss: 22.5335\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 18.6422 - val_loss: 17.8840\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 33.6222 - val_loss: 18.1574\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 16.2964 - val_loss: 51.4237\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 56.4408 - val_loss: 96.8342\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 53.2013 - val_loss: 38.1841\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 33.1895 - val_loss: 4.1937\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 10.0125 - val_loss: 29.5371\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 21.1333 - val_loss: 21.7775\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 15.9307 - val_loss: 30.9575\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 22.1588 - val_loss: 30.3780\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 20.2239 - val_loss: 26.4455\n",
      "Epoch 44/100\n",
      "9/9 - 0s - loss: 22.9454 - val_loss: 77.4831\n",
      "Epoch 45/100\n",
      "9/9 - 0s - loss: 70.9002 - val_loss: 127.3733\n",
      "Epoch 46/100\n",
      "9/9 - 0s - loss: 92.9565 - val_loss: 60.8932\n",
      "Epoch 47/100\n",
      "9/9 - 0s - loss: 57.4234 - val_loss: 64.0249\n",
      "Epoch 48/100\n",
      "9/9 - 0s - loss: 33.8289 - val_loss: 52.2300\n",
      "Epoch 49/100\n",
      "9/9 - 0s - loss: 38.6703 - val_loss: 53.6739\n",
      "Epoch 50/100\n",
      "9/9 - 0s - loss: 37.1285 - val_loss: 46.8091\n",
      "Epoch 51/100\n",
      "9/9 - 0s - loss: 37.4622 - val_loss: 33.3877\n",
      "Epoch 52/100\n",
      "9/9 - 0s - loss: 30.2652 - val_loss: 53.2723\n",
      "Epoch 53/100\n",
      "9/9 - 0s - loss: 61.3241 - val_loss: 58.8349\n",
      "Epoch 54/100\n",
      "9/9 - 0s - loss: 57.2581 - val_loss: 70.7555\n",
      "Epoch 55/100\n",
      "9/9 - 0s - loss: 31.1789 - val_loss: 9.6790\n",
      "Epoch 56/100\n",
      "9/9 - 0s - loss: 21.0164 - val_loss: 71.5870\n",
      "Epoch 57/100\n",
      "9/9 - 0s - loss: 58.6660 - val_loss: 62.8985\n",
      "Epoch 58/100\n",
      "9/9 - 0s - loss: 57.8272 - val_loss: 5.6897\n",
      "Epoch 59/100\n",
      "9/9 - 0s - loss: 55.1155 - val_loss: 57.4548\n",
      "Epoch 60/100\n",
      "9/9 - 0s - loss: 56.1331 - val_loss: 94.2712\n",
      "Epoch 61/100\n",
      "9/9 - 0s - loss: 45.0074 - val_loss: 6.6663\n",
      "Epoch 62/100\n",
      "9/9 - 0s - loss: 14.4761 - val_loss: 7.3650\n",
      "Epoch 63/100\n",
      "9/9 - 0s - loss: 63.9725 - val_loss: 4.2809\n",
      "Epoch 64/100\n",
      "9/9 - 0s - loss: 60.4736 - val_loss: 20.9296\n",
      "Epoch 65/100\n",
      "9/9 - 0s - loss: 41.3735 - val_loss: 92.2708\n",
      "Epoch 66/100\n",
      "9/9 - 0s - loss: 60.2551 - val_loss: 48.0091\n",
      "Epoch 67/100\n",
      "9/9 - 0s - loss: 56.6736 - val_loss: 14.5567\n",
      "Epoch 68/100\n",
      "9/9 - 0s - loss: 58.5198 - val_loss: 24.1256\n",
      "Epoch 69/100\n",
      "9/9 - 0s - loss: 26.1217 - val_loss: 7.6019\n",
      "Epoch 70/100\n",
      "9/9 - 0s - loss: 33.5244 - val_loss: 4.3701\n",
      "Epoch 71/100\n",
      "9/9 - 0s - loss: 18.5606 - val_loss: 24.8644\n",
      "Epoch 72/100\n",
      "9/9 - 0s - loss: 20.8231 - val_loss: 17.8765\n",
      "Epoch 73/100\n",
      "9/9 - 0s - loss: 20.5071 - val_loss: 52.9647\n",
      "Epoch 74/100\n",
      "9/9 - 0s - loss: 63.3411 - val_loss: 38.5323\n",
      "Epoch 75/100\n",
      "9/9 - 0s - loss: 15.7627 - val_loss: 3.4774\n",
      "Epoch 76/100\n",
      "9/9 - 0s - loss: 13.8084 - val_loss: 8.9646\n",
      "Epoch 77/100\n",
      "9/9 - 0s - loss: 21.1358 - val_loss: 9.0298\n",
      "Epoch 78/100\n",
      "9/9 - 0s - loss: 26.5179 - val_loss: 44.6926\n",
      "Epoch 79/100\n",
      "9/9 - 0s - loss: 18.3886 - val_loss: 33.7790\n",
      "Epoch 80/100\n",
      "9/9 - 0s - loss: 30.6269 - val_loss: 50.6863\n",
      "Epoch 81/100\n",
      "9/9 - 0s - loss: 36.2083 - val_loss: 43.7004\n",
      "Epoch 82/100\n",
      "9/9 - 0s - loss: 37.1067 - val_loss: 21.2438\n",
      "Epoch 83/100\n",
      "9/9 - 0s - loss: 22.5696 - val_loss: 73.0858\n",
      "Epoch 84/100\n",
      "9/9 - 0s - loss: 58.9490 - val_loss: 45.2208\n",
      "Epoch 85/100\n",
      "9/9 - 0s - loss: 22.6598 - val_loss: 24.7063\n",
      "Epoch 86/100\n",
      "9/9 - 0s - loss: 21.0685 - val_loss: 18.6044\n",
      "Epoch 87/100\n",
      "9/9 - 0s - loss: 18.4560 - val_loss: 16.2684\n",
      "Epoch 88/100\n",
      "9/9 - 0s - loss: 18.7010 - val_loss: 31.5744\n",
      "Epoch 89/100\n",
      "9/9 - 0s - loss: 35.6728 - val_loss: 68.2062\n",
      "Epoch 90/100\n",
      "9/9 - 0s - loss: 52.5608 - val_loss: 8.7841\n",
      "Epoch 91/100\n",
      "9/9 - 0s - loss: 14.1526 - val_loss: 12.3261\n",
      "Epoch 92/100\n",
      "9/9 - 0s - loss: 9.5074 - val_loss: 12.4626\n",
      "Epoch 93/100\n",
      "9/9 - 0s - loss: 19.3668 - val_loss: 15.9688\n",
      "Epoch 94/100\n",
      "9/9 - 0s - loss: 32.6341 - val_loss: 36.2271\n",
      "Epoch 95/100\n",
      "9/9 - 0s - loss: 35.5505 - val_loss: 15.5593\n",
      "Epoch 96/100\n",
      "9/9 - 0s - loss: 18.3158 - val_loss: 50.9941\n",
      "Epoch 97/100\n",
      "9/9 - 0s - loss: 32.2022 - val_loss: 43.6072\n",
      "Epoch 98/100\n",
      "9/9 - 0s - loss: 56.4482 - val_loss: 50.2844\n",
      "Epoch 99/100\n",
      "9/9 - 0s - loss: 43.8876 - val_loss: 29.0410\n",
      "Epoch 100/100\n",
      "9/9 - 0s - loss: 41.5936 - val_loss: 48.9389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[77.469696],\n",
       "       [71.14866 ],\n",
       "       [76.31318 ],\n",
       "       [68.196205],\n",
       "       [72.368904],\n",
       "       [84.514786],\n",
       "       [70.25062 ],\n",
       "       [73.09792 ],\n",
       "       [72.51673 ],\n",
       "       [67.94489 ],\n",
       "       [68.79024 ],\n",
       "       [65.599   ],\n",
       "       [67.697586],\n",
       "       [71.97364 ],\n",
       "       [80.36729 ],\n",
       "       [73.08812 ],\n",
       "       [68.25088 ],\n",
       "       [78.07401 ],\n",
       "       [68.34635 ],\n",
       "       [79.209366],\n",
       "       [66.51295 ],\n",
       "       [79.367584],\n",
       "       [70.48836 ],\n",
       "       [71.486885],\n",
       "       [75.88871 ],\n",
       "       [68.15867 ],\n",
       "       [80.30823 ],\n",
       "       [76.226944],\n",
       "       [77.32481 ],\n",
       "       [70.72942 ],\n",
       "       [71.44613 ],\n",
       "       [69.95439 ],\n",
       "       [74.997086],\n",
       "       [66.39675 ],\n",
       "       [71.32725 ],\n",
       "       [72.909065],\n",
       "       [75.04423 ],\n",
       "       [70.54749 ],\n",
       "       [71.16452 ],\n",
       "       [72.27059 ],\n",
       "       [73.52431 ],\n",
       "       [68.89938 ],\n",
       "       [70.25947 ],\n",
       "       [76.23491 ],\n",
       "       [70.71241 ],\n",
       "       [73.79117 ],\n",
       "       [84.260574],\n",
       "       [81.681915],\n",
       "       [72.273   ],\n",
       "       [74.44572 ],\n",
       "       [70.20759 ],\n",
       "       [79.42744 ],\n",
       "       [67.77087 ],\n",
       "       [73.678764],\n",
       "       [76.2588  ],\n",
       "       [67.767746],\n",
       "       [65.21364 ],\n",
       "       [74.82556 ],\n",
       "       [67.174194],\n",
       "       [69.62819 ],\n",
       "       [72.503654],\n",
       "       [75.967354],\n",
       "       [70.59092 ],\n",
       "       [75.97246 ],\n",
       "       [78.39037 ],\n",
       "       [78.112404],\n",
       "       [68.090034],\n",
       "       [73.47346 ],\n",
       "       [73.187126],\n",
       "       [72.9864  ],\n",
       "       [74.41336 ],\n",
       "       [69.82517 ],\n",
       "       [76.85482 ],\n",
       "       [68.93012 ],\n",
       "       [76.99913 ],\n",
       "       [70.50494 ],\n",
       "       [70.01119 ],\n",
       "       [74.944756],\n",
       "       [73.13501 ],\n",
       "       [80.71022 ],\n",
       "       [65.658195],\n",
       "       [70.83842 ],\n",
       "       [75.67216 ],\n",
       "       [67.73141 ],\n",
       "       [71.791466],\n",
       "       [72.9093  ],\n",
       "       [81.229515],\n",
       "       [69.88189 ],\n",
       "       [68.66305 ],\n",
       "       [78.82275 ],\n",
       "       [68.5899  ],\n",
       "       [76.414925],\n",
       "       [80.22753 ],\n",
       "       [72.23815 ],\n",
       "       [71.74467 ],\n",
       "       [77.740685],\n",
       "       [81.527626],\n",
       "       [71.113045],\n",
       "       [76.43392 ],\n",
       "       [83.51917 ],\n",
       "       [70.403336],\n",
       "       [72.871765],\n",
       "       [75.36526 ],\n",
       "       [75.562355],\n",
       "       [66.15409 ],\n",
       "       [66.28005 ],\n",
       "       [67.13857 ],\n",
       "       [70.45971 ],\n",
       "       [72.51805 ],\n",
       "       [81.48104 ],\n",
       "       [77.55514 ],\n",
       "       [74.71813 ],\n",
       "       [72.33976 ],\n",
       "       [77.63308 ],\n",
       "       [77.63209 ],\n",
       "       [66.2687  ],\n",
       "       [68.31059 ],\n",
       "       [64.34451 ]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "eval_regression_data(raw_dataset, 'MPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c49fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in \"abcdefghijklmnopqrstuvwxyz0\"]\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
    "n_class = len(word_dict)\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n",
    "\n",
    "n_step = 3\n",
    "n_hidden = 128\n",
    "\n",
    "inputs = [sen[:3] for sen in seq_data]\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "predict =  sess.run([prediction], feed_dict={X: input_batch})\n",
    "print(inputs, '->', [number_dict[n] for n in predict[0]])\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]]\n",
    "        target = word_dict[seq[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "        #print(seq, len(seq))\n",
    "\n",
    "    return input_batch, target_batchtf.reset_default_graph()\n",
    "\n",
    "# Model\n",
    "X = tf.placeholder(tf.float32, [len(seq_data), n_step, n_class]) # [batch_size, n_step, n_class]\n",
    "Y = tf.placeholder(tf.float32, [len(seq_data), n_class])         # [batch_size, n_class]\n",
    "print(X.shape, Y.shape)\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_step]))\n",
    "b = tf.Variable(tf.random_normal([len(seq_data), n_class]))\n",
    "print(W.shape)\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "# outputs : [batch_size, n_step, n_hidden]\n",
    "outputs = tf.transpose(outputs, [0, 1, 2]) # [n_step, batch_size, n_hidden]\n",
    "outputs = outputs[-1] # [batch_size, n_hidden]\n",
    "print(outputs.shape)\n",
    "model = tf.reshape(tf.matmul(outputs, new_W),[9]) + new_b # model : [batch_size, n_class]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(.5).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5370c9",
   "metadata": {},
   "source": [
    "# Build Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "100bd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "    xs = tf.cast(xs, tf.float32) / 255.0\n",
    "    ys = tf.cast(ys, tf.float32)\n",
    "    ys = tf.one_hot(ys, depth=n_classes)\n",
    "    return tf.data.Dataset.from_tensor_slices((xs, ys)).map(preprocess).shuffle(len(ys)).batch(128)\n",
    "\n",
    "def val_nn(training_inputs_data, training_outputs_data, test_inputs):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    training_inputs = tensorflow.compat.v1.placeholder(shape=[None, 3], dtype=tensorflow.float32)  \n",
    "    training_outputs = tensorflow.compat.v1.placeholder(shape=[None, 1], dtype=tensorflow.float32) #Desired outputs for each input  \n",
    "    weights = tensorflow.Variable(initial_value=[[.3], [.1], [.8]], dtype=tensorflow.float32)  \n",
    "    bias = tensorflow.Variable(initial_value=[[1]], dtype=tensorflow.float32)  \n",
    "\n",
    "    af_input = tensorflow.matmul(training_inputs, weights) + bias  \n",
    "  \n",
    "    # Activation function of the output layer neuron  \n",
    "    predictions = tensorflow.nn.sigmoid(af_input)  \n",
    "    # Measuring the prediction error of the network after being trained  \n",
    "    prediction_error = tensorflow.reduce_sum(training_outputs - predictions)  \n",
    "    # Minimizing the prediction error using gradient descent optimizer  \n",
    "    \n",
    "    train_op = tensorflow.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(prediction_error) \n",
    "    # Creating a TensorFlow Session  \n",
    "    sess = tensorflow.compat.v1.Session()  \n",
    "    # Initializing the TensorFlow Variables (weights and bias)  \n",
    "    sess.run(tensorflow.compat.v1.global_variables_initializer())  \n",
    "    \n",
    "    # Training loop of the neural network  \n",
    "    for step in range(10000):  \n",
    "        sess.run(fetches=train_op, feed_dict={training_inputs: training_inputs_data, training_outputs: training_outputs_data})  \n",
    "        # Class scores of some testing data  \n",
    "    score= sess.run(fetches=predictions, feed_dict={training_inputs: [[248, 80, 68], [0, 0, 255]]})\n",
    "    predictions =  sess.run(fetches=test_inputs)\n",
    "    # Closing the TensorFlow Session to free resources  \n",
    "    sess.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "213165e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2., 13.],\n",
       "       [ 7.,  9.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "X_test=tensorflow.convert_to_tensor(value=X_test, dtype=tensorflow.float32)\n",
    "val_nn(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7a0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c624b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
