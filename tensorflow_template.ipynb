{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae887a50",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e8c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb4eee2-587d-4041-8145-6dc44d8a3cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.3\n",
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b1aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels2(train, test, target_train, target_test):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    #if k.image_data_format() == 'channels_first':\n",
    "    #X_train = train.reshape(train.shape[0], 1, img_rows, img_cols)\n",
    "    #X_test = test.reshape(test.shape[0], 1, img_rows, img_cols)\n",
    "    #input_shape = (1, img_rows, img_cols)\n",
    "    #else:\n",
    "    X_train = train.reshape(train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = test.reshape(test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    train_norm = X_train.astype('float32')\n",
    "    test_norm = X_test.astype('float32')\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    target_train = to_categorical(target_train)\n",
    "    target_test =  to_categorical(target_test)\n",
    "    return train_norm, test_norm, target_train, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed36d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels(image, label, depth=10):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.divide(image, 255)\n",
    "    train_norm = tf.image.resize(image, (32, 32))\n",
    "    target = tf.one_hot(label, depth=depth)\n",
    "    return train_norm, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc2ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "\n",
    "# CNN model\n",
    "def val_cnn_model(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(320, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c4c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN optimized for MNIST\n",
    "def val_cnn_mnist(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.1, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    '''model=Sequential()\n",
    "\n",
    "    #model.add(Lambda(standardize,input_shape=(28,28,1)))    \n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,n_channels)))\n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())    \n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])'''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eee238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "#tf.disable_v2_behavior()\n",
    "#data augmentation\n",
    "'''\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)'''\n",
    "\n",
    "# CNN Optimized for CIFAR10\n",
    "def val_cnn_cifar(n_depth, n_channels=3):\n",
    "    weight_decay = 1e-4\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, n_channels)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_depth, activation='softmax'))\n",
    "    opt=RMSprop(learning_rate=0.001,decay=1e-5)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71556d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN network for images\n",
    "def val_rnn_model(x_train):\n",
    "    '''i = Input(shape=x_train[0].shape)\n",
    "    x = LSTM(128)(i)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model=Model(i, x)'''\n",
    "    \n",
    "    model=Sequential()\n",
    "    #model.add(Input(shape=x_train[0].shape))\n",
    "    model.add(LSTM(128, input_shape=x_train[0].shape))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ee9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained MobileNet network for image recognition\n",
    "def val_mn_model(n_channels=3):\n",
    "    bottom_model = MobileNet(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)#top_model = Dense(1024, activation='relu')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f3892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained VGG Network for image recognition\n",
    "def val_vgg_model(n_channels=3):\n",
    "    bottom_model = VGG16(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained ResNet Network for image recognition\n",
    "def val_resnet_model(n_channels=3):\n",
    "    bottom_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80756af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(inputs, targets, model, loss_fn, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs)\n",
    "        loss_value = loss_fn(targets, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update the accuracy metric\n",
    "    accuracy_metric.update_state(targets, logits)\n",
    "    \n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def train_step(dataset, model, loss_fn, optimizer):\n",
    "    training_loss = tf.constant(0.0)\n",
    "    num_batches = tf.constant(0)\n",
    "    \n",
    "    for batch in dataset:\n",
    "        inputs, targets = batch\n",
    "        loss = training_step(inputs, targets, model, loss_fn, optimizer)\n",
    "        training_loss += loss\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Calculate the mean loss over all batches\n",
    "    mean_loss = training_loss / tf.cast(num_batches, dtype=tf.float32)\n",
    "    \n",
    "    # Get the current accuracy from the accuracy metric\n",
    "    current_accuracy = accuracy_metric.result()\n",
    "    \n",
    "    # Reset the accuracy metric for the next epoch\n",
    "    accuracy_metric.reset_states()\n",
    "    \n",
    "    return mean_loss, current_accuracy\n",
    "  \n",
    "def evaluate_image_model(train_dataset, val_dataset, num_epochs, n_channels=3, depth=10):\n",
    "    batch_size=64\n",
    "    history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    #model=val_cnn_mnist(n_channels) # 3 epochs\n",
    "    model=val_cnn_cifar(depth, n_channels) #Needs 80pct accuracy Eurosat gets > 80 at epoch 2 and overfits afterwards\n",
    "    #Needs 80pct accuracy CIFAR10 gets > 80 at epoch 5 for train and epoch 6 for validation. Each epoch is 12 min\n",
    "    #model=val_vgg_model(n_channels) # Reached 80pct at epoch 6. 10 minutes per epoch\n",
    "    model.fit(train_dataset, epochs=num_epochs, steps_per_epoch=60000 // 64, validation_data=val_dataset, verbose=2)\n",
    "    _, acc = model.evaluate(val_dataset, verbose=2)\n",
    "\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92af20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_rnn(x_train, y_train, x_test, y_test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model=val_rnn_model(x_train)\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, verbose=2)\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce19beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "937/937 - 23s - loss: 0.9713 - accuracy: 0.7129 - val_loss: 1.5119 - val_accuracy: 0.6342 - 23s/epoch - 25ms/step\n",
      "Epoch 2/20\n",
      "937/937 - 21s - loss: 0.4833 - accuracy: 0.8594 - val_loss: 0.6936 - val_accuracy: 0.8204 - 21s/epoch - 22ms/step\n",
      "Epoch 3/20\n",
      "937/937 - 19s - loss: 0.3493 - accuracy: 0.9073 - val_loss: 1.0624 - val_accuracy: 0.7886 - 19s/epoch - 20ms/step\n",
      "Epoch 4/20\n",
      "937/937 - 20s - loss: 0.2906 - accuracy: 0.9269 - val_loss: 0.5819 - val_accuracy: 0.8729 - 20s/epoch - 21ms/step\n",
      "Epoch 5/20\n",
      "937/937 - 18s - loss: 0.2569 - accuracy: 0.9400 - val_loss: 0.6128 - val_accuracy: 0.8327 - 18s/epoch - 20ms/step\n",
      "Epoch 6/20\n",
      "937/937 - 18s - loss: 0.2411 - accuracy: 0.9478 - val_loss: 0.7637 - val_accuracy: 0.8353 - 18s/epoch - 20ms/step\n",
      "Epoch 7/20\n",
      "937/937 - 19s - loss: 0.2210 - accuracy: 0.9539 - val_loss: 0.3325 - val_accuracy: 0.9314 - 19s/epoch - 20ms/step\n",
      "Epoch 8/20\n",
      "937/937 - 18s - loss: 0.2094 - accuracy: 0.9593 - val_loss: 0.3418 - val_accuracy: 0.9304 - 18s/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "937/937 - 20s - loss: 0.1972 - accuracy: 0.9638 - val_loss: 0.4246 - val_accuracy: 0.9065 - 20s/epoch - 21ms/step\n",
      "Epoch 10/20\n",
      "937/937 - 23s - loss: 0.1968 - accuracy: 0.9645 - val_loss: 0.3586 - val_accuracy: 0.9212 - 23s/epoch - 24ms/step\n",
      "Epoch 11/20\n",
      "937/937 - 19s - loss: 0.1800 - accuracy: 0.9681 - val_loss: 0.8073 - val_accuracy: 0.8868 - 19s/epoch - 20ms/step\n",
      "Epoch 12/20\n",
      "937/937 - 77s - loss: 0.1811 - accuracy: 0.9701 - val_loss: 0.3951 - val_accuracy: 0.9151 - 77s/epoch - 82ms/step\n",
      "Epoch 13/20\n",
      "937/937 - 96s - loss: 0.1776 - accuracy: 0.9710 - val_loss: 0.7018 - val_accuracy: 0.8742 - 96s/epoch - 102ms/step\n",
      "Epoch 14/20\n",
      "937/937 - 109s - loss: 0.1688 - accuracy: 0.9738 - val_loss: 0.6595 - val_accuracy: 0.8582 - 109s/epoch - 117ms/step\n",
      "Epoch 15/20\n",
      "937/937 - 86s - loss: 0.1711 - accuracy: 0.9742 - val_loss: 0.4689 - val_accuracy: 0.9194 - 86s/epoch - 92ms/step\n",
      "Epoch 16/20\n",
      "937/937 - 86s - loss: 0.1634 - accuracy: 0.9753 - val_loss: 0.5744 - val_accuracy: 0.9010 - 86s/epoch - 92ms/step\n",
      "Epoch 17/20\n",
      "937/937 - 100s - loss: 0.1629 - accuracy: 0.9766 - val_loss: 0.3383 - val_accuracy: 0.9391 - 100s/epoch - 107ms/step\n",
      "Epoch 18/20\n",
      "937/937 - 98s - loss: 0.1567 - accuracy: 0.9782 - val_loss: 0.4418 - val_accuracy: 0.9188 - 98s/epoch - 105ms/step\n",
      "Epoch 19/20\n",
      "937/937 - 98s - loss: 0.1592 - accuracy: 0.9777 - val_loss: 0.4028 - val_accuracy: 0.9253 - 98s/epoch - 104ms/step\n",
      "Epoch 20/20\n",
      "937/937 - 89s - loss: 0.1547 - accuracy: 0.9783 - val_loss: 0.4822 - val_accuracy: 0.9167 - 89s/epoch - 95ms/step\n",
      "106/106 - 3s - loss: 0.4822 - accuracy: 0.9167 - 3s/epoch - 29ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# Load datasets\n",
    "#train_ds, test_ds=tf.keras.datasets.mnist.load_data()\n",
    "train_ds, test_ds = tfds.load('eurosat', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "#train_ds, test_ds = tfds.load('mnist', split=['train','test'], as_supervised=True) # 3 Epochs\n",
    "#train_ds, test_ds = tfds.load('cifar100', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "epochs=20\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model(train, test, epochs, n_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae153b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n",
      "Epoch 1/10\n",
      "1875/1875 - 16s - loss: 0.5760 - accuracy: 0.8105 - val_loss: 0.1612 - val_accuracy: 0.9521 - 16s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 10s - loss: 0.1408 - accuracy: 0.9575 - val_loss: 0.0909 - val_accuracy: 0.9736 - 10s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 10s - loss: 0.0926 - accuracy: 0.9725 - val_loss: 0.0812 - val_accuracy: 0.9767 - 10s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 10s - loss: 0.0727 - accuracy: 0.9778 - val_loss: 0.0752 - val_accuracy: 0.9777 - 10s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 11s - loss: 0.0583 - accuracy: 0.9824 - val_loss: 0.0704 - val_accuracy: 0.9784 - 11s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 10s - loss: 0.0485 - accuracy: 0.9853 - val_loss: 0.0639 - val_accuracy: 0.9808 - 10s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 10s - loss: 0.0406 - accuracy: 0.9877 - val_loss: 0.0603 - val_accuracy: 0.9823 - 10s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 10s - loss: 0.0355 - accuracy: 0.9885 - val_loss: 0.0534 - val_accuracy: 0.9828 - 10s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 10s - loss: 0.0328 - accuracy: 0.9901 - val_loss: 0.0473 - val_accuracy: 0.9859 - 10s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 10s - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.0526 - val_accuracy: 0.9852 - 10s/epoch - 6ms/step\n",
      "313/313 - 1s - loss: 0.0526 - accuracy: 0.9852 - 1s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "#train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "#test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "print(get_available_devices())\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model_rnn(x_train, y_train, x_test, y_test)\n",
    "    #tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910b0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def val_load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=10) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=10)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1cee1",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957575ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import Model, Input\n",
    "def val_vae(input_encoder):\n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(1, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    encoded = Dense(2, activation='relu')(x)\n",
    "\n",
    "    encoder = Model(inputs=inputs, outputs=encoded)\n",
    "    \n",
    "    encoded_inputs = Input(shape=(2,))\n",
    "\n",
    "    x = Dense(4, activation='relu')(encoded_inputs)\n",
    "    x = Reshape((2, 2, 1))(x)\n",
    "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((7, 7))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(inputs=encoded_inputs, outputs=decoded)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = decoder(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=Adam(0.01), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    clr = ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_delta=0.01,\n",
    "        cooldown=0,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        batch_size=256,\n",
    "        epochs=50,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=[clr])\n",
    "\n",
    "    return model, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e111061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_vae(x_train, y_train, x_test, y_test):\n",
    "    scores, histories = list(), list()\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        model, encoder, decoder=val_vae(x_train)\n",
    "        model.fit(x_train, x_train, validation_data=(x_test, x_test),  epochs=3, verbose=2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54995f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 2)                 34889     \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 28, 28, 1)         42417     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,306\n",
      "Trainable params: 77,082\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "235/235 [==============================] - 12s 42ms/step - loss: 0.2748 - accuracy: 0.8067 - mse: 0.0710 - val_loss: 0.2795 - val_accuracy: 0.8072 - val_mse: 0.0715 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.2674 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2699 - val_accuracy: 0.8072 - val_mse: 0.0698 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2682 - val_accuracy: 0.8072 - val_mse: 0.0693 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2671 - accuracy: 0.8088 - mse: 0.0689 - val_loss: 0.2669 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 9s 37ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2666 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2669 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2676 - val_accuracy: 0.8072 - val_mse: 0.0694 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.2669 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2666 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 0.0025\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 0.0025\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2668 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 0.0025\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0012\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0012\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 0.0012\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 6.2500e-04\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 6.2500e-04\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 6.2500e-04\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 3.1250e-04\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 3.1250e-04\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 3.1250e-04\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.5625e-04\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2664 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.5625e-04\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 1.5625e-04\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 7.8125e-05\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 7.8125e-05\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2667 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 7.8125e-05\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 3.9062e-05\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0691 - lr: 3.9062e-05\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 3.9062e-05\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.9531e-05\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.9531e-05\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.9531e-05\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 9.7656e-06\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 9.7656e-06\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 9.7656e-06\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 4.8828e-06\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 4.8828e-06\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 4.8828e-06\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 2.4414e-06\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 2.4414e-06\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 2.4414e-06\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.2207e-06\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.2207e-06\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.2207e-06\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 6.1035e-07\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 6.1035e-07\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 6.1035e-07\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 3.0518e-07\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 3.0518e-07\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 3.0518e-07\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - lr: 1.5259e-07\n",
      "Epoch 1/3\n",
      "1875/1875 - 33s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - 33s/epoch - 17ms/step\n",
      "Epoch 2/3\n",
      "1875/1875 - 32s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - 32s/epoch - 17ms/step\n",
      "Epoch 3/3\n",
      "1875/1875 - 33s - loss: 0.2666 - accuracy: 0.8088 - mse: 0.0688 - val_loss: 0.2663 - val_accuracy: 0.8072 - val_mse: 0.0690 - 33s/epoch - 17ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test, y_train, y_test = prep_pixels2(x_train, x_test, y_train, y_test)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_image_model_vae(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4ecba",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7db3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# LSTM model for NLP\n",
    "def lstm_sequence_model(maxlen,\n",
    "                   max_features,\n",
    "                   embed_size,\n",
    "                   embedding_matrix,\n",
    "                   metrics):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        embeddings = [embedding_matrix]\n",
    "        output_dim = embedding_matrix.shape[1]\n",
    "    else:\n",
    "        embeddings = None\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, trainable=True))\n",
    "    model.add(Reshape((maxlen, embed_size)))\n",
    "    model.add(Bidirectional(LSTM(27, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(20, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    '''\n",
    "    model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.Input(shape=maxlen),\n",
    "        tf.keras.layers.Embedding(max_features, \n",
    "                                  embed_size, \n",
    "                                  weights=[embedding_matrix], \n",
    "                                  trainable=True),\n",
    "        #tf.keras.layers.Reshape((max_len, embed_size))\n",
    "        #tf.keras.layers.Bidirectional(\n",
    "        #    tf.keras.layers.LSTM(27, activation='relu', return_sequences = True)),\n",
    "        #tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        #tf.keras.layers.Dense(16, activation='relu'),\n",
    "        #tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])'''\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, epsilon=0.01)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc8b44bb-fe1e-4902-af70-5bfbfcb7873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in d:\\projects\\venv310\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: wordcloud in d:\\projects\\venv310\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in d:\\projects\\venv310\\lib\\site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in d:\\projects\\venv310\\lib\\site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in d:\\projects\\venv310\\lib\\site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: pillow in d:\\projects\\venv310\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\projects\\venv310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\venv310\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\projects\\venv310\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\venv310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (d:\\projects\\venv310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\projects\\venv310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (d:\\projects\\venv310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (d:\\projects\\venv310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1942ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "# Loading Glove Model\n",
    "def load_glove_model(File):\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    #print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model\n",
    "\n",
    "# NLP classification code\n",
    "def val_nlp_classification(X_train, X_test, Y_train, Y_test, n_folds):\n",
    "    # set maxlen based on right edge of question length histogram\n",
    "    maxlen = 250\n",
    "\n",
    "    # arbitrary choice of top 25000 words\n",
    "    max_features = 25000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, oov_token=\"<oov>\", filters='\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"',\n",
    "                          split=\" \")\n",
    "\n",
    "    tokenizer.fit_on_texts(np.concatenate([X_train, X_test]))\n",
    "    train_df = tokenizer.texts_to_sequences(X_train)\n",
    "    train_df = pad_sequences(train_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    test_df = tokenizer.texts_to_sequences(X_test)\n",
    "    test_df = pad_sequences(test_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "\n",
    "    EMBEDDING_FILE = 'C:/Users/vtoci/Documents/glove.6B.50d.txt'\n",
    "\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    # open the embedding file\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "    # glove_embedding_index = load_glove_model('glove.txt')\n",
    "\n",
    "    # get the mean and standard deviation of the embeddings weights\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # add the missing words to the embeddings and generate the random values\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    METRICS = [\n",
    "        tf.keras.metrics.AUC(name='roc-auc'),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    "    val_metrics = BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    print(f\"Maximum sequence length: {maxlen}\")\n",
    "    print(f\"Number of words in the embedding: {max_features}\")\n",
    "    print(f\"Number of words in the vocabulary: {len(tokenizer.word_index)}\")\n",
    "    print(f\"Number of features per embedding: {embed_size}\")\n",
    "    # embedding_matrix = tf.convert_to_tensor(embedding_matrix)\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model = lstm_sequence_model(maxlen, max_features, embed_size, embedding_matrix, val_metrics)\n",
    "    model.fit(train_df, Y_train, epochs=5, batch_size=64, validation_data=(test_df, Y_test))\n",
    "    scores = list()\n",
    "    # , acc = model.evaluate(test_df, verbose=2)\n",
    "    # scores.append(acc)\n",
    "    # ss_predictions = ss.copy()\n",
    "    # ss_predictions['prediction'] = preds\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c0b83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Generate word cloud to embed the model\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embedding(file):\n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "def make_tokenizer(texts, len_voc):\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "def modify_sentence(sentence, synonyms, p=0.5):\n",
    "    for i in range(len(sentence)):\n",
    "        if np.random.random() > p:\n",
    "            try:\n",
    "                syns = synonyms[sentence[i]]\n",
    "                sentence[i] = np.random.choice(syns)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return sentence\n",
    "\n",
    "def val_nlp_we_da(X, Y):\n",
    "    synonyms_number = 5\n",
    "    word_number = 20000\n",
    "    np.random.seed(100)\n",
    "    len_voc = 100000\n",
    "    glove = load_embedding('C:/Users/vtoci/Documents/glove.6B.50d.txt')\n",
    "    tokenizer = make_tokenizer(X, len_voc)\n",
    "    train_df = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(train_df, maxlen=70, padding=\"post\", truncating='post')\n",
    "    #X = pad_sequences(X, 70)\n",
    "    index_word = {0: ''}\n",
    "    for word in tokenizer.word_index.keys():\n",
    "        index_word[tokenizer.word_index[word]] = word\n",
    "    embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)\n",
    "    nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat) \n",
    "    neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "    synonyms = {x[0]: x[1:] for x in neighbours_mat}\n",
    "    for x in np.random.randint(1, word_number, 10):\n",
    "        print(f\"{index_word[x]} : {[index_word[synonyms[x][i]] for i in range(synonyms_number-1)]}\")\n",
    "    X_pos = X[Y==1]\n",
    "    indexes = np.random.randint(0, X_pos.shape[0], 10)\n",
    "    for x in X_pos[indexes]:\n",
    "        sample =  np.trim_zeros(x)\n",
    "        sentence = ' '.join([index_word[x] for x in sample])\n",
    "\n",
    "        modified = modify_sentence(sample, synonyms)\n",
    "        sentence_m = ' '.join([index_word[x] for x in modified])\n",
    "\n",
    "        print(' ')\n",
    "    X_gen = np.array([modify_sentence(x, synonyms) for x in X_pos[indexes]])\n",
    "    y_gen = np.ones(len(Y))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6035870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vtoci\\AppData\\Local\\Temp\\ipykernel_17876\\2665058715.py:47: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  scores = evaluate_model(train_ds, test_ds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 250\n",
      "Number of words in the embedding: 25000\n",
      "Number of words in the vocabulary: 124253\n",
      "Number of features per embedding: 50\n",
      "Epoch 1/5\n",
      "391/391 [==============================] - 16s 33ms/step - loss: 0.6934 - accuracy: 0.5001 - val_loss: 0.6930 - val_accuracy: 0.5004\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6929 - accuracy: 0.5101 - val_loss: 0.6921 - val_accuracy: 0.5006\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6528 - accuracy: 0.5912 - val_loss: 0.5221 - val_accuracy: 0.7341\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.4210 - accuracy: 0.8036 - val_loss: 0.3452 - val_accuracy: 0.8485\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.3089 - accuracy: 0.8689 - val_loss: 0.3265 - val_accuracy: 0.8582\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def evaluate_model(train, test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    X_train, Y_train = tfds.as_numpy(train)\n",
    "    X_test, Y_test = tfds.as_numpy(test)\n",
    "    #Y_train = Y_train.astype('str').tolist()\n",
    "    #Y_test = Y_test.astype('str').tolist()\n",
    "    X_train=pd.DataFrame(X_train, columns=['Text'])\n",
    "    X_train = X_train['Text'].str.decode(\"utf-8\")\n",
    "    X_test=pd.DataFrame(X_test, columns=['Text'])\n",
    "    X_test = X_test['Text'].str.decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = np.array(label_encoder.fit_transform(Y_train))\n",
    "    Y_test = np.array(label_encoder.fit_transform(Y_test))\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    #X_train, Y_train = val_nlp_we_da(X_train, Y_train)\n",
    "    scores=val_nlp_classification(X_train, X_test, Y_train, Y_test, n_folds)\n",
    "    return scores\n",
    "\n",
    "def evaluate_data_augment_model(train, test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    X_train, Y_train = tfds.as_numpy(train)\n",
    "    X_test, Y_test = tfds.as_numpy(test)\n",
    "    #Y_train = Y_train.astype('str').tolist()\n",
    "    #Y_test = Y_test.astype('str').tolist()\n",
    "    X_train=pd.DataFrame(X_train, columns=['Text'])\n",
    "    X_train = X_train['Text'].str.decode(\"utf-8\")\n",
    "    X_test=pd.DataFrame(X_test, columns=['Text'])\n",
    "    X_test = X_test['Text'].str.decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = np.array(label_encoder.fit_transform(Y_train))\n",
    "    Y_test = np.array(label_encoder.fit_transform(Y_test))\n",
    "    print(set(Y_test))\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    val_nlp_we_da(X_train, Y_train)\n",
    "    return\n",
    "\n",
    "train_ds, test_ds = tfds.load('imdb_reviews', split=['train', 'test'], batch_size=-1, as_supervised=True)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    scores = evaluate_model(train_ds, test_ds)\n",
    "#evaluate_data_augment_model(train_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede868c",
   "metadata": {},
   "source": [
    "## Text2Score\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "import tensorflow.keras.layers as layer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "max_len = 30\n",
    "tk = text.Tokenizer(num_words=50000)\n",
    "tk.fit_on_texts(df_all['text'].str.lower().tolist())\n",
    "X = tk.texts_to_sequences(df_all['text'].str.lower().values)\n",
    "X = sequence.pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "df_train = df_all[df_all['deal_probability'].notnull()]\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X[:len(df_train)], df_train['deal_probability'].values, test_size=0.01)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487685d",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80a6c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# Basic ANN\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if logs['accuracy'] >0.90:\n",
    "            print(\"Accuracy greater than 90%. Stopping Training.\")\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "def val_dnn_model(epochs, X_train, Y_train, X_val, Y_val, callbacks=None):\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f2324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 15ms/step - loss: 1.4225 - accuracy: 0.6029 - val_loss: 0.7206 - val_accuracy: 0.6100\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9611 - accuracy: 0.6357 - val_loss: 0.6664 - val_accuracy: 0.6600\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8454 - accuracy: 0.6500 - val_loss: 0.5681 - val_accuracy: 0.7333\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7691 - accuracy: 0.6914 - val_loss: 0.5555 - val_accuracy: 0.7467\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7324 - accuracy: 0.6543 - val_loss: 0.5953 - val_accuracy: 0.6767\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7255 - accuracy: 0.6643 - val_loss: 0.5592 - val_accuracy: 0.7433\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6991 - accuracy: 0.6886 - val_loss: 0.5677 - val_accuracy: 0.7367\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6723 - accuracy: 0.6900 - val_loss: 0.6136 - val_accuracy: 0.6567\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6347 - accuracy: 0.6829 - val_loss: 0.5918 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6718 - accuracy: 0.6729 - val_loss: 0.5552 - val_accuracy: 0.7433\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6305 - accuracy: 0.6814 - val_loss: 0.6016 - val_accuracy: 0.6733\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6334 - accuracy: 0.7086 - val_loss: 0.5917 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6086 - accuracy: 0.6871 - val_loss: 0.5535 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6054 - accuracy: 0.6914 - val_loss: 0.5667 - val_accuracy: 0.7300\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6037 - accuracy: 0.7071 - val_loss: 0.5556 - val_accuracy: 0.7567\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5858 - accuracy: 0.7314 - val_loss: 0.5798 - val_accuracy: 0.7133\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5693 - accuracy: 0.7286 - val_loss: 0.5443 - val_accuracy: 0.7567\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5740 - accuracy: 0.7257 - val_loss: 0.5479 - val_accuracy: 0.7333\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5690 - accuracy: 0.7243 - val_loss: 0.5567 - val_accuracy: 0.7100\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5736 - accuracy: 0.7200 - val_loss: 0.5377 - val_accuracy: 0.7867\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5615 - accuracy: 0.7329 - val_loss: 0.5543 - val_accuracy: 0.7133\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5509 - accuracy: 0.7200 - val_loss: 0.5339 - val_accuracy: 0.7400\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5477 - accuracy: 0.7357 - val_loss: 0.5345 - val_accuracy: 0.7433\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5490 - accuracy: 0.7429 - val_loss: 0.5388 - val_accuracy: 0.7333\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5666 - accuracy: 0.7357 - val_loss: 0.5250 - val_accuracy: 0.7833\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5608 - accuracy: 0.7186 - val_loss: 0.5218 - val_accuracy: 0.7767\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5282 - accuracy: 0.7443 - val_loss: 0.5244 - val_accuracy: 0.7600\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5219 - accuracy: 0.7500 - val_loss: 0.5195 - val_accuracy: 0.7600\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5292 - accuracy: 0.7557 - val_loss: 0.5125 - val_accuracy: 0.7633\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5504 - accuracy: 0.7200 - val_loss: 0.5261 - val_accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5493 - accuracy: 0.7143 - val_loss: 0.5289 - val_accuracy: 0.7433\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5165 - accuracy: 0.7686 - val_loss: 0.5108 - val_accuracy: 0.7633\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5197 - accuracy: 0.7543 - val_loss: 0.5171 - val_accuracy: 0.7667\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5157 - accuracy: 0.7771 - val_loss: 0.5137 - val_accuracy: 0.7533\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5269 - accuracy: 0.7543 - val_loss: 0.5125 - val_accuracy: 0.7600\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5140 - accuracy: 0.7729 - val_loss: 0.4992 - val_accuracy: 0.7667\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5188 - accuracy: 0.7471 - val_loss: 0.5062 - val_accuracy: 0.7700\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5086 - accuracy: 0.7629 - val_loss: 0.4943 - val_accuracy: 0.7700\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5257 - accuracy: 0.7643 - val_loss: 0.4926 - val_accuracy: 0.7800\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5170 - accuracy: 0.7557 - val_loss: 0.5316 - val_accuracy: 0.7400\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5000 - accuracy: 0.7629 - val_loss: 0.4954 - val_accuracy: 0.7867\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5130 - accuracy: 0.7686 - val_loss: 0.4917 - val_accuracy: 0.7733\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4868 - accuracy: 0.7829 - val_loss: 0.4916 - val_accuracy: 0.7667\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4975 - accuracy: 0.7686 - val_loss: 0.4907 - val_accuracy: 0.7800\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4899 - accuracy: 0.7657 - val_loss: 0.4934 - val_accuracy: 0.7600\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4989 - accuracy: 0.7614 - val_loss: 0.4912 - val_accuracy: 0.7700\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4812 - accuracy: 0.7771 - val_loss: 0.4867 - val_accuracy: 0.7700\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4998 - accuracy: 0.7786 - val_loss: 0.4929 - val_accuracy: 0.7800\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4978 - accuracy: 0.7700 - val_loss: 0.4985 - val_accuracy: 0.7633\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4824 - accuracy: 0.7757 - val_loss: 0.4877 - val_accuracy: 0.7767\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4778 - accuracy: 0.7814 - val_loss: 0.4804 - val_accuracy: 0.7700\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4754 - accuracy: 0.7700 - val_loss: 0.4971 - val_accuracy: 0.7767\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4821 - accuracy: 0.7943 - val_loss: 0.4871 - val_accuracy: 0.7900\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4772 - accuracy: 0.7800 - val_loss: 0.4921 - val_accuracy: 0.7800\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4635 - accuracy: 0.7929 - val_loss: 0.4839 - val_accuracy: 0.7633\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4708 - accuracy: 0.8014 - val_loss: 0.4948 - val_accuracy: 0.7700\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4693 - accuracy: 0.7671 - val_loss: 0.4863 - val_accuracy: 0.7733\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4740 - accuracy: 0.7857 - val_loss: 0.4773 - val_accuracy: 0.7867\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4629 - accuracy: 0.7843 - val_loss: 0.5161 - val_accuracy: 0.7400\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4735 - accuracy: 0.7800 - val_loss: 0.4800 - val_accuracy: 0.7967\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4840 - accuracy: 0.7843 - val_loss: 0.4795 - val_accuracy: 0.7933\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4651 - accuracy: 0.7757 - val_loss: 0.4844 - val_accuracy: 0.7767\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4632 - accuracy: 0.7943 - val_loss: 0.4793 - val_accuracy: 0.7933\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4730 - accuracy: 0.7871 - val_loss: 0.4796 - val_accuracy: 0.7800\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4667 - accuracy: 0.7929 - val_loss: 0.4708 - val_accuracy: 0.7867\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4548 - accuracy: 0.8100 - val_loss: 0.4815 - val_accuracy: 0.7667\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4588 - accuracy: 0.7900 - val_loss: 0.4766 - val_accuracy: 0.7900\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4573 - accuracy: 0.7900 - val_loss: 0.4740 - val_accuracy: 0.7900\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4491 - accuracy: 0.8000 - val_loss: 0.4721 - val_accuracy: 0.7900\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4640 - accuracy: 0.7843 - val_loss: 0.4693 - val_accuracy: 0.7900\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4645 - accuracy: 0.7914 - val_loss: 0.4827 - val_accuracy: 0.7800\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4551 - accuracy: 0.7943 - val_loss: 0.4780 - val_accuracy: 0.7833\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4480 - accuracy: 0.7943 - val_loss: 0.4736 - val_accuracy: 0.7800\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4425 - accuracy: 0.8071 - val_loss: 0.4837 - val_accuracy: 0.7800\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4395 - accuracy: 0.8186 - val_loss: 0.4770 - val_accuracy: 0.7867\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4530 - accuracy: 0.7943 - val_loss: 0.4867 - val_accuracy: 0.7833\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4464 - accuracy: 0.8057 - val_loss: 0.4752 - val_accuracy: 0.7900\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4549 - accuracy: 0.7914 - val_loss: 0.4712 - val_accuracy: 0.7900\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4352 - accuracy: 0.8100 - val_loss: 0.4740 - val_accuracy: 0.7900\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4316 - accuracy: 0.7986 - val_loss: 0.4819 - val_accuracy: 0.7900\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4536 - accuracy: 0.8086 - val_loss: 0.4784 - val_accuracy: 0.7967\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4500 - accuracy: 0.7943 - val_loss: 0.4828 - val_accuracy: 0.7867\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4417 - accuracy: 0.7871 - val_loss: 0.4766 - val_accuracy: 0.7800\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4207 - accuracy: 0.8014 - val_loss: 0.4765 - val_accuracy: 0.7900\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4309 - accuracy: 0.8086 - val_loss: 0.4791 - val_accuracy: 0.7767\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4225 - accuracy: 0.8286 - val_loss: 0.4739 - val_accuracy: 0.7867\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4351 - accuracy: 0.8014 - val_loss: 0.5002 - val_accuracy: 0.7667\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4463 - accuracy: 0.8014 - val_loss: 0.4686 - val_accuracy: 0.7800\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4306 - accuracy: 0.8129 - val_loss: 0.4814 - val_accuracy: 0.8033\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4288 - accuracy: 0.8129 - val_loss: 0.4735 - val_accuracy: 0.7933\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4319 - accuracy: 0.8029 - val_loss: 0.4967 - val_accuracy: 0.7800\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4473 - accuracy: 0.8029 - val_loss: 0.4882 - val_accuracy: 0.7667\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4133 - accuracy: 0.8114 - val_loss: 0.4798 - val_accuracy: 0.7933\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4288 - accuracy: 0.8114 - val_loss: 0.4847 - val_accuracy: 0.7700\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4263 - accuracy: 0.8114 - val_loss: 0.4792 - val_accuracy: 0.7967\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4345 - accuracy: 0.8200 - val_loss: 0.4855 - val_accuracy: 0.7900\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4146 - accuracy: 0.8371 - val_loss: 0.4936 - val_accuracy: 0.7933\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4255 - accuracy: 0.8257 - val_loss: 0.4792 - val_accuracy: 0.7700\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4235 - accuracy: 0.8086 - val_loss: 0.4841 - val_accuracy: 0.7800\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4214 - accuracy: 0.8186 - val_loss: 0.4816 - val_accuracy: 0.7867\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3781 - accuracy: 0.8343\n",
      "Training Set:   [0.37809038162231445, 0.8342857360839844]\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4816 - accuracy: 0.7867\n",
      "Validation Set: [0.48156917095184326, 0.7866666913032532]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = tfds.load('german_credit_numeric', split=['train'], batch_size=-1, as_supervised=True)\n",
    "X=tfds.as_numpy(train[0][0])\n",
    "Y=tfds.as_numpy(train[0][1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "hist_regularized, model_regularized = val_dnn_model(100, X_train, Y_train, X_test, Y_test, callbacks=[EarlyStoppingCallback()])\n",
    "print(f\"Training Set:   {model_regularized.evaluate(X_train, Y_train)}\")\n",
    "print(f\"Validation Set: {model_regularized.evaluate(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242ac5",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3167760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Timeseries LSTM Autoencoder\n",
    "def vtoc_lstm_autoencoder(n_steps, n_horizon, n_features, lr):\n",
    "    serie_size=n_steps\n",
    "    encoder_decoder = Sequential()\n",
    "    encoder_decoder.add(LSTM(n_steps, activation='relu', input_shape=(n_steps, n_features), return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(1, activation='relu'))\n",
    "    encoder_decoder.add(RepeatVector(serie_size))\n",
    "    encoder_decoder.add(LSTM(serie_size, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(TimeDistributed(Dense(1)))\n",
    "    encoder_decoder.summary()\n",
    "\n",
    "    adam = Adam(lr)\n",
    "    encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5c33ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "#Timeseries LSTM Model\n",
    "def vtoc_lstm_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(72, activation='relu', input_shape=(n_steps, n_features), return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=True)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128, activation='relu')),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries CNN Model\n",
    "def vtoc_cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    #tf.keras.layers.Input(shape=(n_steps, n_features)),\n",
    "    #tf.keras.layers.Flatten(input_shape=(n_steps, n_features)),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Flatten()),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss= Huber()\n",
    "    optimizer =Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries DNN Model\n",
    "def vtoc_dnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(n_steps, n_features)), #Use with evaluate_timeseries_model\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='dnn')\n",
    "    \n",
    "    loss=tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Model\n",
    "def lstm_cnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_steps,n_features))),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(LSTM(72, activation='relu', return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=False)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Skip Model\n",
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(72, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(48, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, skip_flatten])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a260d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Prepare timeseries data\n",
    "def multi_baseline_eror(data, pred_cols):\n",
    "    df = data[pred_cols]\n",
    "    #fill nans with linear interpolation because this is how we will fill when using the data in the models.\n",
    "    df_filled = df.interpolate(\"linear\")\n",
    "    mm = MinMaxScaler()\n",
    "    df_scaled = mm.fit_transform(df_filled)\n",
    "    df_prep = pd.DataFrame(df_scaled, columns=pred_cols)\n",
    "    y_true = df_prep[pred_cols[0]]\n",
    "    y_pred_forecast = df_prep[pred_cols[1]]\n",
    "\n",
    "    ### persistence 1 day\n",
    "    #shift series by 24 hours\n",
    "    # realign y_true to have the same length and time samples\n",
    "    y_preds_persistance_1_day = y_true.shift(24).dropna()\n",
    "    persistence_1_day_mae = tf.keras.losses.MAE(y_true[y_preds_persistance_1_day.index], y_preds_persistance_1_day).numpy()\n",
    "    persistence_1_day_mape = tf.keras.losses.MAPE(np.maximum(y_true[y_preds_persistance_1_day.index], 1e-5), np.maximum(y_preds_persistance_1_day, 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ### persistence 3 day average\n",
    "    #shift by 1, 2, 3 days. Realign to have same lengths. Average days and calcualte MAE.\n",
    "\n",
    "    shift_dfs = list()\n",
    "    for i in range(1, 4):\n",
    "        shift_dfs.append(pd.Series(y_true.shift(24 * i), name=f\"d{i}\"))\n",
    "\n",
    "    y_persistance_3d = pd.concat(shift_dfs, axis=1).dropna()\n",
    "    y_persistance_3d[\"avg\"] = (y_persistance_3d[\"d1\"] + y_persistance_3d[\"d2\"] + y_persistance_3d[\"d3\"])/3\n",
    "    d3_idx = y_persistance_3d.index\n",
    "    persistence_3day_avg_mae = tf.keras.losses.MAE(y_true[d3_idx], y_persistance_3d['avg']).numpy()\n",
    "    persistence_3day_avg_mape = tf.keras.losses.MAPE(np.maximum(y_true[d3_idx], 1e-5), np.maximum(y_persistance_3d['avg'], 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ref_error = pd.DataFrame({\n",
    "        \"Method\": [\"TSO Forecast\", \"Persistence 1 Day\", \"Persitence 3 Day Avg\"],\n",
    "        \"MAE\": [tf.keras.losses.MAE(y_true, y_pred_forecast).numpy(),\n",
    "                persistence_1_day_mae,\n",
    "                persistence_3day_avg_mae],\n",
    "        \"MAPE\":[tf.keras.losses.MAPE(np.maximum(y_true, 1e-5), np.maximum(y_pred_forecast, 1e-5)).numpy(),\n",
    "                persistence_1_day_mape,\n",
    "                persistence_3day_avg_mape]}, \n",
    "        index=[i for i in range(3)])\n",
    "    return ref_error\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "def split_data(series, train_fraq, test_len=8760):\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "\n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_window_data(dataset, look_back=1, n_features=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :n_features]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :n_features])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "    \n",
    "    #expand dimensions to 3D to fit with LSTM inputs\n",
    "    #creat the inital tensor dataset\n",
    "    if expand_dims:\n",
    "        ds = tf.expand_dims(data, axis=-1)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    \n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_multi_dataset(data, pred_cols, multivar):\n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    for column in pred_cols:\n",
    "        # Getting rid of outliers\n",
    "        data.loc[data[column] == -9999.0, column] = 0.0\n",
    "    ref_error = multi_baseline_eror(timeseries_df, pred_cols)\n",
    "    data=MinMaxScaler().fit_transform(data)\n",
    "    tf.random.set_seed(42)\n",
    "    train_fraq=0.65\n",
    "    lr = 3e-4\n",
    "    n_steps = 14#24*30\n",
    "    n_horizon = 14\n",
    "    batch_size = 64#256\n",
    "    shuffle_buffer = 100\n",
    "    if multivar:\n",
    "        n_features=len(data[0])\n",
    "    else:\n",
    "        n_features=1\n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=8760)\n",
    "    \n",
    "    (X_train, Y_train)=create_window_data(train_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_test, Y_test)=create_window_data(test_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_val, Y_val)=create_window_data(val_data, look_back=n_horizon, n_features=n_features)\n",
    "    #train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #test_ds = window_dataset(test_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #val_ds = window_dataset(val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #split_sequences(train_ds, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False)\n",
    "    #print(f\"Train Data Shape: {train_ds.shape}\")\n",
    "    #print(f\"Val Data Shape: {val_ds.shape}\")\n",
    "    #kfold = KFold(n_folds=5, shuffle=True, random_state=1)\n",
    "    model=vtoc_dnn_model(n_steps, n_horizon, n_features, lr)\n",
    "    model.summary()\n",
    "    model_hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, verbose=3)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    model.evaluate(X_train, Y_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    #_, acc = model.evaluate(test, verbose=2)\n",
    "    #scores.append(acc)\n",
    "    return model_hist.history\n",
    "\n",
    "def evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=False):\n",
    "    df_processed = create_multi_dataset(timeseries_df, pred_cols = ['wv (m/s)', 'max. wv (m/s)'], multivar=multivar)\n",
    "    print(df_processed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b94ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path) #We load the dataset in a csv_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16fb1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 196)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25216     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,534\n",
      "Trainable params: 43,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\venv310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "8365/8365 [==============================] - 39s 5ms/step - loss: 0.0015 - mae: 0.0321\n",
      "4504/4504 [==============================] - 8s 2ms/step\n",
      "{'loss': [0.0026063276454806328, 0.001634026993997395, 0.0015658314805477858, 0.0015392930945381522, 0.0015249659772962332], 'mae': [0.044398292899131775, 0.03303585201501846, 0.031847577542066574, 0.0313672199845314, 0.031105734407901764], 'val_loss': [0.00159911485388875, 0.0018306932179257274, 0.0017452491447329521, 0.0015111904358491302, 0.0015793809434399009], 'val_mae': [0.03261595591902733, 0.03585873544216156, 0.03526473790407181, 0.030268341302871704, 0.031942300498485565]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa18b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import datetime\n",
    "\n",
    "def create_dataset(X, y, delay=24, lookback=48):\n",
    "    window_length = lookback + delay\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)-delay):\n",
    "        v = X.iloc[i-lookback:i].to_numpy() # every one hour, we take the past 48 hours of features\n",
    "        Xs.append(v)\n",
    "        w = y.iloc[i+delay] # Every timestep, we take the temperature the next delay (here one day)\n",
    "        ys.append(w)\n",
    "    return(np.array(Xs), np.array(ys))\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    # Getting rid of outliers\n",
    "    data.loc[data['wv (m/s)'] == -9999.0, 'wv (m/s)'] = 0.0\n",
    "    data.loc[data['max. wv (m/s)'] == -9999.0, 'max. wv (m/s)'] = 0.0\n",
    "    \n",
    "    # Taking values every hours\n",
    "    data = data[5::6]# df[start,stop,step]\n",
    "    \n",
    "    wv = data.pop('wv (m/s)')\n",
    "    max_wv = data.pop('max. wv (m/s)')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = data.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    data['Wx'] = wv*np.cos(wd_rad)\n",
    "    data['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "    # Calculate the max wind x and y components.\n",
    "    data['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "    data['max Wy'] = max_wv*np.sin(wd_rad)\n",
    "    \n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    \n",
    "    day = 24*60*60 # Time is second within a single day\n",
    "    year = 365.2425*day # Time in second withon a year\n",
    "\n",
    "    data['Day sin'] = np.sin(timestamp_s * (2*np.pi / day))\n",
    "    data['Day cos'] = np.cos(timestamp_s * (2*np.pi / day))\n",
    "    data['Year sin'] = np.sin(timestamp_s * (2*np.pi / year))\n",
    "    data['Year cos'] = np.cos(timestamp_s * (2*np.pi / year))\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def split(data):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    train_df = data.iloc[0: n * 70 //100] # \"iloc\" because we have to select the lines at the indicies 0 to int(n*0.7) compared to \"loc\"\n",
    "    val_df = data.iloc[n * 70 //100 : n * 90 //100]\n",
    "    test_df = data.iloc[n * 90 //100:]\n",
    "    \n",
    "    return(train_df, val_df, test_df)\n",
    "\n",
    "def naive_eval_arr(X, y, lookback, delay):\n",
    "    batch_maes = []\n",
    "    for i in range(0, len(X)):\n",
    "        preds = X[i, -1, 1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n",
    "        mae = np.mean(np.abs(preds - y[i]))\n",
    "        batch_maes.append(mae)\n",
    "    return(np.mean(batch_maes))\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=True, batch_size=32, step=1):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while True:\n",
    "        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n",
    "            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n",
    "        else:\n",
    "            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n",
    "                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n",
    "            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n",
    "            i+=len(rows) # rows represents the number of sample in one batch\n",
    "            \n",
    "        samples = np.zeros((len(rows), lookback//step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n",
    "        targets = np.zeros((len(rows),)) #Shape = (batch_size,)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1] #We only want to predict the temperature for now,since [1], the second column\n",
    "        return samples, targets # The yield that replace the return to create a generator and not a regular function.\n",
    "        \n",
    "def evaluate_timeseries_model(df, n_folds=5, multivar=False):\n",
    "    scores, histories = list(), list()\n",
    "    df_processed = preprocessing(df)\n",
    "    train_df, val_df, test_df = split(df_processed)\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean)/train_std # As simple as that !\n",
    "    val_df = (val_df - train_mean)/train_std\n",
    "    test_df = (test_df - train_mean)/train_std\n",
    "    lookback = 48 # Looking at all features for the past 2 days\n",
    "    delay = 24 # Trying to predict the temperature for the next day\n",
    "    batch_size = 64 # Features will be batched 32 by 32.\n",
    "    X_train, Y_train = create_dataset(train_df, train_df['T (degC)'], delay = delay, lookback = lookback)\n",
    "    X_val, Y_val = create_dataset(val_df, val_df['T (degC)'], delay = delay)\n",
    "    naive_loss_arr = naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay)\n",
    "\n",
    "    naive_loss_arr = round(naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay),2) # Round the value\n",
    "    \n",
    "\n",
    "    data_train = train_df.to_numpy()\n",
    "    (X_train, Y_train) = generator(data = data_train, lookback = lookback, delay =delay, min_index = 0, \n",
    "                                   max_index = len(data_train), shuffle = True, batch_size = batch_size)\n",
    "\n",
    "    data_val = val_df.to_numpy()\n",
    "    (X_val, Y_val) = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, \n",
    "                               max_index = len(data_val), batch_size = batch_size)\n",
    "\n",
    "    data_test = test_df.to_numpy()\n",
    "    test_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0,\n",
    "                         max_index = len(data_test), batch_size = batch_size)\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=2)\n",
    "    \n",
    "    '''for train_index, test_index in tscv.split(X_train):\n",
    "        X_train, X_test = X_train[train_index], X_train[test_index]\n",
    "        Y_train, Y_test = Y_train[train_index], Y_train[test_index]'''\n",
    "    print(naive_loss_arr)\n",
    "    lr = 3e-4\n",
    "    n_steps=48#24*30\n",
    "    n_horizon=24\n",
    "    #batch_size=64\n",
    "    if multivar:\n",
    "        n_features=5\n",
    "    else:\n",
    "        n_features=1\n",
    "    model=vtoc_cnn_model(n_steps, n_horizon, n_features, lr)\n",
    "\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, verbose=3, shuffle=True)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    train_encoded = model.predict(X_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    print('Encoded time-series shape', train_encoded.shape)\n",
    "    print('Encoded time-series sample', train_encoded[0])\n",
    "    return model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a6c93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#csv_path = \"/kaggle/input/energy-consumption-generation-prices-and-weather/energy_dataset.csv\"\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "#evaluate_timeseries_model(timeseries_df, n_folds=5, multivar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c309969",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75d585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "def vtoc_regression_model(norm, model_type):\n",
    "    if model_type=='linear':\n",
    "        model = Sequential()\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_regression_data(dataset, target):\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset.isna().sum()\n",
    "    dataset=dataset.dropna()\n",
    "    #dataset['Origin'] = dataset['Origin'].map({1.0: 'USA', 2.0: 'Europe', 3.0: 'Japan'})\n",
    "    Y=dataset[target]\n",
    "    X=dataset.loc[:, dataset.columns != target]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "    horsepower = np.array(X_train['Horsepower']).astype('float32')\n",
    "\n",
    "    horsepower_normalizer = Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "    \n",
    "    linear_model=vtoc_regression_model(horsepower_normalizer, model_type='linear')\n",
    "    history = linear_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=2)\n",
    "    test_results = linear_model.predict(X_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7480131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 - 0s - loss: 194.2360 - val_loss: 61.6155 - 364ms/epoch - 40ms/step\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 67.7591 - val_loss: 29.2617 - 106ms/epoch - 12ms/step\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 42.0638 - val_loss: 62.1918 - 104ms/epoch - 12ms/step\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 57.2574 - val_loss: 24.3298 - 101ms/epoch - 11ms/step\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 37.8617 - val_loss: 90.2354 - 75ms/epoch - 8ms/step\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 54.9181 - val_loss: 72.4536 - 85ms/epoch - 9ms/step\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 54.6606 - val_loss: 25.8207 - 77ms/epoch - 9ms/step\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 53.9102 - val_loss: 38.0537 - 93ms/epoch - 10ms/step\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 48.9882 - val_loss: 88.1773 - 82ms/epoch - 9ms/step\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 53.3175 - val_loss: 75.0479 - 75ms/epoch - 8ms/step\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 54.2211 - val_loss: 5.6753 - 76ms/epoch - 8ms/step\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 30.4686 - val_loss: 14.3455 - 76ms/epoch - 8ms/step\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 29.9300 - val_loss: 31.5822 - 90ms/epoch - 10ms/step\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 32.7246 - val_loss: 34.7290 - 82ms/epoch - 9ms/step\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 32.9951 - val_loss: 41.5052 - 83ms/epoch - 9ms/step\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 30.8591 - val_loss: 32.9212 - 87ms/epoch - 10ms/step\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 20.7231 - val_loss: 4.8132 - 89ms/epoch - 10ms/step\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 9.4295 - val_loss: 16.6685 - 88ms/epoch - 10ms/step\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 24.5117 - val_loss: 24.2643 - 86ms/epoch - 10ms/step\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 51.8993 - val_loss: 45.3484 - 82ms/epoch - 9ms/step\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 54.8967 - val_loss: 64.0239 - 101ms/epoch - 11ms/step\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 54.2935 - val_loss: 80.1626 - 79ms/epoch - 9ms/step\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 55.5285 - val_loss: 36.0526 - 75ms/epoch - 8ms/step\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 54.4891 - val_loss: 17.6626 - 90ms/epoch - 10ms/step\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 21.2326 - val_loss: 27.3307 - 81ms/epoch - 9ms/step\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 21.1012 - val_loss: 8.4403 - 77ms/epoch - 9ms/step\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 11.0640 - val_loss: 43.6832 - 78ms/epoch - 9ms/step\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 26.6568 - val_loss: 17.7765 - 78ms/epoch - 9ms/step\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 26.3724 - val_loss: 50.1041 - 79ms/epoch - 9ms/step\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 31.0686 - val_loss: 47.5462 - 78ms/epoch - 9ms/step\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 20.4163 - val_loss: 8.1712 - 75ms/epoch - 8ms/step\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 17.6001 - val_loss: 15.0015 - 74ms/epoch - 8ms/step\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 25.5982 - val_loss: 36.0756 - 83ms/epoch - 9ms/step\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 32.7402 - val_loss: 31.7455 - 75ms/epoch - 8ms/step\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 33.6203 - val_loss: 51.9791 - 78ms/epoch - 9ms/step\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 35.8351 - val_loss: 35.7064 - 75ms/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 52.1201 - val_loss: 42.4552 - 77ms/epoch - 9ms/step\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 52.0010 - val_loss: 47.6132 - 80ms/epoch - 9ms/step\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 62.1675 - val_loss: 55.2965 - 77ms/epoch - 9ms/step\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 37.3147 - val_loss: 31.2423 - 76ms/epoch - 8ms/step\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 32.3636 - val_loss: 42.4491 - 78ms/epoch - 9ms/step\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 34.3304 - val_loss: 37.8643 - 75ms/epoch - 8ms/step\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 46.1015 - val_loss: 88.1343 - 78ms/epoch - 9ms/step\n",
      "Epoch 44/100\n",
      "9/9 - 0s - loss: 56.2074 - val_loss: 43.8982 - 76ms/epoch - 8ms/step\n",
      "Epoch 45/100\n",
      "9/9 - 0s - loss: 51.3519 - val_loss: 35.1194 - 76ms/epoch - 8ms/step\n",
      "Epoch 46/100\n",
      "9/9 - 0s - loss: 53.0274 - val_loss: 56.9428 - 79ms/epoch - 9ms/step\n",
      "Epoch 47/100\n",
      "9/9 - 0s - loss: 38.8092 - val_loss: 22.3290 - 76ms/epoch - 8ms/step\n",
      "Epoch 48/100\n",
      "9/9 - 0s - loss: 30.8847 - val_loss: 33.3649 - 80ms/epoch - 9ms/step\n",
      "Epoch 49/100\n",
      "9/9 - 0s - loss: 34.1483 - val_loss: 36.7138 - 76ms/epoch - 8ms/step\n",
      "Epoch 50/100\n",
      "9/9 - 0s - loss: 31.2780 - val_loss: 5.5106 - 74ms/epoch - 8ms/step\n",
      "Epoch 51/100\n",
      "9/9 - 0s - loss: 11.8823 - val_loss: 45.2866 - 82ms/epoch - 9ms/step\n",
      "Epoch 52/100\n",
      "9/9 - 0s - loss: 53.1544 - val_loss: 47.6208 - 82ms/epoch - 9ms/step\n",
      "Epoch 53/100\n",
      "9/9 - 0s - loss: 48.7064 - val_loss: 52.4496 - 82ms/epoch - 9ms/step\n",
      "Epoch 54/100\n",
      "9/9 - 0s - loss: 46.5441 - val_loss: 3.2955 - 83ms/epoch - 9ms/step\n",
      "Epoch 55/100\n",
      "9/9 - 0s - loss: 33.7925 - val_loss: 9.7137 - 72ms/epoch - 8ms/step\n",
      "Epoch 56/100\n",
      "9/9 - 0s - loss: 52.2868 - val_loss: 62.4189 - 65ms/epoch - 7ms/step\n",
      "Epoch 57/100\n",
      "9/9 - 0s - loss: 48.5492 - val_loss: 3.6267 - 65ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "9/9 - 0s - loss: 13.6723 - val_loss: 6.8011 - 73ms/epoch - 8ms/step\n",
      "Epoch 59/100\n",
      "9/9 - 0s - loss: 42.0669 - val_loss: 29.6369 - 72ms/epoch - 8ms/step\n",
      "Epoch 60/100\n",
      "9/9 - 0s - loss: 34.4439 - val_loss: 32.4739 - 64ms/epoch - 7ms/step\n",
      "Epoch 61/100\n",
      "9/9 - 0s - loss: 32.3223 - val_loss: 32.6152 - 63ms/epoch - 7ms/step\n",
      "Epoch 62/100\n",
      "9/9 - 0s - loss: 28.2303 - val_loss: 13.0764 - 62ms/epoch - 7ms/step\n",
      "Epoch 63/100\n",
      "9/9 - 0s - loss: 15.7806 - val_loss: 7.4465 - 64ms/epoch - 7ms/step\n",
      "Epoch 64/100\n",
      "9/9 - 0s - loss: 33.0223 - val_loss: 8.2704 - 61ms/epoch - 7ms/step\n",
      "Epoch 65/100\n",
      "9/9 - 0s - loss: 33.1716 - val_loss: 22.9518 - 65ms/epoch - 7ms/step\n",
      "Epoch 66/100\n",
      "9/9 - 0s - loss: 24.3310 - val_loss: 8.7492 - 61ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "9/9 - 0s - loss: 38.4550 - val_loss: 102.0890 - 65ms/epoch - 7ms/step\n",
      "Epoch 68/100\n",
      "9/9 - 0s - loss: 45.0272 - val_loss: 39.8021 - 77ms/epoch - 9ms/step\n",
      "Epoch 69/100\n",
      "9/9 - 0s - loss: 36.6091 - val_loss: 28.7989 - 69ms/epoch - 8ms/step\n",
      "Epoch 70/100\n",
      "9/9 - 0s - loss: 30.3385 - val_loss: 45.7223 - 62ms/epoch - 7ms/step\n",
      "Epoch 71/100\n",
      "9/9 - 0s - loss: 38.3598 - val_loss: 3.9033 - 65ms/epoch - 7ms/step\n",
      "Epoch 72/100\n",
      "9/9 - 0s - loss: 58.6937 - val_loss: 3.4064 - 62ms/epoch - 7ms/step\n",
      "Epoch 73/100\n",
      "9/9 - 0s - loss: 35.7046 - val_loss: 48.7231 - 62ms/epoch - 7ms/step\n",
      "Epoch 74/100\n",
      "9/9 - 0s - loss: 53.3610 - val_loss: 39.3769 - 73ms/epoch - 8ms/step\n",
      "Epoch 75/100\n",
      "9/9 - 0s - loss: 52.8404 - val_loss: 43.2817 - 67ms/epoch - 7ms/step\n",
      "Epoch 76/100\n",
      "9/9 - 0s - loss: 52.2859 - val_loss: 82.1883 - 67ms/epoch - 7ms/step\n",
      "Epoch 77/100\n",
      "9/9 - 0s - loss: 54.4365 - val_loss: 88.1577 - 65ms/epoch - 7ms/step\n",
      "Epoch 78/100\n",
      "9/9 - 0s - loss: 53.3204 - val_loss: 34.0285 - 72ms/epoch - 8ms/step\n",
      "Epoch 79/100\n",
      "9/9 - 0s - loss: 48.1750 - val_loss: 63.6392 - 66ms/epoch - 7ms/step\n",
      "Epoch 80/100\n",
      "9/9 - 0s - loss: 47.4155 - val_loss: 24.2977 - 64ms/epoch - 7ms/step\n",
      "Epoch 81/100\n",
      "9/9 - 0s - loss: 19.3631 - val_loss: 13.0914 - 63ms/epoch - 7ms/step\n",
      "Epoch 82/100\n",
      "9/9 - 0s - loss: 18.7119 - val_loss: 10.1693 - 76ms/epoch - 8ms/step\n",
      "Epoch 83/100\n",
      "9/9 - 0s - loss: 15.3573 - val_loss: 5.7738 - 64ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "9/9 - 0s - loss: 8.8782 - val_loss: 9.4013 - 87ms/epoch - 10ms/step\n",
      "Epoch 85/100\n",
      "9/9 - 0s - loss: 23.7941 - val_loss: 33.1482 - 67ms/epoch - 7ms/step\n",
      "Epoch 86/100\n",
      "9/9 - 0s - loss: 19.6384 - val_loss: 4.5448 - 64ms/epoch - 7ms/step\n",
      "Epoch 87/100\n",
      "9/9 - 0s - loss: 18.0102 - val_loss: 12.6899 - 66ms/epoch - 7ms/step\n",
      "Epoch 88/100\n",
      "9/9 - 0s - loss: 17.0113 - val_loss: 15.4198 - 69ms/epoch - 8ms/step\n",
      "Epoch 89/100\n",
      "9/9 - 0s - loss: 18.3390 - val_loss: 3.1096 - 67ms/epoch - 7ms/step\n",
      "Epoch 90/100\n",
      "9/9 - 0s - loss: 15.3114 - val_loss: 14.6577 - 65ms/epoch - 7ms/step\n",
      "Epoch 91/100\n",
      "9/9 - 0s - loss: 21.1427 - val_loss: 17.8688 - 65ms/epoch - 7ms/step\n",
      "Epoch 92/100\n",
      "9/9 - 0s - loss: 16.3582 - val_loss: 17.4884 - 64ms/epoch - 7ms/step\n",
      "Epoch 93/100\n",
      "9/9 - 0s - loss: 15.6280 - val_loss: 17.2741 - 63ms/epoch - 7ms/step\n",
      "Epoch 94/100\n",
      "9/9 - 0s - loss: 31.5439 - val_loss: 22.9111 - 62ms/epoch - 7ms/step\n",
      "Epoch 95/100\n",
      "9/9 - 0s - loss: 33.0612 - val_loss: 18.6217 - 68ms/epoch - 8ms/step\n",
      "Epoch 96/100\n",
      "9/9 - 0s - loss: 31.9737 - val_loss: 13.5101 - 71ms/epoch - 8ms/step\n",
      "Epoch 97/100\n",
      "9/9 - 0s - loss: 53.9816 - val_loss: 32.8417 - 73ms/epoch - 8ms/step\n",
      "Epoch 98/100\n",
      "9/9 - 0s - loss: 40.8525 - val_loss: 28.1392 - 73ms/epoch - 8ms/step\n",
      "Epoch 99/100\n",
      "9/9 - 0s - loss: 29.3811 - val_loss: 35.9143 - 69ms/epoch - 8ms/step\n",
      "Epoch 100/100\n",
      "9/9 - 0s - loss: 33.9936 - val_loss: 22.5838 - 64ms/epoch - 7ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[43.463013],\n",
       "       [43.44545 ],\n",
       "       [44.78642 ],\n",
       "       [49.48813 ],\n",
       "       [44.788414],\n",
       "       [51.799793],\n",
       "       [43.008366],\n",
       "       [49.03182 ],\n",
       "       [50.31293 ],\n",
       "       [47.010227],\n",
       "       [49.642067],\n",
       "       [45.571774],\n",
       "       [41.986553],\n",
       "       [45.069218],\n",
       "       [44.75677 ],\n",
       "       [45.196423],\n",
       "       [41.884197],\n",
       "       [47.534626],\n",
       "       [44.819557],\n",
       "       [44.403965],\n",
       "       [46.77242 ],\n",
       "       [48.725063],\n",
       "       [40.44493 ],\n",
       "       [44.78484 ],\n",
       "       [51.37041 ],\n",
       "       [42.118347],\n",
       "       [51.978924],\n",
       "       [41.30731 ],\n",
       "       [44.803684],\n",
       "       [41.90442 ],\n",
       "       [42.957764],\n",
       "       [44.710243],\n",
       "       [40.922073],\n",
       "       [49.394764],\n",
       "       [54.643833],\n",
       "       [48.031597],\n",
       "       [41.401855],\n",
       "       [52.670456],\n",
       "       [49.63508 ],\n",
       "       [49.170483],\n",
       "       [47.0755  ],\n",
       "       [39.499863],\n",
       "       [43.866688],\n",
       "       [41.500603],\n",
       "       [44.62073 ],\n",
       "       [46.550533],\n",
       "       [41.174747],\n",
       "       [41.926453],\n",
       "       [48.66471 ],\n",
       "       [45.97857 ],\n",
       "       [42.348164],\n",
       "       [49.57093 ],\n",
       "       [44.641266],\n",
       "       [42.4163  ],\n",
       "       [45.81929 ],\n",
       "       [43.179554],\n",
       "       [43.69881 ],\n",
       "       [45.74252 ],\n",
       "       [47.965317],\n",
       "       [47.83871 ],\n",
       "       [39.22541 ],\n",
       "       [45.846127],\n",
       "       [47.903736],\n",
       "       [46.551304],\n",
       "       [43.84762 ],\n",
       "       [47.740837],\n",
       "       [45.83728 ],\n",
       "       [43.00617 ],\n",
       "       [46.060307],\n",
       "       [47.15521 ],\n",
       "       [41.543133],\n",
       "       [41.37989 ],\n",
       "       [43.785686],\n",
       "       [41.205677],\n",
       "       [46.274483],\n",
       "       [40.475513],\n",
       "       [48.43007 ],\n",
       "       [41.090008],\n",
       "       [43.92744 ],\n",
       "       [41.916286],\n",
       "       [49.055485],\n",
       "       [43.036156],\n",
       "       [48.590252],\n",
       "       [51.47034 ],\n",
       "       [48.0355  ],\n",
       "       [45.33424 ],\n",
       "       [53.467674],\n",
       "       [47.6498  ],\n",
       "       [52.923275],\n",
       "       [45.901356],\n",
       "       [45.48155 ],\n",
       "       [44.969185],\n",
       "       [47.77609 ],\n",
       "       [39.843155],\n",
       "       [44.687214],\n",
       "       [44.73996 ],\n",
       "       [40.310627],\n",
       "       [43.685978],\n",
       "       [49.69607 ],\n",
       "       [44.352695],\n",
       "       [47.7281  ],\n",
       "       [45.1782  ],\n",
       "       [40.52435 ],\n",
       "       [44.621967],\n",
       "       [42.22255 ],\n",
       "       [50.677895],\n",
       "       [47.22851 ],\n",
       "       [42.40422 ],\n",
       "       [45.784946],\n",
       "       [45.772778],\n",
       "       [48.111416],\n",
       "       [49.56094 ],\n",
       "       [44.731674],\n",
       "       [50.66946 ],\n",
       "       [45.86309 ],\n",
       "       [45.03533 ],\n",
       "       [50.093204],\n",
       "       [45.040985]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "eval_regression_data(raw_dataset, 'MPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56c49fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'char_arr = [c for c in \"abcdefghijklmnopqrstuvwxyz0\"]\\nword_dict = {n: i for i, n in enumerate(char_arr)}\\nnumber_dict = {i: w for i, w in enumerate(char_arr)}\\nn_class = len(word_dict)\\nseq_data = [\\'make\\', \\'need\\', \\'coal\\', \\'word\\', \\'love\\', \\'hate\\', \\'live\\', \\'home\\', \\'hash\\', \\'star\\']\\n\\nn_step = 3\\nn_hidden = 128\\n\\ninputs = [sen[:3] for sen in seq_data]\\ninput_batch, target_batch = make_batch(seq_data)\\npredict =  sess.run([prediction], feed_dict={X: input_batch})\\nprint(inputs, \\'->\\', [number_dict[n] for n in predict[0]])\\n\\ndef make_batch(seq_data):\\n    input_batch, target_batch = [], []\\n\\n    for seq in seq_data:\\n        input = [word_dict[n] for n in seq[:-1]]\\n        target = word_dict[seq[-1]]\\n        input_batch.append(np.eye(n_class)[input])\\n        target_batch.append(np.eye(n_class)[target])\\n        #print(seq, len(seq))\\n\\n    return input_batch, target_batchtf.reset_default_graph()\\n\\n# Model\\nX = tf.placeholder(tf.float32, [len(seq_data), n_step, n_class]) # [batch_size, n_step, n_class]\\nY = tf.placeholder(tf.float32, [len(seq_data), n_class])         # [batch_size, n_class]\\nprint(X.shape, Y.shape)\\nW = tf.Variable(tf.random_normal([n_hidden, n_step]))\\nb = tf.Variable(tf.random_normal([len(seq_data), n_class]))\\nprint(W.shape)\\ncell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\\noutputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\\n\\n# outputs : [batch_size, n_step, n_hidden]\\noutputs = tf.transpose(outputs, [0, 1, 2]) # [n_step, batch_size, n_hidden]\\noutputs = outputs[-1] # [batch_size, n_hidden]\\nprint(outputs.shape)\\nmodel = tf.reshape(tf.matmul(outputs, new_W),[9]) + new_b # model : [batch_size, n_class]\\n\\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\\noptimizer = tf.train.AdamOptimizer(.5).minimize(cost)\\n\\nprediction = tf.cast(tf.argmax(model, 1), tf.int32)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''char_arr = [c for c in \"abcdefghijklmnopqrstuvwxyz0\"]\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
    "n_class = len(word_dict)\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n",
    "\n",
    "n_step = 3\n",
    "n_hidden = 128\n",
    "\n",
    "inputs = [sen[:3] for sen in seq_data]\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "predict =  sess.run([prediction], feed_dict={X: input_batch})\n",
    "print(inputs, '->', [number_dict[n] for n in predict[0]])\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]]\n",
    "        target = word_dict[seq[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "        #print(seq, len(seq))\n",
    "\n",
    "    return input_batch, target_batchtf.reset_default_graph()\n",
    "\n",
    "# Model\n",
    "X = tf.placeholder(tf.float32, [len(seq_data), n_step, n_class]) # [batch_size, n_step, n_class]\n",
    "Y = tf.placeholder(tf.float32, [len(seq_data), n_class])         # [batch_size, n_class]\n",
    "print(X.shape, Y.shape)\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_step]))\n",
    "b = tf.Variable(tf.random_normal([len(seq_data), n_class]))\n",
    "print(W.shape)\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "# outputs : [batch_size, n_step, n_hidden]\n",
    "outputs = tf.transpose(outputs, [0, 1, 2]) # [n_step, batch_size, n_hidden]\n",
    "outputs = outputs[-1] # [batch_size, n_hidden]\n",
    "print(outputs.shape)\n",
    "model = tf.reshape(tf.matmul(outputs, new_W),[9]) + new_b # model : [batch_size, n_class]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(.5).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5370c9",
   "metadata": {},
   "source": [
    "# Build Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "100bd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Neural Network\n",
    "import tensorflow as tf\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "    xs = tf.cast(xs, tf.float32) / 255.0\n",
    "    ys = tf.cast(ys, tf.float32)\n",
    "    ys = tf.one_hot(ys, depth=n_classes)\n",
    "    return tf.data.Dataset.from_tensor_slices((xs, ys)).map(preprocess).shuffle(len(ys)).batch(128)\n",
    "\n",
    "def val_nn(training_inputs_data, training_outputs_data, test_inputs):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    training_inputs = tensorflow.compat.v1.placeholder(shape=[None, 3], dtype=tensorflow.float32)  \n",
    "    training_outputs = tensorflow.compat.v1.placeholder(shape=[None, 1], dtype=tensorflow.float32) #Desired outputs for each input  \n",
    "    weights = tensorflow.Variable(initial_value=[[.3], [.1], [.8]], dtype=tensorflow.float32)  \n",
    "    bias = tensorflow.Variable(initial_value=[[1]], dtype=tensorflow.float32)  \n",
    "\n",
    "    af_input = tensorflow.matmul(training_inputs, weights) + bias  \n",
    "  \n",
    "    # Activation function of the output layer neuron  \n",
    "    predictions = tensorflow.nn.sigmoid(af_input)  \n",
    "    # Measuring the prediction error of the network after being trained  \n",
    "    prediction_error = tensorflow.reduce_sum(training_outputs - predictions)  \n",
    "    # Minimizing the prediction error using gradient descent optimizer  \n",
    "    \n",
    "    train_op = tensorflow.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(prediction_error) \n",
    "    # Creating a TensorFlow Session  \n",
    "    sess = tensorflow.compat.v1.Session()  \n",
    "    # Initializing the TensorFlow Variables (weights and bias)  \n",
    "    sess.run(tensorflow.compat.v1.global_variables_initializer())  \n",
    "    \n",
    "    # Training loop of the neural network  \n",
    "    for step in range(10000):  \n",
    "        sess.run(fetches=train_op, feed_dict={training_inputs: training_inputs_data, training_outputs: training_outputs_data})  \n",
    "        # Class scores of some testing data  \n",
    "    score= sess.run(fetches=predictions, feed_dict={training_inputs: [[248, 80, 68], [0, 0, 255]]})\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predictions =  sess.run(fetches=test_inputs)\n",
    "    # Closing the TensorFlow Session to free resources  \n",
    "    sess.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "213165e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2., 13.],\n",
       "       [ 7.,  9.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "X_test=tensorflow.convert_to_tensor(value=X_test, dtype=tensorflow.float32)\n",
    "val_nn(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa10de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop Neural Network\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = 1.0 / (1.0 + exp(-activation))\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            transfer_derivative = neuron['output'] * (1.0 - neuron['output'])\n",
    "            neuron['delta'] = errors[j] * transfer_derivative\n",
    "\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    "\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10c141a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': 0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': -0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': 0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': -0.03803132596437354}]\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836, 2.550537003, 0],\n",
    "           [1.465489372, 2.362125076, 0],\n",
    "           [3.396561688, 4.400293529, 0],\n",
    "           [1.38807019, 1.850220317, 0],\n",
    "           [3.06407232, 3.005305973, 0],\n",
    "           [7.627531214, 2.759262235, 1],\n",
    "           [5.332441248, 2.088626775, 1],\n",
    "           [6.922596716, 1.77106367, 1],\n",
    "           [8.675418651, -0.242068655, 1],\n",
    "           [7.673756466, 3.508563011, 1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "           [{'weights': [0.2550690257394217, 0.49543508709194095]},\n",
    "            {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1c775",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "290c624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "xt=tf.constant(X_train)\n",
    "xt.shape, tf.rank(xt)\n",
    "y1 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "y2 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob4=tf.matmul(y1, tf.transpose(y2))\n",
    "prob5=tf.tensordot(y1, tf.transpose(y2), axes=1)\n",
    "y6 = tf.random.uniform(shape=[224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob6=tf.math.reduce_max(y6, axis=0)\n",
    "y7 = tf.random.uniform(shape=[1, 224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob7 = tf.squeeze(y7, axis=0)\n",
    "y8 = tf.random.uniform(shape=[10], minval=0, maxval=10, dtype=tf.int64)\n",
    "prob9=tf.math.argmax(y8)\n",
    "prob10=tf.one_hot(y8, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c14aa121-df37-4304-9b37-b51afe532078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'random_uniform_5:0' shape=(3, 4) dtype=int32>,\n",
       " <tf.Tensor 'random_normal:0' shape=(3, 4) dtype=float32>,\n",
       " <tf.Tensor 'zeros:0' shape=(3, 4) dtype=float32>,\n",
       " <tf.Tensor 'Fill:0' shape=(3, 4) dtype=float32>,\n",
       " <tf.Tensor 'ones:0' shape=(3, 4) dtype=float32>,\n",
       " <tf.Tensor 'eye/diag:0' shape=(5, 5) dtype=float32>,\n",
       " TensorShape([3, 4]),\n",
       " tf.float32,\n",
       " <tf.Tensor 'boolean_mask/GatherV2:0' shape=(None,) dtype=int32>,\n",
       " <tf.Tensor 'SelectV2:0' shape=(4, 1) dtype=int32>,\n",
       " <tf.Tensor 'Reshape:0' shape=(25,) dtype=float32>,\n",
       " <tf.Tensor 'transpose_2:0' shape=(1, 10) dtype=float64>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(10,) dtype=float64>,\n",
       " <tf.Tensor 'concat:0' shape=(35, 1) dtype=float32>,\n",
       " <tf.Tensor 'MatMul_2:0' shape=(1, 1) dtype=int32>,\n",
       " <tf.Tensor 'Const_4:0' shape=(4,) dtype=float64>,\n",
       " <tf.Tensor 'Min:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'Max_1:0' shape=(4,) dtype=int32>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(X_train)\n",
    "a = tf.linspace(-1, 1, 10)\n",
    "a_new = tf.expand_dims(a, axis=1)\n",
    "a_new\n",
    "a_transpose=tf.transpose(a_new)\n",
    "a_randint = tf.random.uniform(shape=[3,4], minval=1, maxval=10, dtype=tf.int32)\n",
    "a_randn = tf.random.normal(shape=[3,4])\n",
    "a_zeros = tf.zeros(shape=[3,4])\n",
    "a_ones = tf.ones(shape=[3,4])\n",
    "a_fives = tf.fill([3,4], 5.2)\n",
    "a_eye = tf.eye(5)\n",
    "x = tf.expand_dims(tf.Variable([1, 2, 0, 4]), axis=1)\n",
    "mask = x >= 2\n",
    "a_slice = tf.boolean_mask(x, mask)\n",
    "masked = tf.greater(x,1)\n",
    "zeros=tf.zeros_like(x)\n",
    "a_masked = tf.where(masked, x, zeros)\n",
    "a_sq_eye = tf.reshape(a_eye, [25])\n",
    "a_new_transpose = tf.squeeze(a_transpose)\n",
    "a_concat = tf.concat((tf.cast(a_new, tf.float32), tf.cast(tf.expand_dims(a_sq_eye, axis=1), tf.float32)), axis=0)\n",
    "a_matmul = tf.matmul(tf.transpose(a_masked), x)\n",
    "\n",
    "numpy_arr = np.array([10.0, 11.0, 12.0, 13.0])\n",
    "from_numpy_to_tensor = tf.convert_to_tensor(numpy_arr)\n",
    "a_min = tf.reduce_min(x)\n",
    "a_max = tf.reduce_max(x, axis=1)\n",
    "a_randint, a_randn, a_zeros, a_fives, a_ones, a_eye, a_randint.shape, a_randn.dtype, a_slice, a_masked, a_sq_eye, a_transpose, a_new_transpose, a_concat, a_matmul, from_numpy_to_tensor, a_min, a_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94968d-902c-4f2b-b846-7c3b0a3cf64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
