{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae887a50",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b1aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels2(train, test, target_train, target_test):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    #if k.image_data_format() == 'channels_first':\n",
    "    #X_train = train.reshape(train.shape[0], 1, img_rows, img_cols)\n",
    "    #X_test = test.reshape(test.shape[0], 1, img_rows, img_cols)\n",
    "    #input_shape = (1, img_rows, img_cols)\n",
    "    #else:\n",
    "    X_train = train.reshape(train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = test.reshape(test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    train_norm = X_train.astype('float32')\n",
    "    test_norm = X_test.astype('float32')\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    target_train = to_categorical(target_train)\n",
    "    target_test =  to_categorical(target_test)\n",
    "    return train_norm, test_norm, target_train, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e8c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed36d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Prep pixels for tfds datasets load dataset\n",
    "def prep_pixels(image, label):\n",
    "    img_rows=28\n",
    "    img_cols=28\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.divide(image, 255)\n",
    "    train_norm = tf.image.resize(image, (32, 32))\n",
    "    target = tf.one_hot(label, depth=10)\n",
    "    return train_norm, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc2ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "\n",
    "# CNN model\n",
    "def val_cnn_model(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(320, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c4c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN optimized for MNIST\n",
    "def val_cnn_mnist(n_channels=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, n_channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.1, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    '''model=Sequential()\n",
    "\n",
    "    #model.add(Lambda(standardize,input_shape=(28,28,1)))    \n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,n_channels)))\n",
    "    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())    \n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])'''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eee238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "#data augmentation\n",
    "'''\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)'''\n",
    "\n",
    "# CNN Optimized for CIFAR10\n",
    "def val_cnn_cifar(n_channels=3):\n",
    "    weight_decay = 1e-4\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, n_channels)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt=RMSprop(lr=0.001,decay=1e-6)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71556d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN network for images\n",
    "def val_rnn_model(x_train):\n",
    "    '''i = Input(shape=x_train[0].shape)\n",
    "    x = LSTM(128)(i)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model=Model(i, x)'''\n",
    "    \n",
    "    model=Sequential()\n",
    "    #model.add(Input(shape=x_train[0].shape))\n",
    "    model.add(LSTM(128, input_shape=x_train[0].shape))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ee9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained MobileNet network for image recognition\n",
    "def val_mn_model(n_channels=3):\n",
    "    bottom_model = MobileNet(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)#top_model = Dense(1024, activation='relu')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f3892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained VGG Network for image recognition\n",
    "def val_vgg_model(n_channels=3):\n",
    "    bottom_model = VGG16(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Pretrained ResNet Network for image recognition\n",
    "def val_resnet_model(n_channels=3):\n",
    "    bottom_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32,32, n_channels))\n",
    "    for layer in bottom_model.layers:\n",
    "        layer.trainable = False\n",
    "    top_model = Flatten(name='flatten')(bottom_model.output)\n",
    "    top_model = Dense(10, activation='relu')(top_model)\n",
    "    top_model = Dense(10, activation='softmax')(top_model)\n",
    "    model = Model(inputs = bottom_model.inputs, outputs=top_model)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80756af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_model(train, test, n_channels=3, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model=val_cnn_mnist(n_channels) # 3 epochs\n",
    "    #model=val_cnn_cifar(n_channels) #Needs 80pct accuracy Eurosat gets > 80 at epoch 2 and overfits afterwards\n",
    "    #Needs 80pct accuracy CIFAR10 gets > 80 at epoch 5 for train and epoch 6 for validation. Each epoch is 12 min\n",
    "    #model=val_vgg_model(n_channels) # Reached 80pct at epoch 6. 10 minutes per epoch\n",
    "    model.fit(train, batch_size=128, epochs=10, steps_per_epoch=60000 // 64, validation_data=test, verbose=2)\n",
    "    _, acc = model.evaluate(test, verbose=2)\n",
    "    scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92af20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_rnn(x_train, y_train, x_test, y_test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model=val_rnn_model(x_train)\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, verbose=2)\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56da8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, Y_train) = train_ds\n",
    "#(X_test, Y_test) = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce19beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 - 14s - loss: 0.3426 - accuracy: 0.9001 - val_loss: 0.1861 - val_accuracy: 0.9471\n",
      "Epoch 2/10\n",
      "937/937 - 11s - loss: 0.1962 - accuracy: 0.9490 - val_loss: 0.1332 - val_accuracy: 0.9644\n",
      "Epoch 3/10\n",
      "937/937 - 11s - loss: 0.2093 - accuracy: 0.9486 - val_loss: 0.2049 - val_accuracy: 0.9564\n",
      "Epoch 4/10\n",
      "937/937 - 10s - loss: 0.3618 - accuracy: 0.9221 - val_loss: 0.5486 - val_accuracy: 0.8806\n",
      "Epoch 5/10\n",
      "937/937 - 10s - loss: 0.3865 - accuracy: 0.9163 - val_loss: 0.2696 - val_accuracy: 0.9439\n",
      "Epoch 6/10\n",
      "937/937 - 11s - loss: 0.3321 - accuracy: 0.9210 - val_loss: 0.2251 - val_accuracy: 0.9449\n",
      "Epoch 7/10\n",
      "937/937 - 10s - loss: 0.2328 - accuracy: 0.9445 - val_loss: 0.2333 - val_accuracy: 0.9378\n",
      "Epoch 8/10\n",
      "937/937 - 11s - loss: 0.2304 - accuracy: 0.9466 - val_loss: 0.2309 - val_accuracy: 0.9562\n",
      "Epoch 9/10\n",
      "937/937 - 12s - loss: 0.2101 - accuracy: 0.9500 - val_loss: 0.2017 - val_accuracy: 0.9575\n",
      "Epoch 10/10\n",
      "937/937 - 12s - loss: 0.1957 - accuracy: 0.9554 - val_loss: 0.1915 - val_accuracy: 0.9605\n",
      "157/157 - 1s - loss: 0.1915 - accuracy: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9605000019073486]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load datasets\n",
    "#train_ds, test_ds=tf.keras.datasets.mnist.load_data()\n",
    "#train_ds, test_ds = tfds.load('eurosat', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train_ds, test_ds = tfds.load('mnist', split=['train','test'], as_supervised=True) # 3 Epochs\n",
    "#train_ds, test_ds = tfds.load('cifar10', split=['train[:75%]','train[75%:]'], as_supervised=True)\n",
    "train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "evaluate_image_model(train, test, n_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae153b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 21s - loss: 0.5758 - accuracy: 0.8092 - val_loss: 0.1768 - val_accuracy: 0.9487\n",
      "Epoch 2/10\n",
      "1875/1875 - 22s - loss: 0.1425 - accuracy: 0.9574 - val_loss: 0.1125 - val_accuracy: 0.9676\n",
      "Epoch 3/10\n",
      "1875/1875 - 20s - loss: 0.0955 - accuracy: 0.9711 - val_loss: 0.0901 - val_accuracy: 0.9727\n",
      "Epoch 4/10\n",
      "1875/1875 - 20s - loss: 0.0739 - accuracy: 0.9778 - val_loss: 0.0850 - val_accuracy: 0.9744\n",
      "Epoch 5/10\n",
      "1875/1875 - 20s - loss: 0.0612 - accuracy: 0.9813 - val_loss: 0.0683 - val_accuracy: 0.9787\n",
      "Epoch 6/10\n",
      "1875/1875 - 23s - loss: 0.0501 - accuracy: 0.9844 - val_loss: 0.0667 - val_accuracy: 0.9796\n",
      "Epoch 7/10\n",
      "1875/1875 - 21s - loss: 0.0426 - accuracy: 0.9869 - val_loss: 0.0589 - val_accuracy: 0.9811\n",
      "Epoch 8/10\n",
      "1875/1875 - 20s - loss: 0.0376 - accuracy: 0.9881 - val_loss: 0.0600 - val_accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "1875/1875 - 19s - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.0588 - val_accuracy: 0.9822\n",
      "Epoch 10/10\n",
      "1875/1875 - 21s - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.0597 - val_accuracy: 0.9812\n",
      "313/313 - 1s - loss: 0.0597 - accuracy: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9811999797821045]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "#train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "#test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "evaluate_image_model_rnn(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910b0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def val_load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=10) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=10)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1cee1",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957575ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import Model, Input\n",
    "def val_vae(input_encoder):\n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(1, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    encoded = Dense(2, activation='relu')(x)\n",
    "\n",
    "    encoder = Model(inputs=inputs, outputs=encoded)\n",
    "    \n",
    "    encoded_inputs = Input(shape=(2,))\n",
    "\n",
    "    x = Dense(4, activation='relu')(encoded_inputs)\n",
    "    x = Reshape((2, 2, 1))(x)\n",
    "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((7, 7))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(inputs=encoded_inputs, outputs=decoded)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = decoder(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=Adam(0.01), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    clr = ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_delta=0.01,\n",
    "        cooldown=0,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        batch_size=256,\n",
    "        epochs=50,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=[clr])\n",
    "\n",
    "    return model, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e111061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on RNN network\n",
    "def evaluate_image_model_vae(x_train, y_train, x_test, y_test):\n",
    "    scores, histories = list(), list()\n",
    "    \n",
    "    model, encoder, decoder=val_vae(x_train)\n",
    "    \n",
    "    model.fit(x_train, x_train, validation_data=(x_test, x_test),  epochs=3, verbose=2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54995f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 2)                 34889     \n",
      "_________________________________________________________________\n",
      "functional_3 (Functional)    (None, 28, 28, 1)         42417     \n",
      "=================================================================\n",
      "Total params: 77,306\n",
      "Trainable params: 77,082\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "235/235 [==============================] - 149s 632ms/step - loss: 0.2744 - accuracy: 0.8060 - mse: 0.0711 - val_loss: 0.2829 - val_accuracy: 0.8072 - val_mse: 0.0734\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 153s 652ms/step - loss: 0.2636 - accuracy: 0.8048 - mse: 0.0675 - val_loss: 0.2672 - val_accuracy: 0.8072 - val_mse: 0.0688\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 156s 664ms/step - loss: 0.2635 - accuracy: 0.8047 - mse: 0.0674 - val_loss: 0.2639 - val_accuracy: 0.8047 - val_mse: 0.0678\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 165s 702ms/step - loss: 0.2635 - accuracy: 0.8046 - mse: 0.0674 - val_loss: 0.2638 - val_accuracy: 0.8033 - val_mse: 0.0677\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.8045 - mse: 0.0674\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "235/235 [==============================] - 167s 711ms/step - loss: 0.2633 - accuracy: 0.8045 - mse: 0.0674 - val_loss: 0.2636 - val_accuracy: 0.8036 - val_mse: 0.0677\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 173s 735ms/step - loss: 0.2631 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2632 - val_accuracy: 0.8019 - val_mse: 0.0677\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 169s 717ms/step - loss: 0.2631 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2630 - val_accuracy: 0.8010 - val_mse: 0.0676\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.8043 - mse: 0.0673\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "235/235 [==============================] - 165s 702ms/step - loss: 0.2631 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2634 - val_accuracy: 0.8029 - val_mse: 0.0677\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 150s 638ms/step - loss: 0.2630 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2629 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 150s 638ms/step - loss: 0.2630 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2628 - val_accuracy: 0.8008 - val_mse: 0.0675\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.8041 - mse: 0.0673\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "235/235 [==============================] - 150s 640ms/step - loss: 0.2630 - accuracy: 0.8041 - mse: 0.0673 - val_loss: 0.2628 - val_accuracy: 0.8037 - val_mse: 0.0675\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 150s 640ms/step - loss: 0.2629 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8034 - val_mse: 0.0675\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 149s 633ms/step - loss: 0.2629 - accuracy: 0.8041 - mse: 0.0673 - val_loss: 0.2627 - val_accuracy: 0.8034 - val_mse: 0.0675\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.8042 - mse: 0.0673\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "235/235 [==============================] - 150s 637ms/step - loss: 0.2629 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2627 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 152s 647ms/step - loss: 0.2629 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2626 - val_accuracy: 0.8022 - val_mse: 0.0675\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 152s 645ms/step - loss: 0.2629 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2626 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.8042 - mse: 0.0673\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "235/235 [==============================] - 150s 640ms/step - loss: 0.2629 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2627 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 151s 642ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2626 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 155s 660ms/step - loss: 0.2628 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "235/235 [==============================] - 155s 660ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2626 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 154s 655ms/step - loss: 0.2628 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 148s 628ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8042 - mse: 0.0673\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "235/235 [==============================] - 150s 639ms/step - loss: 0.2628 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8034 - val_mse: 0.0675\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 156s 666ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 154s 653ms/step - loss: 0.2628 - accuracy: 0.8042 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "235/235 [==============================] - 146s 620ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 149s 635ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 146s 622ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "235/235 [==============================] - 145s 617ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8026 - val_mse: 0.0675\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 144s 615ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 147s 626ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "235/235 [==============================] - 149s 635ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8026 - val_mse: 0.0675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "235/235 [==============================] - 162s 688ms/step - loss: 0.2628 - accuracy: 0.8043 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 166s 706ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "235/235 [==============================] - 154s 656ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 150s 637ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 149s 635ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "235/235 [==============================] - 149s 632ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 149s 633ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 146s 622ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "235/235 [==============================] - 147s 626ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 149s 633ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 153s 651ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "235/235 [==============================] - 150s 636ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 150s 637ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 149s 635ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "235/235 [==============================] - 152s 646ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 150s 637ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 150s 639ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "235/235 [==============================] - 150s 638ms/step - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 1/3\n",
      "1875/1875 - 203s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 2/3\n",
      "1875/1875 - 197s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n",
      "Epoch 3/3\n",
      "1875/1875 - 197s - loss: 0.2628 - accuracy: 0.8044 - mse: 0.0673 - val_loss: 0.2625 - val_accuracy: 0.8030 - val_mse: 0.0675\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1177 test_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4823 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 28, 28, 1) vs (None, 10))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18648/2674947672.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#test = test_ds.map(prep_pixels).cache().batch(64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mevaluate_image_model_vae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18648/4005328483.py\u001b[0m in \u001b[0;36mevaluate_image_model_vae\u001b[1;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3142\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1177 test_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4823 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 28, 28, 1) vs (None, 10))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test, y_train, y_test = prep_pixels2(x_train, x_test, y_train, y_test)\n",
    "#train = train_ds.map(prep_pixels).cache().shuffle(100).batch(64).repeat()\n",
    "#test = test_ds.map(prep_pixels).cache().batch(64)\n",
    "evaluate_image_model_vae(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4ecba",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a7db3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# LSTM model for NLP\n",
    "def lstm_sequence_model(maxlen,\n",
    "                   max_features,\n",
    "                   embed_size,\n",
    "                   embedding_matrix,\n",
    "                   metrics):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        embeddings = [embedding_matrix]\n",
    "        output_dim = embedding_matrix.shape[1]\n",
    "    else:\n",
    "        embeddings = None\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, trainable=True))\n",
    "    model.add(Reshape((maxlen, embed_size)))\n",
    "    model.add(Bidirectional(LSTM(27, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(20, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    '''\n",
    "    model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.Input(shape=maxlen),\n",
    "        tf.keras.layers.Embedding(max_features, \n",
    "                                  embed_size, \n",
    "                                  weights=[embedding_matrix], \n",
    "                                  trainable=True),\n",
    "        #tf.keras.layers.Reshape((max_len, embed_size))\n",
    "        #tf.keras.layers.Bidirectional(\n",
    "        #    tf.keras.layers.LSTM(27, activation='relu', return_sequences = True)),\n",
    "        #tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        #tf.keras.layers.Dense(16, activation='relu'),\n",
    "        #tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])'''\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.1, epsilon=0.01)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1942ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "# Loading Glove Model\n",
    "def load_glove_model(File):\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    #print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model\n",
    "\n",
    "# NLP classification code\n",
    "def val_nlp_classification(X_train, X_test, Y_train, Y_test, n_folds):\n",
    "    # set maxlen based on right edge of question length histogram\n",
    "    maxlen = 250\n",
    "\n",
    "    # arbitrary choice of top 25000 words\n",
    "    max_features = 25000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, oov_token=\"<oov>\", filters='\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"',\n",
    "                          split=\" \")\n",
    "\n",
    "    tokenizer.fit_on_texts(np.concatenate([X_train, X_test]))\n",
    "    train_df = tokenizer.texts_to_sequences(X_train)\n",
    "    train_df = pad_sequences(train_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    test_df = tokenizer.texts_to_sequences(X_test)\n",
    "    test_df = pad_sequences(test_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "\n",
    "    EMBEDDING_FILE = 'C:/Users/vtoci/Documents/glove.6B.50d.txt'\n",
    "\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    # open the embedding file\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "    # glove_embedding_index = load_glove_model('glove.txt')\n",
    "\n",
    "    # get the mean and standard deviation of the embeddings weights\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # add the missing words to the embeddings and generate the random values\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    METRICS = [\n",
    "        tf.keras.metrics.AUC(name='roc-auc'),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    "    val_metrics = BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    print(f\"Maximum sequence length: {maxlen}\")\n",
    "    print(f\"Number of words in the embedding: {max_features}\")\n",
    "    print(f\"Number of words in the vocabulary: {len(tokenizer.word_index)}\")\n",
    "    print(f\"Number of features per embedding: {embed_size}\")\n",
    "    # embedding_matrix = tf.convert_to_tensor(embedding_matrix)\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    model = lstm_sequence_model(maxlen, max_features, embed_size, embedding_matrix, val_metrics)\n",
    "    model.fit(train_df, Y_train, epochs=5, batch_size=64, validation_data=(test_df, Y_test))\n",
    "    scores = list()\n",
    "    # , acc = model.evaluate(test_df, verbose=2)\n",
    "    # scores.append(acc)\n",
    "    # ss_predictions = ss.copy()\n",
    "    # ss_predictions['prediction'] = preds\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c0b83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Generate word cloud to embed the model\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embedding(file):\n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "def make_tokenizer(texts, len_voc):\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "def modify_sentence(sentence, synonyms, p=0.5):\n",
    "    for i in range(len(sentence)):\n",
    "        if np.random.random() > p:\n",
    "            try:\n",
    "                syns = synonyms[sentence[i]]\n",
    "                sentence[i] = np.random.choice(syns)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return sentence\n",
    "\n",
    "def val_nlp_we_da(X, Y):\n",
    "    synonyms_number = 5\n",
    "    word_number = 20000\n",
    "    np.random.seed(100)\n",
    "    len_voc = 100000\n",
    "    glove = load_embedding('C:/Users/vtoci/Documents/glove.6B.50d.txt')\n",
    "    tokenizer = make_tokenizer(X, len_voc)\n",
    "    train_df = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(train_df, maxlen=70, padding=\"post\", truncating='post')\n",
    "    #X = pad_sequences(X, 70)\n",
    "    index_word = {0: ''}\n",
    "    for word in tokenizer.word_index.keys():\n",
    "        index_word[tokenizer.word_index[word]] = word\n",
    "    embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)\n",
    "    nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat) \n",
    "    neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "    synonyms = {x[0]: x[1:] for x in neighbours_mat}\n",
    "    for x in np.random.randint(1, word_number, 10):\n",
    "        print(f\"{index_word[x]} : {[index_word[synonyms[x][i]] for i in range(synonyms_number-1)]}\")\n",
    "    X_pos = X[Y==1]\n",
    "    indexes = np.random.randint(0, X_pos.shape[0], 10)\n",
    "    for x in X_pos[indexes]:\n",
    "        sample =  np.trim_zeros(x)\n",
    "        sentence = ' '.join([index_word[x] for x in sample])\n",
    "\n",
    "        modified = modify_sentence(sample, synonyms)\n",
    "        sentence_m = ' '.join([index_word[x] for x in modified])\n",
    "\n",
    "        print(' ')\n",
    "    X_gen = np.array([modify_sentence(x, synonyms) for x in X_pos[indexes]])\n",
    "    y_gen = np.ones(len(Y))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6035870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 250\n",
      "Number of words in the embedding: 25000\n",
      "Number of words in the vocabulary: 124253\n",
      "Number of features per embedding: 50\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5006WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.6938 - accuracy: 0.5006 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.6942 - accuracy: 0.5038 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 113s 5ms/sample - loss: 0.6942 - accuracy: 0.4982 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 113s 5ms/sample - loss: 0.6945 - accuracy: 0.5040 - val_loss: 0.6990 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 114s 5ms/sample - loss: 0.6949 - accuracy: 0.5046 - val_loss: 0.6945 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def evaluate_model(train, test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    X_train, Y_train = tfds.as_numpy(train)\n",
    "    X_test, Y_test = tfds.as_numpy(test)\n",
    "    #Y_train = Y_train.astype('str').tolist()\n",
    "    #Y_test = Y_test.astype('str').tolist()\n",
    "    X_train=pd.DataFrame(X_train, columns=['Text'])\n",
    "    X_train = X_train['Text'].str.decode(\"utf-8\")\n",
    "    X_test=pd.DataFrame(X_test, columns=['Text'])\n",
    "    X_test = X_test['Text'].str.decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = np.array(label_encoder.fit_transform(Y_train))\n",
    "    Y_test = np.array(label_encoder.fit_transform(Y_test))\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    #X_train, Y_train = val_nlp_we_da(X_train, Y_train)\n",
    "    scores=val_nlp_classification(X_train, X_test, Y_train, Y_test, n_folds)\n",
    "    return scores\n",
    "\n",
    "def evaluate_data_augment_model(train, test, n_folds=5):\n",
    "    scores, histories = list(), list()\n",
    "    X_train, Y_train = tfds.as_numpy(train)\n",
    "    X_test, Y_test = tfds.as_numpy(test)\n",
    "    #Y_train = Y_train.astype('str').tolist()\n",
    "    #Y_test = Y_test.astype('str').tolist()\n",
    "    X_train=pd.DataFrame(X_train, columns=['Text'])\n",
    "    X_train = X_train['Text'].str.decode(\"utf-8\")\n",
    "    X_test=pd.DataFrame(X_test, columns=['Text'])\n",
    "    X_test = X_test['Text'].str.decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = np.array(label_encoder.fit_transform(Y_train))\n",
    "    Y_test = np.array(label_encoder.fit_transform(Y_test))\n",
    "    print(set(Y_test))\n",
    "    kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "    val_nlp_we_da(X_train, Y_train)\n",
    "    return\n",
    "\n",
    "train_ds, test_ds = tfds.load('imdb_reviews', split=['train', 'test'], batch_size=-1, as_supervised=True)\n",
    "scores = evaluate_model(train_ds, test_ds)\n",
    "#evaluate_data_augment_model(train_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede868c",
   "metadata": {},
   "source": [
    "## Text2Score\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "import tensorflow.keras.layers as layer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "max_len = 30\n",
    "tk = text.Tokenizer(num_words=50000)\n",
    "tk.fit_on_texts(df_all['text'].str.lower().tolist())\n",
    "X = tk.texts_to_sequences(df_all['text'].str.lower().values)\n",
    "X = sequence.pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "df_train = df_all[df_all['deal_probability'].notnull()]\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X[:len(df_train)], df_train['deal_probability'].values, test_size=0.01)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65f2914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for sentiment analysis\n",
    "def vtoc_sentiment_model(vocab_size, embed_size, max_len):\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "    x = tf.keras.layers.Embedding(input_dim = vocab_size, \n",
    "                              output_dim = embed_size,\n",
    "                              input_length=max_len)(inputs)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
    "    # Add a classifier\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile and Run \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def vectorize_sequences(train_data):\n",
    "    training_sentences = []\n",
    "    training_labels = []\n",
    "    \n",
    "    for sentence in train_data[0]:\n",
    "        training_sentences.append(str(sentence.numpy()))\n",
    "    for label in train_data[1]:\n",
    "        training_labels.append(str(label.numpy()))\n",
    "    return training_sentences, np.asarray(training_labels)\n",
    "\n",
    "def evaluate_sentiment_model(train_ds, test_ds):\n",
    "    X_train, Y_train = vectorize_sequences(train_ds)\n",
    "    X_test, Y_test = vectorize_sequences(test_ds)\n",
    "    #arbitrary choice of top 25000 words\n",
    "    maxlen = 250\n",
    "    max_features=25000\n",
    "    vocab_size = 2000 # The maximum number of words to keep, based on word frequency. \n",
    "    embed_size = 30   # Dimension of the dense embedding.\n",
    "    max_len = 100  \n",
    "    tokenizer = Tokenizer(num_words=max_features, oov_token=\"<oov>\", filters='\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\"', split=\" \")\n",
    "    tokenizer.fit_on_texts(np.concatenate([X_train, X_test]))\n",
    "    train_df = tokenizer.texts_to_sequences(X_train)\n",
    "    train_padded = pad_sequences(train_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    test_df = tokenizer.texts_to_sequences(X_test)\n",
    "    test_padded = pad_sequences(test_df, maxlen=maxlen, padding=\"post\", truncating='post')\n",
    "    model=vtoc_sentiment_model(vocab_size, embed_size, max_len)\n",
    "    model.summary()\n",
    "    model.fit(train_padded, Y_train, validation_data=(test_padded, Y_test), epochs=5, verbose=3)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487685d",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80a6c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# Basic ANN\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if logs['accuracy'] >0.90:\n",
    "            print(\"Accuracy greater than 90%. Stopping Training.\")\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "def val_dnn_model(epochs, X_train, Y_train, X_val, Y_val, callbacks=None):\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f2324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1.3413 - accuracy: 0.6229 - val_loss: 0.9283 - val_accuracy: 0.6967\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0812 - accuracy: 0.6229 - val_loss: 0.6731 - val_accuracy: 0.7033\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9070 - accuracy: 0.6400 - val_loss: 0.5974 - val_accuracy: 0.7100\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8055 - accuracy: 0.6529 - val_loss: 0.5453 - val_accuracy: 0.7033\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7232 - accuracy: 0.6857 - val_loss: 0.5641 - val_accuracy: 0.7033\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.6871 - val_loss: 0.6090 - val_accuracy: 0.7033\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.6871 - val_loss: 0.5729 - val_accuracy: 0.7167\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.7029 - val_loss: 0.5638 - val_accuracy: 0.7167\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.7057 - val_loss: 0.5708 - val_accuracy: 0.7300\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.7114 - val_loss: 0.5576 - val_accuracy: 0.7300\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.7029 - val_loss: 0.5519 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6336 - accuracy: 0.6943 - val_loss: 0.5800 - val_accuracy: 0.7067\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.7086 - val_loss: 0.5595 - val_accuracy: 0.7533\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.7129 - val_loss: 0.5494 - val_accuracy: 0.7433\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.7029 - val_loss: 0.5451 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7243 - val_loss: 0.5385 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7171 - val_loss: 0.5352 - val_accuracy: 0.7367\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.7586 - val_loss: 0.5337 - val_accuracy: 0.7400\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.7471 - val_loss: 0.5337 - val_accuracy: 0.7267\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.7343 - val_loss: 0.5283 - val_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.7543 - val_loss: 0.5366 - val_accuracy: 0.7233\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.7471 - val_loss: 0.5363 - val_accuracy: 0.7300\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7457 - val_loss: 0.5283 - val_accuracy: 0.7400\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.7400 - val_loss: 0.5260 - val_accuracy: 0.7400\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7500 - val_loss: 0.5441 - val_accuracy: 0.7333\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7186 - val_loss: 0.5337 - val_accuracy: 0.7267\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.7457 - val_loss: 0.5288 - val_accuracy: 0.7533\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7671 - val_loss: 0.5191 - val_accuracy: 0.7700\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7514 - val_loss: 0.5218 - val_accuracy: 0.7333\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7343 - val_loss: 0.5179 - val_accuracy: 0.7433\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7643 - val_loss: 0.5178 - val_accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7443 - val_loss: 0.5272 - val_accuracy: 0.7367\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7671 - val_loss: 0.5282 - val_accuracy: 0.7567\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7757 - val_loss: 0.5175 - val_accuracy: 0.7467\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7643 - val_loss: 0.5165 - val_accuracy: 0.7667\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7557 - val_loss: 0.5066 - val_accuracy: 0.7467\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7500 - val_loss: 0.5146 - val_accuracy: 0.7567\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7729 - val_loss: 0.5131 - val_accuracy: 0.7600\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.7671 - val_loss: 0.5107 - val_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7700 - val_loss: 0.5101 - val_accuracy: 0.7467\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7743 - val_loss: 0.4966 - val_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7671 - val_loss: 0.5055 - val_accuracy: 0.7700\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7629 - val_loss: 0.4981 - val_accuracy: 0.7833\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.7686 - val_loss: 0.4972 - val_accuracy: 0.7600\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7714 - val_loss: 0.5055 - val_accuracy: 0.7600\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7800 - val_loss: 0.5151 - val_accuracy: 0.7733\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.7843 - val_loss: 0.4992 - val_accuracy: 0.7700\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7743 - val_loss: 0.4968 - val_accuracy: 0.7733\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7771 - val_loss: 0.5024 - val_accuracy: 0.7700\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7714 - val_loss: 0.5049 - val_accuracy: 0.7567\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7743 - val_loss: 0.4948 - val_accuracy: 0.8033\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.7914 - val_loss: 0.5047 - val_accuracy: 0.7667\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.7957 - val_loss: 0.4975 - val_accuracy: 0.7767\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.7814 - val_loss: 0.4997 - val_accuracy: 0.7867\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7843 - val_loss: 0.5030 - val_accuracy: 0.7800\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7929 - val_loss: 0.4886 - val_accuracy: 0.7833\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.7871 - val_loss: 0.4928 - val_accuracy: 0.7733\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4520 - accuracy: 0.7914 - val_loss: 0.4937 - val_accuracy: 0.7967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.8071 - val_loss: 0.4992 - val_accuracy: 0.7767\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7800 - val_loss: 0.4942 - val_accuracy: 0.7800\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.7871 - val_loss: 0.4963 - val_accuracy: 0.7733\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8086 - val_loss: 0.4944 - val_accuracy: 0.7933\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8043 - val_loss: 0.4914 - val_accuracy: 0.7733\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7886 - val_loss: 0.4853 - val_accuracy: 0.7900\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.7943 - val_loss: 0.4997 - val_accuracy: 0.7800\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7829 - val_loss: 0.4971 - val_accuracy: 0.7833\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.7986 - val_loss: 0.4885 - val_accuracy: 0.7867\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8071 - val_loss: 0.4982 - val_accuracy: 0.7867\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.7900 - val_loss: 0.5026 - val_accuracy: 0.7800\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7743 - val_loss: 0.5032 - val_accuracy: 0.7867\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7914 - val_loss: 0.4976 - val_accuracy: 0.7767\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7986 - val_loss: 0.4969 - val_accuracy: 0.7767\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.7843 - val_loss: 0.4936 - val_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8014 - val_loss: 0.4968 - val_accuracy: 0.7900\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8043 - val_loss: 0.4935 - val_accuracy: 0.7900\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.7971 - val_loss: 0.5182 - val_accuracy: 0.7767\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8043 - val_loss: 0.5229 - val_accuracy: 0.7633\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8086 - val_loss: 0.5023 - val_accuracy: 0.7833\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8171 - val_loss: 0.5095 - val_accuracy: 0.7900\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.7971 - val_loss: 0.5015 - val_accuracy: 0.7867\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8057 - val_loss: 0.5112 - val_accuracy: 0.7900\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8171 - val_loss: 0.5046 - val_accuracy: 0.7867\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8143 - val_loss: 0.5108 - val_accuracy: 0.7767\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8100 - val_loss: 0.5086 - val_accuracy: 0.7733\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8171 - val_loss: 0.5046 - val_accuracy: 0.7800\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8143 - val_loss: 0.5181 - val_accuracy: 0.7700\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8171 - val_loss: 0.5015 - val_accuracy: 0.7800\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8200 - val_loss: 0.5065 - val_accuracy: 0.7833\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8214 - val_loss: 0.5118 - val_accuracy: 0.7800\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8214 - val_loss: 0.5126 - val_accuracy: 0.7867\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8200 - val_loss: 0.5056 - val_accuracy: 0.7800\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8143 - val_loss: 0.5229 - val_accuracy: 0.7600\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8114 - val_loss: 0.5119 - val_accuracy: 0.7933\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4172 - accuracy: 0.7971 - val_loss: 0.5175 - val_accuracy: 0.7933\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8171 - val_loss: 0.5204 - val_accuracy: 0.7767\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8200 - val_loss: 0.5135 - val_accuracy: 0.7900\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8329 - val_loss: 0.5199 - val_accuracy: 0.7967\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8171 - val_loss: 0.5294 - val_accuracy: 0.7800\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8357 - val_loss: 0.5153 - val_accuracy: 0.7900\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8314 - val_loss: 0.5167 - val_accuracy: 0.7933\n",
      "22/22 [==============================] - 0s 649us/step - loss: 0.3528 - accuracy: 0.8586\n",
      "Training Set:   [0.3527942895889282, 0.8585714101791382]\n",
      "10/10 [==============================] - 0s 652us/step - loss: 0.5167 - accuracy: 0.7933\n",
      "Validation Set: [0.5166658759117126, 0.7933333516120911]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = tfds.load('german_credit_numeric', split=['train'], batch_size=-1, as_supervised=True)\n",
    "X=tfds.as_numpy(train[0][0])\n",
    "Y=tfds.as_numpy(train[0][1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "hist_regularized, model_regularized = val_dnn_model(100, X_train, Y_train, X_test, Y_test, callbacks=[EarlyStoppingCallback()])\n",
    "print(f\"Training Set:   {model_regularized.evaluate(X_train, Y_train)}\")\n",
    "print(f\"Validation Set: {model_regularized.evaluate(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56da78d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Serialization\\nimport tensorflow as tf\\nimport numpy as np\\n\\nPATH = '/kaggle/working/data.tfrecord'\\n\\nwith tf.io.TFRecordWriter(path=PATH) as f:\\n    f.write(b'123') # write one record\\n    f.write(b'xyz314') # write another record\\n\\nwith open(PATH, 'rb') as f:\\n    print(f.read())\\n    \\nx = tf.constant([[1, 2], [3, 4]], dtype=tf.uint8)\\nprint('x:', x, '\\n')\\n\\nx_bytes = tf.io.serialize_tensor(x)\\nprint('x_bytes:', x_bytes, '\\n')\\n\\nprint('x:', tf.io.parse_tensor(x_bytes, out_type=tf.uint8))\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "PATH = '/kaggle/working/data.tfrecord'\n",
    "\n",
    "with tf.io.TFRecordWriter(path=PATH) as f:\n",
    "    f.write(b'123') # write one record\n",
    "    f.write(b'xyz314') # write another record\n",
    "\n",
    "with open(PATH, 'rb') as f:\n",
    "    print(f.read())\n",
    "    \n",
    "x = tf.constant([[1, 2], [3, 4]], dtype=tf.uint8)\n",
    "print('x:', x, '\\n')\n",
    "\n",
    "x_bytes = tf.io.serialize_tensor(x)\n",
    "print('x_bytes:', x_bytes, '\\n')\n",
    "\n",
    "print('x:', tf.io.parse_tensor(x_bytes, out_type=tf.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242ac5",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3167760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Timeseries LSTM Autoencoder\n",
    "def vtoc_lstm_autoencoder(n_steps, n_horizon, n_features, lr):\n",
    "    serie_size=n_steps\n",
    "    encoder_decoder = Sequential()\n",
    "    encoder_decoder.add(LSTM(n_steps, activation='relu', input_shape=(n_steps, n_features), return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(1, activation='relu'))\n",
    "    encoder_decoder.add(RepeatVector(serie_size))\n",
    "    encoder_decoder.add(LSTM(serie_size, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(TimeDistributed(Dense(1)))\n",
    "    encoder_decoder.summary()\n",
    "\n",
    "    adam = Adam(lr)\n",
    "    encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5c33ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "#Timeseries LSTM Model\n",
    "def vtoc_lstm_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(72, activation='relu', input_shape=(n_steps, n_features), return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=True)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128, activation='relu')),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries CNN Model\n",
    "def vtoc_cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    #tf.keras.layers.Input(shape=(n_steps, n_features)),\n",
    "    #tf.keras.layers.Flatten(input_shape=(n_steps, n_features)),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Flatten()),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss= Huber()\n",
    "    optimizer =Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries DNN Model\n",
    "def vtoc_dnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(n_steps, n_features)), #Use with evaluate_timeseries_model\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='dnn')\n",
    "    \n",
    "    loss=tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Model\n",
    "def lstm_cnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_steps,n_features))),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(LSTM(72, activation='relu', return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=False)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Skip Model\n",
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(72, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(48, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, skip_flatten])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a260d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Prepare timeseries data\n",
    "def multi_baseline_eror(data, pred_cols):\n",
    "    df = data[pred_cols]\n",
    "    #fill nans with linear interpolation because this is how we will fill when using the data in the models.\n",
    "    df_filled = df.interpolate(\"linear\")\n",
    "    mm = MinMaxScaler()\n",
    "    df_scaled = mm.fit_transform(df_filled)\n",
    "    df_prep = pd.DataFrame(df_scaled, columns=pred_cols)\n",
    "    y_true = df_prep[pred_cols[0]]\n",
    "    y_pred_forecast = df_prep[pred_cols[1]]\n",
    "\n",
    "    ### persistence 1 day\n",
    "    #shift series by 24 hours\n",
    "    # realign y_true to have the same length and time samples\n",
    "    y_preds_persistance_1_day = y_true.shift(24).dropna()\n",
    "    persistence_1_day_mae = tf.keras.losses.MAE(y_true[y_preds_persistance_1_day.index], y_preds_persistance_1_day).numpy()\n",
    "    persistence_1_day_mape = tf.keras.losses.MAPE(np.maximum(y_true[y_preds_persistance_1_day.index], 1e-5), np.maximum(y_preds_persistance_1_day, 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ### persistence 3 day average\n",
    "    #shift by 1, 2, 3 days. Realign to have same lengths. Average days and calcualte MAE.\n",
    "\n",
    "    shift_dfs = list()\n",
    "    for i in range(1, 4):\n",
    "        shift_dfs.append(pd.Series(y_true.shift(24 * i), name=f\"d{i}\"))\n",
    "\n",
    "    y_persistance_3d = pd.concat(shift_dfs, axis=1).dropna()\n",
    "    y_persistance_3d[\"avg\"] = (y_persistance_3d[\"d1\"] + y_persistance_3d[\"d2\"] + y_persistance_3d[\"d3\"])/3\n",
    "    d3_idx = y_persistance_3d.index\n",
    "    persistence_3day_avg_mae = tf.keras.losses.MAE(y_true[d3_idx], y_persistance_3d['avg']).numpy()\n",
    "    persistence_3day_avg_mape = tf.keras.losses.MAPE(np.maximum(y_true[d3_idx], 1e-5), np.maximum(y_persistance_3d['avg'], 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ref_error = pd.DataFrame({\n",
    "        \"Method\": [\"TSO Forecast\", \"Persistence 1 Day\", \"Persitence 3 Day Avg\"],\n",
    "        \"MAE\": [tf.keras.losses.MAE(y_true, y_pred_forecast).numpy(),\n",
    "                persistence_1_day_mae,\n",
    "                persistence_3day_avg_mae],\n",
    "        \"MAPE\":[tf.keras.losses.MAPE(np.maximum(y_true, 1e-5), np.maximum(y_pred_forecast, 1e-5)).numpy(),\n",
    "                persistence_1_day_mape,\n",
    "                persistence_3day_avg_mape]}, \n",
    "        index=[i for i in range(3)])\n",
    "    return ref_error\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "def split_data(series, train_fraq, test_len=8760):\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "\n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_window_data(dataset, look_back=1, n_features=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :n_features]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :n_features])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "    \n",
    "    #expand dimensions to 3D to fit with LSTM inputs\n",
    "    #creat the inital tensor dataset\n",
    "    if expand_dims:\n",
    "        ds = tf.expand_dims(data, axis=-1)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    \n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_multi_dataset(data, pred_cols, multivar):\n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    for column in pred_cols:\n",
    "        # Getting rid of outliers\n",
    "        data.loc[data[column] == -9999.0, column] = 0.0\n",
    "    ref_error = multi_baseline_eror(timeseries_df, pred_cols)\n",
    "    data=MinMaxScaler().fit_transform(data)\n",
    "    tf.random.set_seed(42)\n",
    "    train_fraq=0.65\n",
    "    lr = 3e-4\n",
    "    n_steps = 14#24*30\n",
    "    n_horizon = 14\n",
    "    batch_size = 64#256\n",
    "    shuffle_buffer = 100\n",
    "    if multivar:\n",
    "        n_features=len(data[0])\n",
    "    else:\n",
    "        n_features=1\n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=8760)\n",
    "    \n",
    "    (X_train, Y_train)=create_window_data(train_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_test, Y_test)=create_window_data(test_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_val, Y_val)=create_window_data(val_data, look_back=n_horizon, n_features=n_features)\n",
    "    #train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #test_ds = window_dataset(test_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #val_ds = window_dataset(val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #split_sequences(train_ds, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False)\n",
    "    #print(f\"Train Data Shape: {train_ds.shape}\")\n",
    "    #print(f\"Val Data Shape: {val_ds.shape}\")\n",
    "    #kfold = KFold(n_folds=5, shuffle=True, random_state=1)\n",
    "    model=vtoc_dnn_model(n_steps, n_horizon, n_features, lr)\n",
    "    model.summary()\n",
    "    model_hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, verbose=3)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    model.evaluate(X_train, Y_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    #_, acc = model.evaluate(test, verbose=2)\n",
    "    #scores.append(acc)\n",
    "    return model_hist.history\n",
    "\n",
    "def evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=False):\n",
    "    df_processed = create_multi_dataset(timeseries_df, pred_cols = ['wv (m/s)', 'max. wv (m/s)'], multivar=multivar)\n",
    "    print(df_processed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b94ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path) #We load the dataset in a csv_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16fb1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               25216     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 43,534\n",
      "Trainable params: 43,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "8365/8365 [==============================] - 5s 540us/step - loss: 0.0015 - mae: 0.0326\n",
      "{'loss': [0.0026133705396205187, 0.0016304290620610118, 0.0015607296954840422, 0.0015348155284300447, 0.001521471654996276], 'mae': [0.04442968592047691, 0.032973457127809525, 0.031751763075590134, 0.031300127506256104, 0.031048348173499107], 'val_loss': [0.0016255037626251578, 0.0017620386788621545, 0.002023919252678752, 0.001825809944421053, 0.001594674657098949], 'val_mae': [0.03237354755401611, 0.0348031260073185, 0.03872617706656456, 0.03650348260998726, 0.032519251108169556]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa18b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import datetime\n",
    "\n",
    "def create_dataset(X, y, delay=24, lookback=48):\n",
    "    window_length = lookback + delay\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)-delay):\n",
    "        v = X.iloc[i-lookback:i].to_numpy() # every one hour, we take the past 48 hours of features\n",
    "        Xs.append(v)\n",
    "        w = y.iloc[i+delay] # Every timestep, we take the temperature the next delay (here one day)\n",
    "        ys.append(w)\n",
    "    return(np.array(Xs), np.array(ys))\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    # Getting rid of outliers\n",
    "    data.loc[data['wv (m/s)'] == -9999.0, 'wv (m/s)'] = 0.0\n",
    "    data.loc[data['max. wv (m/s)'] == -9999.0, 'max. wv (m/s)'] = 0.0\n",
    "    \n",
    "    # Taking values every hours\n",
    "    data = data[5::6]# df[start,stop,step]\n",
    "    \n",
    "    wv = data.pop('wv (m/s)')\n",
    "    max_wv = data.pop('max. wv (m/s)')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = data.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    data['Wx'] = wv*np.cos(wd_rad)\n",
    "    data['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "    # Calculate the max wind x and y components.\n",
    "    data['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "    data['max Wy'] = max_wv*np.sin(wd_rad)\n",
    "    \n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    \n",
    "    day = 24*60*60 # Time is second within a single day\n",
    "    year = 365.2425*day # Time in second withon a year\n",
    "\n",
    "    data['Day sin'] = np.sin(timestamp_s * (2*np.pi / day))\n",
    "    data['Day cos'] = np.cos(timestamp_s * (2*np.pi / day))\n",
    "    data['Year sin'] = np.sin(timestamp_s * (2*np.pi / year))\n",
    "    data['Year cos'] = np.cos(timestamp_s * (2*np.pi / year))\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def split(data):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    train_df = data.iloc[0: n * 70 //100] # \"iloc\" because we have to select the lines at the indicies 0 to int(n*0.7) compared to \"loc\"\n",
    "    val_df = data.iloc[n * 70 //100 : n * 90 //100]\n",
    "    test_df = data.iloc[n * 90 //100:]\n",
    "    \n",
    "    return(train_df, val_df, test_df)\n",
    "\n",
    "def naive_eval_arr(X, y, lookback, delay):\n",
    "    batch_maes = []\n",
    "    for i in range(0, len(X)):\n",
    "        preds = X[i, -1, 1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n",
    "        mae = np.mean(np.abs(preds - y[i]))\n",
    "        batch_maes.append(mae)\n",
    "    return(np.mean(batch_maes))\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=True, batch_size=32, step=1):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while True:\n",
    "        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n",
    "            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n",
    "        else:\n",
    "            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n",
    "                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n",
    "            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n",
    "            i+=len(rows) # rows represents the number of sample in one batch\n",
    "            \n",
    "        samples = np.zeros((len(rows), lookback//step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n",
    "        targets = np.zeros((len(rows),)) #Shape = (batch_size,)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1] #We only want to predict the temperature for now,since [1], the second column\n",
    "        return samples, targets # The yield that replace the return to create a generator and not a regular function.\n",
    "        \n",
    "def evaluate_timeseries_model(df, n_folds=5, multivar=False):\n",
    "    scores, histories = list(), list()\n",
    "    df_processed = preprocessing(df)\n",
    "    train_df, val_df, test_df = split(df_processed)\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean)/train_std # As simple as that !\n",
    "    val_df = (val_df - train_mean)/train_std\n",
    "    test_df = (test_df - train_mean)/train_std\n",
    "    lookback = 48 # Looking at all features for the past 2 days\n",
    "    delay = 24 # Trying to predict the temperature for the next day\n",
    "    batch_size = 64 # Features will be batched 32 by 32.\n",
    "    X_train, Y_train = create_dataset(train_df, train_df['T (degC)'], delay = delay, lookback = lookback)\n",
    "    X_val, Y_val = create_dataset(val_df, val_df['T (degC)'], delay = delay)\n",
    "    naive_loss_arr = naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay)\n",
    "\n",
    "    naive_loss_arr = round(naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay),2) # Round the value\n",
    "    \n",
    "\n",
    "    data_train = train_df.to_numpy()\n",
    "    (X_train, Y_train) = generator(data = data_train, lookback = lookback, delay =delay, min_index = 0, \n",
    "                                   max_index = len(data_train), shuffle = True, batch_size = batch_size)\n",
    "\n",
    "    data_val = val_df.to_numpy()\n",
    "    (X_val, Y_val) = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, \n",
    "                               max_index = len(data_val), batch_size = batch_size)\n",
    "\n",
    "    data_test = test_df.to_numpy()\n",
    "    test_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0,\n",
    "                         max_index = len(data_test), batch_size = batch_size)\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=2)\n",
    "    \n",
    "    '''for train_index, test_index in tscv.split(X_train):\n",
    "        X_train, X_test = X_train[train_index], X_train[test_index]\n",
    "        Y_train, Y_test = Y_train[train_index], Y_train[test_index]'''\n",
    "    print(naive_loss_arr)\n",
    "    lr = 3e-4\n",
    "    n_steps=48#24*30\n",
    "    n_horizon=24\n",
    "    #batch_size=64\n",
    "    if multivar:\n",
    "        n_features=5\n",
    "    else:\n",
    "        n_features=1\n",
    "    model=vtoc_cnn_model(n_steps, n_horizon, n_features, lr)\n",
    "\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, verbose=3, shuffle=True)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    train_encoded = model.predict(X_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    print('Encoded time-series shape', train_encoded.shape)\n",
    "    print('Encoded time-series sample', train_encoded[0])\n",
    "    return model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a6c93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#csv_path = \"/kaggle/input/energy-consumption-generation-prices-and-weather/energy_dataset.csv\"\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "#evaluate_timeseries_model(timeseries_df, n_folds=5, multivar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c309969",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75d585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "def vtoc_regression_model(norm, model_type):\n",
    "    if model_type=='linear':\n",
    "        model = Sequential()\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_regression_data(dataset, target):\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset.isna().sum()\n",
    "    dataset=dataset.dropna()\n",
    "    #dataset['Origin'] = dataset['Origin'].map({1.0: 'USA', 2.0: 'Europe', 3.0: 'Japan'})\n",
    "    Y=dataset[target]\n",
    "    X=dataset.loc[:, dataset.columns != target]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "    horsepower = np.array(X_train['Horsepower']).astype('float32')\n",
    "\n",
    "    horsepower_normalizer = Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "    \n",
    "    linear_model=vtoc_regression_model(horsepower_normalizer, model_type='linear')\n",
    "    history = linear_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=2)\n",
    "    test_results = linear_model.predict(X_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7480131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 - 0s - loss: 207.0230 - val_loss: 186.9233\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 105.8817 - val_loss: 59.0272\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 57.3015 - val_loss: 16.9091\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 50.3619 - val_loss: 28.8476\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 26.4307 - val_loss: 10.1797\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 16.0334 - val_loss: 7.6933\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 9.5975 - val_loss: 18.6179\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 17.9664 - val_loss: 16.5457\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 24.5081 - val_loss: 14.3826\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 20.3868 - val_loss: 48.9944\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 32.9330 - val_loss: 15.6101\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 13.6707 - val_loss: 14.1699\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 27.3060 - val_loss: 50.2797\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 14.2132 - val_loss: 6.6686\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 12.3285 - val_loss: 29.4934\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 14.4060 - val_loss: 7.6376\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 16.4003 - val_loss: 12.7363\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 25.9461 - val_loss: 32.2487\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 35.3533 - val_loss: 38.9727\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 36.0491 - val_loss: 22.8638\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 35.9169 - val_loss: 54.9937\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 62.7754 - val_loss: 43.5046\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 27.1110 - val_loss: 37.5357\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 34.1586 - val_loss: 58.2223\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 37.3496 - val_loss: 32.2926\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 35.3447 - val_loss: 53.6830\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 37.5981 - val_loss: 23.6061\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 9.9623 - val_loss: 9.2099\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 21.1700 - val_loss: 71.0256\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 60.6307 - val_loss: 63.4801\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 59.8063 - val_loss: 42.3541\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 50.2780 - val_loss: 62.4196\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 61.5802 - val_loss: 80.5935\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 56.0627 - val_loss: 91.5250\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 33.9356 - val_loss: 14.5979\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 9.9226 - val_loss: 37.2765\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 49.8121 - val_loss: 62.9708\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 58.0018 - val_loss: 14.0899\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 53.2789 - val_loss: 67.1250\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 51.6339 - val_loss: 15.0251\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 28.2077 - val_loss: 58.7031\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 22.8932 - val_loss: 25.0669\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 17.5354 - val_loss: 23.2124\n",
      "Epoch 44/100\n",
      "9/9 - 0s - loss: 16.4190 - val_loss: 27.4354\n",
      "Epoch 45/100\n",
      "9/9 - 0s - loss: 18.1967 - val_loss: 24.5240\n",
      "Epoch 46/100\n",
      "9/9 - 0s - loss: 15.9018 - val_loss: 28.0002\n",
      "Epoch 47/100\n",
      "9/9 - 0s - loss: 28.0102 - val_loss: 59.0393\n",
      "Epoch 48/100\n",
      "9/9 - 0s - loss: 31.5145 - val_loss: 71.5435\n",
      "Epoch 49/100\n",
      "9/9 - 0s - loss: 45.4074 - val_loss: 16.0271\n",
      "Epoch 50/100\n",
      "9/9 - 0s - loss: 23.7309 - val_loss: 65.8326\n",
      "Epoch 51/100\n",
      "9/9 - 0s - loss: 45.4420 - val_loss: 14.1236\n",
      "Epoch 52/100\n",
      "9/9 - 0s - loss: 63.9142 - val_loss: 10.1696\n",
      "Epoch 53/100\n",
      "9/9 - 0s - loss: 82.2251 - val_loss: 104.0280\n",
      "Epoch 54/100\n",
      "9/9 - 0s - loss: 64.3671 - val_loss: 44.3485\n",
      "Epoch 55/100\n",
      "9/9 - 0s - loss: 33.6851 - val_loss: 54.2454\n",
      "Epoch 56/100\n",
      "9/9 - 0s - loss: 58.1769 - val_loss: 23.8316\n",
      "Epoch 57/100\n",
      "9/9 - 0s - loss: 52.0661 - val_loss: 52.7168\n",
      "Epoch 58/100\n",
      "9/9 - 0s - loss: 58.7063 - val_loss: 46.7522\n",
      "Epoch 59/100\n",
      "9/9 - 0s - loss: 32.9262 - val_loss: 59.2029\n",
      "Epoch 60/100\n",
      "9/9 - 0s - loss: 24.4130 - val_loss: 19.4112\n",
      "Epoch 61/100\n",
      "9/9 - 0s - loss: 19.4320 - val_loss: 4.8213\n",
      "Epoch 62/100\n",
      "9/9 - 0s - loss: 16.7284 - val_loss: 11.9034\n",
      "Epoch 63/100\n",
      "9/9 - 0s - loss: 37.7925 - val_loss: 101.6448\n",
      "Epoch 64/100\n",
      "9/9 - 0s - loss: 59.5993 - val_loss: 64.9873\n",
      "Epoch 65/100\n",
      "9/9 - 0s - loss: 54.4914 - val_loss: 17.0098\n",
      "Epoch 66/100\n",
      "9/9 - 0s - loss: 10.9223 - val_loss: 9.9307\n",
      "Epoch 67/100\n",
      "9/9 - 0s - loss: 9.4551 - val_loss: 5.3203\n",
      "Epoch 68/100\n",
      "9/9 - 0s - loss: 8.4776 - val_loss: 12.5648\n",
      "Epoch 69/100\n",
      "9/9 - 0s - loss: 11.4364 - val_loss: 33.9199\n",
      "Epoch 70/100\n",
      "9/9 - 0s - loss: 35.4482 - val_loss: 19.0951\n",
      "Epoch 71/100\n",
      "9/9 - 0s - loss: 9.1772 - val_loss: 10.3691\n",
      "Epoch 72/100\n",
      "9/9 - 0s - loss: 9.2922 - val_loss: 37.9965\n",
      "Epoch 73/100\n",
      "9/9 - 0s - loss: 47.4245 - val_loss: 78.1090\n",
      "Epoch 74/100\n",
      "9/9 - 0s - loss: 55.5075 - val_loss: 3.9095\n",
      "Epoch 75/100\n",
      "9/9 - 0s - loss: 62.9231 - val_loss: 5.5407\n",
      "Epoch 76/100\n",
      "9/9 - 0s - loss: 19.8876 - val_loss: 12.7160\n",
      "Epoch 77/100\n",
      "9/9 - 0s - loss: 17.7464 - val_loss: 23.3118\n",
      "Epoch 78/100\n",
      "9/9 - 0s - loss: 19.2133 - val_loss: 4.5891\n",
      "Epoch 79/100\n",
      "9/9 - 0s - loss: 12.7250 - val_loss: 12.6271\n",
      "Epoch 80/100\n",
      "9/9 - 0s - loss: 16.7514 - val_loss: 13.1441\n",
      "Epoch 81/100\n",
      "9/9 - 0s - loss: 40.4607 - val_loss: 96.0915\n",
      "Epoch 82/100\n",
      "9/9 - 0s - loss: 57.2098 - val_loss: 65.1243\n",
      "Epoch 83/100\n",
      "9/9 - 0s - loss: 58.7768 - val_loss: 20.3059\n",
      "Epoch 84/100\n",
      "9/9 - 0s - loss: 33.7134 - val_loss: 23.2150\n",
      "Epoch 85/100\n",
      "9/9 - 0s - loss: 19.0758 - val_loss: 14.9197\n",
      "Epoch 86/100\n",
      "9/9 - 0s - loss: 33.4138 - val_loss: 32.6622\n",
      "Epoch 87/100\n",
      "9/9 - 0s - loss: 13.3279 - val_loss: 17.5343\n",
      "Epoch 88/100\n",
      "9/9 - 0s - loss: 18.7227 - val_loss: 27.2342\n",
      "Epoch 89/100\n",
      "9/9 - 0s - loss: 16.9263 - val_loss: 20.1870\n",
      "Epoch 90/100\n",
      "9/9 - 0s - loss: 19.0941 - val_loss: 17.5116\n",
      "Epoch 91/100\n",
      "9/9 - 0s - loss: 17.1602 - val_loss: 21.9102\n",
      "Epoch 92/100\n",
      "9/9 - 0s - loss: 19.0422 - val_loss: 13.5632\n",
      "Epoch 93/100\n",
      "9/9 - 0s - loss: 17.4397 - val_loss: 28.7942\n",
      "Epoch 94/100\n",
      "9/9 - 0s - loss: 19.8009 - val_loss: 33.7634\n",
      "Epoch 95/100\n",
      "9/9 - 0s - loss: 34.1056 - val_loss: 38.6769\n",
      "Epoch 96/100\n",
      "9/9 - 0s - loss: 32.5983 - val_loss: 57.1578\n",
      "Epoch 97/100\n",
      "9/9 - 0s - loss: 34.1410 - val_loss: 52.4998\n",
      "Epoch 98/100\n",
      "9/9 - 0s - loss: 32.9322 - val_loss: 63.5676\n",
      "Epoch 99/100\n",
      "9/9 - 0s - loss: 39.6385 - val_loss: 40.7320\n",
      "Epoch 100/100\n",
      "9/9 - 0s - loss: 40.7104 - val_loss: 4.1629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[32.476585],\n",
       "       [25.81191 ],\n",
       "       [23.810892],\n",
       "       [33.60027 ],\n",
       "       [14.306817],\n",
       "       [34.71142 ],\n",
       "       [33.98316 ],\n",
       "       [19.636993],\n",
       "       [34.9159  ],\n",
       "       [27.910555],\n",
       "       [26.794949],\n",
       "       [29.861685],\n",
       "       [26.126131],\n",
       "       [27.336956],\n",
       "       [30.981146],\n",
       "       [29.830883],\n",
       "       [21.951605],\n",
       "       [23.900434],\n",
       "       [34.31024 ],\n",
       "       [18.411036],\n",
       "       [31.809025],\n",
       "       [24.225199],\n",
       "       [20.900694],\n",
       "       [31.512413],\n",
       "       [23.361996],\n",
       "       [20.005302],\n",
       "       [30.802748],\n",
       "       [ 8.71432 ],\n",
       "       [28.212976],\n",
       "       [26.4481  ],\n",
       "       [19.08701 ],\n",
       "       [33.67961 ],\n",
       "       [28.977564],\n",
       "       [32.34758 ],\n",
       "       [17.560305],\n",
       "       [27.645283],\n",
       "       [28.82028 ],\n",
       "       [16.56996 ],\n",
       "       [29.75811 ],\n",
       "       [28.830397],\n",
       "       [29.593637],\n",
       "       [30.413723],\n",
       "       [25.331316],\n",
       "       [18.336843],\n",
       "       [20.054445],\n",
       "       [16.25602 ],\n",
       "       [32.527287],\n",
       "       [26.173738],\n",
       "       [30.594522],\n",
       "       [30.332525],\n",
       "       [29.547132],\n",
       "       [29.942026],\n",
       "       [22.006224],\n",
       "       [24.433374],\n",
       "       [25.041302],\n",
       "       [25.44463 ],\n",
       "       [33.398552],\n",
       "       [30.910475],\n",
       "       [29.389507],\n",
       "       [15.51101 ],\n",
       "       [33.0917  ],\n",
       "       [14.599795],\n",
       "       [11.655498],\n",
       "       [23.981508],\n",
       "       [28.624443],\n",
       "       [33.694   ],\n",
       "       [24.308443],\n",
       "       [33.83318 ],\n",
       "       [24.454607],\n",
       "       [34.06561 ],\n",
       "       [24.82326 ],\n",
       "       [27.20459 ],\n",
       "       [26.808405],\n",
       "       [25.891645],\n",
       "       [29.812918],\n",
       "       [29.015795],\n",
       "       [25.399242],\n",
       "       [27.345076],\n",
       "       [29.860107],\n",
       "       [31.111546],\n",
       "       [23.075502],\n",
       "       [26.818726],\n",
       "       [27.594519],\n",
       "       [34.667194],\n",
       "       [31.199831],\n",
       "       [11.279561],\n",
       "       [22.990932],\n",
       "       [33.74789 ],\n",
       "       [26.200518],\n",
       "       [24.710241],\n",
       "       [30.75553 ],\n",
       "       [34.815014],\n",
       "       [24.717142],\n",
       "       [32.56861 ],\n",
       "       [35.34454 ],\n",
       "       [23.17983 ],\n",
       "       [19.473898],\n",
       "       [16.181581],\n",
       "       [30.454433],\n",
       "       [16.472185],\n",
       "       [24.624537],\n",
       "       [26.664106],\n",
       "       [25.604618],\n",
       "       [31.6299  ],\n",
       "       [11.381541],\n",
       "       [29.115713],\n",
       "       [20.349594],\n",
       "       [31.076296],\n",
       "       [30.553362],\n",
       "       [34.706608],\n",
       "       [31.438196],\n",
       "       [32.3938  ],\n",
       "       [29.40796 ],\n",
       "       [26.998259],\n",
       "       [28.880278],\n",
       "       [28.57688 ],\n",
       "       [31.96083 ],\n",
       "       [32.661617]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "eval_regression_data(raw_dataset, 'MPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56c49fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'char_arr = [c for c in \"abcdefghijklmnopqrstuvwxyz0\"]\\nword_dict = {n: i for i, n in enumerate(char_arr)}\\nnumber_dict = {i: w for i, w in enumerate(char_arr)}\\nn_class = len(word_dict)\\nseq_data = [\\'make\\', \\'need\\', \\'coal\\', \\'word\\', \\'love\\', \\'hate\\', \\'live\\', \\'home\\', \\'hash\\', \\'star\\']\\n\\nn_step = 3\\nn_hidden = 128\\n\\ninputs = [sen[:3] for sen in seq_data]\\ninput_batch, target_batch = make_batch(seq_data)\\npredict =  sess.run([prediction], feed_dict={X: input_batch})\\nprint(inputs, \\'->\\', [number_dict[n] for n in predict[0]])\\n\\ndef make_batch(seq_data):\\n    input_batch, target_batch = [], []\\n\\n    for seq in seq_data:\\n        input = [word_dict[n] for n in seq[:-1]]\\n        target = word_dict[seq[-1]]\\n        input_batch.append(np.eye(n_class)[input])\\n        target_batch.append(np.eye(n_class)[target])\\n        #print(seq, len(seq))\\n\\n    return input_batch, target_batchtf.reset_default_graph()\\n\\n# Model\\nX = tf.placeholder(tf.float32, [len(seq_data), n_step, n_class]) # [batch_size, n_step, n_class]\\nY = tf.placeholder(tf.float32, [len(seq_data), n_class])         # [batch_size, n_class]\\nprint(X.shape, Y.shape)\\nW = tf.Variable(tf.random_normal([n_hidden, n_step]))\\nb = tf.Variable(tf.random_normal([len(seq_data), n_class]))\\nprint(W.shape)\\ncell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\\noutputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\\n\\n# outputs : [batch_size, n_step, n_hidden]\\noutputs = tf.transpose(outputs, [0, 1, 2]) # [n_step, batch_size, n_hidden]\\noutputs = outputs[-1] # [batch_size, n_hidden]\\nprint(outputs.shape)\\nmodel = tf.reshape(tf.matmul(outputs, new_W),[9]) + new_b # model : [batch_size, n_class]\\n\\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\\noptimizer = tf.train.AdamOptimizer(.5).minimize(cost)\\n\\nprediction = tf.cast(tf.argmax(model, 1), tf.int32)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_arr = [c for c in \"abcdefghijklmnopqrstuvwxyz0\"]\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
    "n_class = len(word_dict)\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n",
    "\n",
    "n_step = 3\n",
    "n_hidden = 128\n",
    "\n",
    "inputs = [sen[:3] for sen in seq_data]\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "predict =  sess.run([prediction], feed_dict={X: input_batch})\n",
    "print(inputs, '->', [number_dict[n] for n in predict[0]])\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]]\n",
    "        target = word_dict[seq[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "        #print(seq, len(seq))\n",
    "\n",
    "    return input_batch, target_batchtf.reset_default_graph()\n",
    "\n",
    "# Model\n",
    "X = tf.placeholder(tf.float32, [len(seq_data), n_step, n_class]) # [batch_size, n_step, n_class]\n",
    "Y = tf.placeholder(tf.float32, [len(seq_data), n_class])         # [batch_size, n_class]\n",
    "print(X.shape, Y.shape)\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_step]))\n",
    "b = tf.Variable(tf.random_normal([len(seq_data), n_class]))\n",
    "print(W.shape)\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "# outputs : [batch_size, n_step, n_hidden]\n",
    "outputs = tf.transpose(outputs, [0, 1, 2]) # [n_step, batch_size, n_hidden]\n",
    "outputs = outputs[-1] # [batch_size, n_hidden]\n",
    "print(outputs.shape)\n",
    "model = tf.reshape(tf.matmul(outputs, new_W),[9]) + new_b # model : [batch_size, n_class]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(.5).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5370c9",
   "metadata": {},
   "source": [
    "# Build Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "100bd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Neural Network\n",
    "import tensorflow as tf\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "    xs = tf.cast(xs, tf.float32) / 255.0\n",
    "    ys = tf.cast(ys, tf.float32)\n",
    "    ys = tf.one_hot(ys, depth=n_classes)\n",
    "    return tf.data.Dataset.from_tensor_slices((xs, ys)).map(preprocess).shuffle(len(ys)).batch(128)\n",
    "\n",
    "def val_nn(training_inputs_data, training_outputs_data, test_inputs):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    training_inputs = tensorflow.compat.v1.placeholder(shape=[None, 3], dtype=tensorflow.float32)  \n",
    "    training_outputs = tensorflow.compat.v1.placeholder(shape=[None, 1], dtype=tensorflow.float32) #Desired outputs for each input  \n",
    "    weights = tensorflow.Variable(initial_value=[[.3], [.1], [.8]], dtype=tensorflow.float32)  \n",
    "    bias = tensorflow.Variable(initial_value=[[1]], dtype=tensorflow.float32)  \n",
    "\n",
    "    af_input = tensorflow.matmul(training_inputs, weights) + bias  \n",
    "  \n",
    "    # Activation function of the output layer neuron  \n",
    "    predictions = tensorflow.nn.sigmoid(af_input)  \n",
    "    # Measuring the prediction error of the network after being trained  \n",
    "    prediction_error = tensorflow.reduce_sum(training_outputs - predictions)  \n",
    "    # Minimizing the prediction error using gradient descent optimizer  \n",
    "    \n",
    "    train_op = tensorflow.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(prediction_error) \n",
    "    # Creating a TensorFlow Session  \n",
    "    sess = tensorflow.compat.v1.Session()  \n",
    "    # Initializing the TensorFlow Variables (weights and bias)  \n",
    "    sess.run(tensorflow.compat.v1.global_variables_initializer())  \n",
    "    \n",
    "    # Training loop of the neural network  \n",
    "    for step in range(10000):  \n",
    "        sess.run(fetches=train_op, feed_dict={training_inputs: training_inputs_data, training_outputs: training_outputs_data})  \n",
    "        # Class scores of some testing data  \n",
    "    score= sess.run(fetches=predictions, feed_dict={training_inputs: [[248, 80, 68], [0, 0, 255]]})\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    predictions =  sess.run(fetches=test_inputs)\n",
    "    # Closing the TensorFlow Session to free resources  \n",
    "    sess.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "213165e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2., 13.],\n",
       "       [ 7.,  9.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "X_test=tensorflow.convert_to_tensor(value=X_test, dtype=tensorflow.float32)\n",
    "val_nn(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f891b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop Neural Network\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = 1.0 / (1.0 + exp(-activation))\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            transfer_derivative = neuron['output'] * (1.0 - neuron['output'])\n",
    "            neuron['delta'] = errors[j] * transfer_derivative\n",
    "\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    "\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e79752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': 0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': -0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': 0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': -0.03803132596437354}]\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=0, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836, 2.550537003, 0],\n",
    "           [1.465489372, 2.362125076, 0],\n",
    "           [3.396561688, 4.400293529, 0],\n",
    "           [1.38807019, 1.850220317, 0],\n",
    "           [3.06407232, 3.005305973, 0],\n",
    "           [7.627531214, 2.759262235, 1],\n",
    "           [5.332441248, 2.088626775, 1],\n",
    "           [6.922596716, 1.77106367, 1],\n",
    "           [8.675418651, -0.242068655, 1],\n",
    "           [7.673756466, 3.508563011, 1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "           [{'weights': [0.2550690257394217, 0.49543508709194095]},\n",
    "            {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1c775",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "290c624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = [[255, 0, 0], [248, 80, 68], [0, 0, 255],[67, 15, 210]]  \n",
    "Y_train = [[1], [1], [0], [0]] \n",
    "X_test = [[5, 2, 13], [7, 9, 0]]\n",
    "xt=tf.constant(X_train)\n",
    "xt.shape, tf.rank(xt)\n",
    "y1 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "y2 = tf.random.uniform(shape=[5, 300], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob4=tf.matmul(y1, tf.transpose(y2))\n",
    "prob5=tf.tensordot(y1, tf.transpose(y2), axes=1)\n",
    "y6 = tf.random.uniform(shape=[224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob6=tf.math.reduce_max(y6, axis=0)\n",
    "y7 = tf.random.uniform(shape=[1, 224, 224, 3], minval=0, maxval=1, dtype=tf.float64)\n",
    "prob7 = tf.squeeze(y7, axis=0)\n",
    "y8 = tf.random.uniform(shape=[10], minval=0, maxval=10, dtype=tf.int64)\n",
    "prob9=tf.math.argmax(y8)\n",
    "prob10=tf.one_hot(y8, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceacfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
