{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12242ac5",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3167760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# Timeseries LSTM Autoencoder\n",
    "def vtoc_lstm_autoencoder(n_steps, n_horizon, n_features, lr):\n",
    "    serie_size=n_steps\n",
    "    encoder_decoder = Sequential()\n",
    "    encoder_decoder.add(LSTM(n_steps, activation='relu', input_shape=(n_steps, n_features), return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(1, activation='relu'))\n",
    "    encoder_decoder.add(RepeatVector(serie_size))\n",
    "    encoder_decoder.add(LSTM(serie_size, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "    encoder_decoder.add(TimeDistributed(Dense(1)))\n",
    "    encoder_decoder.summary()\n",
    "\n",
    "    adam = Adam(lr)\n",
    "    encoder_decoder.compile(loss='mse', optimizer=adam)\n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5c33ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "#Timeseries LSTM Model\n",
    "def vtoc_lstm_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(72, activation='relu', input_shape=(n_steps, n_features), return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=True)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128, activation='relu')),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries CNN Model\n",
    "def vtoc_cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    #tf.keras.layers.Input(shape=(n_steps, n_features)),\n",
    "    #tf.keras.layers.Flatten(input_shape=(n_steps, n_features)),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Flatten()),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss= Huber()\n",
    "    optimizer =Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries DNN Model\n",
    "def vtoc_dnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(n_steps, n_features)), #Use with evaluate_timeseries_model\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='dnn')\n",
    "    \n",
    "    loss=tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Model\n",
    "def lstm_cnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_steps,n_features))),\n",
    "    model.add(Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features))),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu')),\n",
    "    model.add(MaxPooling1D(2)),\n",
    "    model.add(LSTM(72, activation='relu', return_sequences=True)),\n",
    "    model.add(LSTM(48, activation='relu', return_sequences=False)),\n",
    "    model.add(Flatten()),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(128)),\n",
    "    model.add(Dropout(0.3)),\n",
    "    model.add(Dense(n_horizon))\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Timeseries LSTM-CNN Skip Model\n",
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(72, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(48, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, skip_flatten])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a260d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Prepare timeseries data\n",
    "def multi_baseline_eror(data, pred_cols):\n",
    "    df = data[pred_cols]\n",
    "    #fill nans with linear interpolation because this is how we will fill when using the data in the models.\n",
    "    df_filled = df.interpolate(\"linear\")\n",
    "    mm = MinMaxScaler()\n",
    "    df_scaled = mm.fit_transform(df_filled)\n",
    "    df_prep = pd.DataFrame(df_scaled, columns=pred_cols)\n",
    "    y_true = df_prep[pred_cols[0]]\n",
    "    y_pred_forecast = df_prep[pred_cols[1]]\n",
    "\n",
    "    ### persistence 1 day\n",
    "    #shift series by 24 hours\n",
    "    # realign y_true to have the same length and time samples\n",
    "    y_preds_persistance_1_day = y_true.shift(24).dropna()\n",
    "    persistence_1_day_mae = tf.keras.losses.MAE(y_true[y_preds_persistance_1_day.index], y_preds_persistance_1_day).numpy()\n",
    "    persistence_1_day_mape = tf.keras.losses.MAPE(np.maximum(y_true[y_preds_persistance_1_day.index], 1e-5), np.maximum(y_preds_persistance_1_day, 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ### persistence 3 day average\n",
    "    #shift by 1, 2, 3 days. Realign to have same lengths. Average days and calcualte MAE.\n",
    "\n",
    "    shift_dfs = list()\n",
    "    for i in range(1, 4):\n",
    "        shift_dfs.append(pd.Series(y_true.shift(24 * i), name=f\"d{i}\"))\n",
    "\n",
    "    y_persistance_3d = pd.concat(shift_dfs, axis=1).dropna()\n",
    "    y_persistance_3d[\"avg\"] = (y_persistance_3d[\"d1\"] + y_persistance_3d[\"d2\"] + y_persistance_3d[\"d3\"])/3\n",
    "    d3_idx = y_persistance_3d.index\n",
    "    persistence_3day_avg_mae = tf.keras.losses.MAE(y_true[d3_idx], y_persistance_3d['avg']).numpy()\n",
    "    persistence_3day_avg_mape = tf.keras.losses.MAPE(np.maximum(y_true[d3_idx], 1e-5), np.maximum(y_persistance_3d['avg'], 1e-5)).numpy()\n",
    "\n",
    "\n",
    "    ref_error = pd.DataFrame({\n",
    "        \"Method\": [\"TSO Forecast\", \"Persistence 1 Day\", \"Persitence 3 Day Avg\"],\n",
    "        \"MAE\": [tf.keras.losses.MAE(y_true, y_pred_forecast).numpy(),\n",
    "                persistence_1_day_mae,\n",
    "                persistence_3day_avg_mae],\n",
    "        \"MAPE\":[tf.keras.losses.MAPE(np.maximum(y_true, 1e-5), np.maximum(y_pred_forecast, 1e-5)).numpy(),\n",
    "                persistence_1_day_mape,\n",
    "                persistence_3day_avg_mape]}, \n",
    "        index=[i for i in range(3)])\n",
    "    return ref_error\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "def split_data(series, train_fraq, test_len=8760):\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "\n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_window_data(dataset, look_back=1, n_features=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :n_features]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :n_features])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "    \n",
    "    #expand dimensions to 3D to fit with LSTM inputs\n",
    "    #creat the inital tensor dataset\n",
    "    if expand_dims:\n",
    "        ds = tf.expand_dims(data, axis=-1)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    \n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_multi_dataset(data, pred_cols, multivar):\n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    for column in pred_cols:\n",
    "        # Getting rid of outliers\n",
    "        data.loc[data[column] == -9999.0, column] = 0.0\n",
    "    ref_error = multi_baseline_eror(timeseries_df, pred_cols)\n",
    "    data=MinMaxScaler().fit_transform(data)\n",
    "    tf.random.set_seed(42)\n",
    "    train_fraq=0.65\n",
    "    lr = 3e-4\n",
    "    n_steps = 14#24*30\n",
    "    n_horizon = 14\n",
    "    batch_size = 64#256\n",
    "    shuffle_buffer = 100\n",
    "    if multivar:\n",
    "        n_features=len(data[0])\n",
    "    else:\n",
    "        n_features=1\n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=8760)\n",
    "    \n",
    "    (X_train, Y_train)=create_window_data(train_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_test, Y_test)=create_window_data(test_data, look_back=n_horizon, n_features=n_features)\n",
    "    (X_val, Y_val)=create_window_data(val_data, look_back=n_horizon, n_features=n_features)\n",
    "    #train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #test_ds = window_dataset(test_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #val_ds = window_dataset(val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multivar)\n",
    "    #split_sequences(train_ds, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False)\n",
    "    #print(f\"Train Data Shape: {train_ds.shape}\")\n",
    "    #print(f\"Val Data Shape: {val_ds.shape}\")\n",
    "    #kfold = KFold(n_folds=5, shuffle=True, random_state=1)\n",
    "    model=vtoc_dnn_model(n_steps, n_horizon, n_features, lr)\n",
    "    model.summary()\n",
    "    model_hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, verbose=3)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    model.evaluate(X_train, Y_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    #_, acc = model.evaluate(test, verbose=2)\n",
    "    #scores.append(acc)\n",
    "    return model_hist.history\n",
    "\n",
    "def evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=False):\n",
    "    df_processed = create_multi_dataset(timeseries_df, pred_cols = ['wv (m/s)', 'max. wv (m/s)'], multivar=multivar)\n",
    "    print(df_processed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b94ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path) #We load the dataset in a csv_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16fb1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 196)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25216     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,534\n",
      "Trainable params: 43,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "8365/8365 [==============================] - 29s 3ms/step - loss: 0.0016 - mae: 0.0342\n",
      "4504/4504 [==============================] - 7s 2ms/step\n",
      "{'loss': [0.002608983078971505, 0.0016388605581596494, 0.0015648044645786285, 0.0015329904854297638, 0.0015151449479162693], 'mae': [0.044424545019865036, 0.03305155038833618, 0.03175380825996399, 0.031170841306447983, 0.030865579843521118], 'val_loss': [0.001654260908253491, 0.001841593300923705, 0.0017942589474841952, 0.0014471292961388826, 0.0016615190543234348], 'val_mae': [0.03435081988573074, 0.03647252917289734, 0.03513265773653984, 0.029650719836354256, 0.03391486033797264]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "with tf.device('/device:GPU:0'):\n",
    "    evaluate_multi_timeseries_model(timeseries_df, n_folds=5, multivar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa18b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import datetime\n",
    "\n",
    "def create_dataset(X, y, delay=24, lookback=48):\n",
    "    window_length = lookback + delay\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)-delay):\n",
    "        v = X.iloc[i-lookback:i].to_numpy() # every one hour, we take the past 48 hours of features\n",
    "        Xs.append(v)\n",
    "        w = y.iloc[i+delay] # Every timestep, we take the temperature the next delay (here one day)\n",
    "        ys.append(w)\n",
    "    return(np.array(Xs), np.array(ys))\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    # Getting rid of outliers\n",
    "    data.loc[data['wv (m/s)'] == -9999.0, 'wv (m/s)'] = 0.0\n",
    "    data.loc[data['max. wv (m/s)'] == -9999.0, 'max. wv (m/s)'] = 0.0\n",
    "    \n",
    "    # Taking values every hours\n",
    "    data = data[5::6]# df[start,stop,step]\n",
    "    \n",
    "    wv = data.pop('wv (m/s)')\n",
    "    max_wv = data.pop('max. wv (m/s)')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = data.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components.\n",
    "    data['Wx'] = wv*np.cos(wd_rad)\n",
    "    data['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "    # Calculate the max wind x and y components.\n",
    "    data['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "    data['max Wy'] = max_wv*np.sin(wd_rad)\n",
    "    \n",
    "    date_time = pd.to_datetime(data.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    \n",
    "    day = 24*60*60 # Time is second within a single day\n",
    "    year = 365.2425*day # Time in second withon a year\n",
    "\n",
    "    data['Day sin'] = np.sin(timestamp_s * (2*np.pi / day))\n",
    "    data['Day cos'] = np.cos(timestamp_s * (2*np.pi / day))\n",
    "    data['Year sin'] = np.sin(timestamp_s * (2*np.pi / year))\n",
    "    data['Year cos'] = np.cos(timestamp_s * (2*np.pi / year))\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def split(data):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    train_df = data.iloc[0: n * 70 //100] # \"iloc\" because we have to select the lines at the indicies 0 to int(n*0.7) compared to \"loc\"\n",
    "    val_df = data.iloc[n * 70 //100 : n * 90 //100]\n",
    "    test_df = data.iloc[n * 90 //100:]\n",
    "    \n",
    "    return(train_df, val_df, test_df)\n",
    "\n",
    "def naive_eval_arr(X, y, lookback, delay):\n",
    "    batch_maes = []\n",
    "    for i in range(0, len(X)):\n",
    "        preds = X[i, -1, 1] #For all elements in the batch, we are saying the prediction of temperature is equal to the last temperature recorded within the 48 hours\n",
    "        mae = np.mean(np.abs(preds - y[i]))\n",
    "        batch_maes.append(mae)\n",
    "    return(np.mean(batch_maes))\n",
    "\n",
    "def split_sequences(sequences, n_steps, extra_lag=False, long_lag_step=7, max_step=30, idx=0, multivar=False):\n",
    "    #if not adding extra lag features adjust max_step and n_steps to aling\n",
    "    if not extra_lag:\n",
    "        max_step=n_steps\n",
    "        n_steps+=1\n",
    "        \n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        #end_ix = i + n_steps\n",
    "        end_ix = i + max_step\n",
    "        \n",
    "        #create a list with the indexes we want to include in each sample\n",
    "        slices = [x for x in range(end_ix-1,end_ix-n_steps, -1)] + [y for y in range(end_ix-n_steps, i, -long_lag_step)]\n",
    "        \n",
    "        #reverse the slice indexes\n",
    "        slices = list(reversed(slices))\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[slices, :]\n",
    "        seq_y = sequences[end_ix, :]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if multivar:\n",
    "        #unstack the 3rd dimension and select the first element(energy load)\n",
    "        y = y[:,idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=True, batch_size=32, step=1):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while True:\n",
    "        if shuffle == True: # pay attention ! We are not shuffeling timesteps but elemnts within a batch ! It is important to keep the data in time order\n",
    "            rows = np.random.randint(min_index + lookback, max_index-delay-1, size=batch_size) # return an array containing size elements ranging from min_index+lookback to max_index\n",
    "        else:\n",
    "            if i + batch_size >= max_index-delay-1: #Since we are incrementing on \"i\". If its value is greater than the max_index --> start from the begining\n",
    "                i = min_index + lookback # We need to start from the indice lookback, since we want to take lookback elements here.\n",
    "            rows = np.arange(i, min(i + batch_size, max_index)) # Just creating an array that contain the indices of each sample in the batch\n",
    "            i+=len(rows) # rows represents the number of sample in one batch\n",
    "            \n",
    "        samples = np.zeros((len(rows), lookback//step, data.shape[-1])) # shape = (batch_size, lookback, nbr_of_features)\n",
    "        targets = np.zeros((len(rows),)) #Shape = (batch_size,)\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step) #From one indice given by rows[j], we are picking loockback previous elements in the dataset\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1] #We only want to predict the temperature for now,since [1], the second column\n",
    "        return samples, targets # The yield that replace the return to create a generator and not a regular function.\n",
    "        \n",
    "def evaluate_timeseries_model(df, n_folds=5, multivar=False):\n",
    "    scores, histories = list(), list()\n",
    "    df_processed = preprocessing(df)\n",
    "    train_df, val_df, test_df = split(df_processed)\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean)/train_std # As simple as that !\n",
    "    val_df = (val_df - train_mean)/train_std\n",
    "    test_df = (test_df - train_mean)/train_std\n",
    "    lookback = 48 # Looking at all features for the past 2 days\n",
    "    delay = 24 # Trying to predict the temperature for the next day\n",
    "    batch_size = 64 # Features will be batched 32 by 32.\n",
    "    X_train, Y_train = create_dataset(train_df, train_df['T (degC)'], delay = delay, lookback = lookback)\n",
    "    X_val, Y_val = create_dataset(val_df, val_df['T (degC)'], delay = delay)\n",
    "    naive_loss_arr = naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay)\n",
    "\n",
    "    naive_loss_arr = round(naive_eval_arr(X_val, Y_val, lookback = lookback, delay = delay),2) # Round the value\n",
    "    \n",
    "\n",
    "    data_train = train_df.to_numpy()\n",
    "    (X_train, Y_train) = generator(data = data_train, lookback = lookback, delay =delay, min_index = 0, \n",
    "                                   max_index = len(data_train), shuffle = True, batch_size = batch_size)\n",
    "\n",
    "    data_val = val_df.to_numpy()\n",
    "    (X_val, Y_val) = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0, \n",
    "                               max_index = len(data_val), batch_size = batch_size)\n",
    "\n",
    "    data_test = test_df.to_numpy()\n",
    "    test_gen = generator(data = data_val, lookback = lookback, delay =delay, min_index = 0,\n",
    "                         max_index = len(data_test), batch_size = batch_size)\n",
    "    tscv = TimeSeriesSplit(n_splits=5, test_size=2)\n",
    "    \n",
    "    '''for train_index, test_index in tscv.split(X_train):\n",
    "        X_train, X_test = X_train[train_index], X_train[test_index]\n",
    "        Y_train, Y_test = Y_train[train_index], Y_train[test_index]'''\n",
    "    print(naive_loss_arr)\n",
    "    lr = 3e-4\n",
    "    n_steps=48#24*30\n",
    "    n_horizon=24\n",
    "    #batch_size=64\n",
    "    if multivar:\n",
    "        n_features=5\n",
    "    else:\n",
    "        n_features=1\n",
    "    model=vtoc_cnn_model(n_steps, n_horizon, n_features, lr)\n",
    "\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, verbose=3, shuffle=True)\n",
    "    #model.fit(X_train, X_train, epochs=10, verbose=3, shuffle=True)\n",
    "    train_encoded = model.predict(X_train)\n",
    "    validation_encoded = model.predict(X_val)\n",
    "    print('Encoded time-series shape', train_encoded.shape)\n",
    "    print('Encoded time-series sample', train_encoded[0])\n",
    "    return model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a6c93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#csv_path = \"/kaggle/input/energy-consumption-generation-prices-and-weather/energy_dataset.csv\"\n",
    "timeseries_df = pd.read_csv(csv_path)\n",
    "#evaluate_timeseries_model(timeseries_df, n_folds=5, multivar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c309969",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80a6c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Basic ANN\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if logs['accuracy'] >0.90:\n",
    "            print(\"Accuracy greater than 90%. Stopping Training.\")\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "def val_dnn_model(epochs, X_train, Y_train, X_val, Y_val, callbacks=None):\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f2324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 15ms/step - loss: 1.4392 - accuracy: 0.6114 - val_loss: 0.7750 - val_accuracy: 0.6233\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 1.1206 - accuracy: 0.6157 - val_loss: 0.6447 - val_accuracy: 0.7033\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8553 - accuracy: 0.6257 - val_loss: 0.6501 - val_accuracy: 0.6900\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7674 - accuracy: 0.6686 - val_loss: 0.7009 - val_accuracy: 0.7067\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7329 - accuracy: 0.6914 - val_loss: 0.6520 - val_accuracy: 0.7067\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6660 - accuracy: 0.6971 - val_loss: 0.5500 - val_accuracy: 0.7133\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6634 - accuracy: 0.7043 - val_loss: 0.5326 - val_accuracy: 0.7267\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6450 - accuracy: 0.7129 - val_loss: 0.5418 - val_accuracy: 0.7400\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6165 - accuracy: 0.7000 - val_loss: 0.5371 - val_accuracy: 0.7433\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6055 - accuracy: 0.7186 - val_loss: 0.5474 - val_accuracy: 0.7400\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6062 - accuracy: 0.7371 - val_loss: 0.5392 - val_accuracy: 0.7400\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6180 - accuracy: 0.7071 - val_loss: 0.5260 - val_accuracy: 0.7433\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6034 - accuracy: 0.7200 - val_loss: 0.5164 - val_accuracy: 0.7600\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5607 - accuracy: 0.7200 - val_loss: 0.5201 - val_accuracy: 0.7567\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5718 - accuracy: 0.7343 - val_loss: 0.5166 - val_accuracy: 0.7467\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5430 - accuracy: 0.7271 - val_loss: 0.5070 - val_accuracy: 0.7567\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5696 - accuracy: 0.7200 - val_loss: 0.5098 - val_accuracy: 0.7633\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5631 - accuracy: 0.7229 - val_loss: 0.5047 - val_accuracy: 0.7700\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5531 - accuracy: 0.7514 - val_loss: 0.5150 - val_accuracy: 0.7700\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5388 - accuracy: 0.7400 - val_loss: 0.5281 - val_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5621 - accuracy: 0.7371 - val_loss: 0.5080 - val_accuracy: 0.7600\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5140 - accuracy: 0.7571 - val_loss: 0.5162 - val_accuracy: 0.7567\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5327 - accuracy: 0.7300 - val_loss: 0.5128 - val_accuracy: 0.7500\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5317 - accuracy: 0.7386 - val_loss: 0.5103 - val_accuracy: 0.7567\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5244 - accuracy: 0.7557 - val_loss: 0.5045 - val_accuracy: 0.7467\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5090 - accuracy: 0.7614 - val_loss: 0.5027 - val_accuracy: 0.7567\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5077 - accuracy: 0.7629 - val_loss: 0.5076 - val_accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5386 - accuracy: 0.7500 - val_loss: 0.5084 - val_accuracy: 0.7633\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5311 - accuracy: 0.7386 - val_loss: 0.5080 - val_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5170 - accuracy: 0.7500 - val_loss: 0.5034 - val_accuracy: 0.7467\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5123 - accuracy: 0.7643 - val_loss: 0.5002 - val_accuracy: 0.7600\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5178 - accuracy: 0.7586 - val_loss: 0.4937 - val_accuracy: 0.7600\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4829 - accuracy: 0.7771 - val_loss: 0.4932 - val_accuracy: 0.7767\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4962 - accuracy: 0.7657 - val_loss: 0.4841 - val_accuracy: 0.7767\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5112 - accuracy: 0.7529 - val_loss: 0.4925 - val_accuracy: 0.7667\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4965 - accuracy: 0.7614 - val_loss: 0.4956 - val_accuracy: 0.7700\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5147 - accuracy: 0.7614 - val_loss: 0.5170 - val_accuracy: 0.7433\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5018 - accuracy: 0.7557 - val_loss: 0.4951 - val_accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4988 - accuracy: 0.7600 - val_loss: 0.4952 - val_accuracy: 0.7767\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5125 - accuracy: 0.7629 - val_loss: 0.4929 - val_accuracy: 0.7700\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4973 - accuracy: 0.7643 - val_loss: 0.5019 - val_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4959 - accuracy: 0.7657 - val_loss: 0.4953 - val_accuracy: 0.7767\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4985 - accuracy: 0.7514 - val_loss: 0.4994 - val_accuracy: 0.7733\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4775 - accuracy: 0.7743 - val_loss: 0.4933 - val_accuracy: 0.7733\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4774 - accuracy: 0.7771 - val_loss: 0.4879 - val_accuracy: 0.7800\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4756 - accuracy: 0.7814 - val_loss: 0.4895 - val_accuracy: 0.7867\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4733 - accuracy: 0.7843 - val_loss: 0.4879 - val_accuracy: 0.7800\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4704 - accuracy: 0.7986 - val_loss: 0.4933 - val_accuracy: 0.7933\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4884 - accuracy: 0.7671 - val_loss: 0.4971 - val_accuracy: 0.7700\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4669 - accuracy: 0.7700 - val_loss: 0.5045 - val_accuracy: 0.7433\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4759 - accuracy: 0.7814 - val_loss: 0.4930 - val_accuracy: 0.7733\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4819 - accuracy: 0.7729 - val_loss: 0.4969 - val_accuracy: 0.7667\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4856 - accuracy: 0.7771 - val_loss: 0.5038 - val_accuracy: 0.7700\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4755 - accuracy: 0.7786 - val_loss: 0.4993 - val_accuracy: 0.7767\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4727 - accuracy: 0.7743 - val_loss: 0.4932 - val_accuracy: 0.7733\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4714 - accuracy: 0.7729 - val_loss: 0.4924 - val_accuracy: 0.7700\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4762 - accuracy: 0.7771 - val_loss: 0.4886 - val_accuracy: 0.7767\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4688 - accuracy: 0.7871 - val_loss: 0.4980 - val_accuracy: 0.7733\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4617 - accuracy: 0.7943 - val_loss: 0.4944 - val_accuracy: 0.7867\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4764 - accuracy: 0.7843 - val_loss: 0.4901 - val_accuracy: 0.7700\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4545 - accuracy: 0.7986 - val_loss: 0.4895 - val_accuracy: 0.7800\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4538 - accuracy: 0.7943 - val_loss: 0.4936 - val_accuracy: 0.7667\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4496 - accuracy: 0.8014 - val_loss: 0.4911 - val_accuracy: 0.7833\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4285 - accuracy: 0.7929 - val_loss: 0.4946 - val_accuracy: 0.7767\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4500 - accuracy: 0.7800 - val_loss: 0.4944 - val_accuracy: 0.7567\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4362 - accuracy: 0.8000 - val_loss: 0.4925 - val_accuracy: 0.7833\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4386 - accuracy: 0.8086 - val_loss: 0.4906 - val_accuracy: 0.7833\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4480 - accuracy: 0.8000 - val_loss: 0.4908 - val_accuracy: 0.7633\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4337 - accuracy: 0.8029 - val_loss: 0.4994 - val_accuracy: 0.7800\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4320 - accuracy: 0.8100 - val_loss: 0.4890 - val_accuracy: 0.7767\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4308 - accuracy: 0.8100 - val_loss: 0.4900 - val_accuracy: 0.7700\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4490 - accuracy: 0.7843 - val_loss: 0.4934 - val_accuracy: 0.7667\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4401 - accuracy: 0.8000 - val_loss: 0.4965 - val_accuracy: 0.7600\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4360 - accuracy: 0.7914 - val_loss: 0.4915 - val_accuracy: 0.7700\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4238 - accuracy: 0.8157 - val_loss: 0.5013 - val_accuracy: 0.7933\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4361 - accuracy: 0.7957 - val_loss: 0.5054 - val_accuracy: 0.7767\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4349 - accuracy: 0.8157 - val_loss: 0.5010 - val_accuracy: 0.7833\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4184 - accuracy: 0.8057 - val_loss: 0.4960 - val_accuracy: 0.7667\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4292 - accuracy: 0.8057 - val_loss: 0.4958 - val_accuracy: 0.7700\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4254 - accuracy: 0.8143 - val_loss: 0.4926 - val_accuracy: 0.7967\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4165 - accuracy: 0.8100 - val_loss: 0.4930 - val_accuracy: 0.7767\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4202 - accuracy: 0.8057 - val_loss: 0.5021 - val_accuracy: 0.7767\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4198 - accuracy: 0.8171 - val_loss: 0.5082 - val_accuracy: 0.7567\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4296 - accuracy: 0.8057 - val_loss: 0.5010 - val_accuracy: 0.7633\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4299 - accuracy: 0.7871 - val_loss: 0.4905 - val_accuracy: 0.7700\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4287 - accuracy: 0.8157 - val_loss: 0.5199 - val_accuracy: 0.7900\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4313 - accuracy: 0.8143 - val_loss: 0.5003 - val_accuracy: 0.7767\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4376 - accuracy: 0.8029 - val_loss: 0.4983 - val_accuracy: 0.7767\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4232 - accuracy: 0.8114 - val_loss: 0.5006 - val_accuracy: 0.7767\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4028 - accuracy: 0.8086 - val_loss: 0.4963 - val_accuracy: 0.7667\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4128 - accuracy: 0.8129 - val_loss: 0.4987 - val_accuracy: 0.7767\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4316 - accuracy: 0.7971 - val_loss: 0.4888 - val_accuracy: 0.7933\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4338 - accuracy: 0.8100 - val_loss: 0.4997 - val_accuracy: 0.7833\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4052 - accuracy: 0.8171 - val_loss: 0.5043 - val_accuracy: 0.7767\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4024 - accuracy: 0.8200 - val_loss: 0.5006 - val_accuracy: 0.7800\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4149 - accuracy: 0.8157 - val_loss: 0.5057 - val_accuracy: 0.7833\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4215 - accuracy: 0.8014 - val_loss: 0.5060 - val_accuracy: 0.7700\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4150 - accuracy: 0.8229 - val_loss: 0.5052 - val_accuracy: 0.7633\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4180 - accuracy: 0.8057 - val_loss: 0.5054 - val_accuracy: 0.7767\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4100 - accuracy: 0.8200 - val_loss: 0.5086 - val_accuracy: 0.7733\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8429\n",
      "Training Set:   [0.3707040846347809, 0.8428571224212646]\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5086 - accuracy: 0.7733\n",
      "Validation Set: [0.5085697174072266, 0.7733333110809326]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = tfds.load('german_credit_numeric', split=['train'], batch_size=-1, as_supervised=True)\n",
    "X=tfds.as_numpy(train[0][0])\n",
    "Y=tfds.as_numpy(train[0][1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "hist_regularized, model_regularized = val_dnn_model(100, X_train, Y_train, X_test, Y_test, callbacks=[EarlyStoppingCallback()])\n",
    "print(f\"Training Set:   {model_regularized.evaluate(X_train, Y_train)}\")\n",
    "print(f\"Validation Set: {model_regularized.evaluate(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75d585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "def vtoc_regression_model(norm, model_type):\n",
    "    if model_type=='linear':\n",
    "        model = Sequential()\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_regression_data(dataset, target):\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset.isna().sum()\n",
    "    dataset=dataset.dropna()\n",
    "    #dataset['Origin'] = dataset['Origin'].map({1.0: 'USA', 2.0: 'Europe', 3.0: 'Japan'})\n",
    "    Y=dataset[target]\n",
    "    X=dataset.loc[:, dataset.columns != target]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n",
    "    horsepower = np.array(X_train['Horsepower']).astype('float32')\n",
    "\n",
    "    horsepower_normalizer = Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "    \n",
    "    linear_model=vtoc_regression_model(horsepower_normalizer, model_type='linear')\n",
    "    history = linear_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=2)\n",
    "    test_results = linear_model.predict(X_test)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7480131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 - 0s - loss: 231.0550 - val_loss: 290.7013 - 349ms/epoch - 39ms/step\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 164.8466 - val_loss: 26.5685 - 119ms/epoch - 13ms/step\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 89.5644 - val_loss: 49.4564 - 100ms/epoch - 11ms/step\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 32.7282 - val_loss: 28.5107 - 103ms/epoch - 11ms/step\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 22.5977 - val_loss: 8.0420 - 79ms/epoch - 9ms/step\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 8.5712 - val_loss: 13.5126 - 81ms/epoch - 9ms/step\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 16.3905 - val_loss: 9.2972 - 67ms/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 35.3781 - val_loss: 11.4659 - 80ms/epoch - 9ms/step\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 14.0517 - val_loss: 18.2754 - 77ms/epoch - 9ms/step\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 16.9186 - val_loss: 32.8455 - 84ms/epoch - 9ms/step\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 11.7090 - val_loss: 9.9018 - 142ms/epoch - 16ms/step\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 10.5167 - val_loss: 43.4549 - 136ms/epoch - 15ms/step\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 35.9725 - val_loss: 43.8688 - 70ms/epoch - 8ms/step\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 52.5922 - val_loss: 91.5081 - 68ms/epoch - 8ms/step\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 59.7495 - val_loss: 25.4189 - 71ms/epoch - 8ms/step\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 50.3082 - val_loss: 13.7712 - 67ms/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 9.6793 - val_loss: 11.8424 - 62ms/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 8.4244 - val_loss: 6.2219 - 58ms/epoch - 6ms/step\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 14.3290 - val_loss: 47.5983 - 59ms/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 47.1582 - val_loss: 60.4549 - 58ms/epoch - 6ms/step\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 59.8589 - val_loss: 36.6870 - 65ms/epoch - 7ms/step\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 57.8566 - val_loss: 34.4942 - 65ms/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 39.9671 - val_loss: 20.7316 - 60ms/epoch - 7ms/step\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 35.9354 - val_loss: 63.6661 - 59ms/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 56.7610 - val_loss: 10.4825 - 55ms/epoch - 6ms/step\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 17.5762 - val_loss: 18.2828 - 60ms/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 18.8054 - val_loss: 28.4866 - 60ms/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 19.3754 - val_loss: 13.4766 - 59ms/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 19.5100 - val_loss: 15.7004 - 59ms/epoch - 7ms/step\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 18.3483 - val_loss: 11.0659 - 59ms/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 19.2071 - val_loss: 16.9530 - 63ms/epoch - 7ms/step\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 24.2225 - val_loss: 30.0491 - 58ms/epoch - 6ms/step\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 34.4608 - val_loss: 3.1661 - 71ms/epoch - 8ms/step\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 8.5103 - val_loss: 17.0906 - 68ms/epoch - 8ms/step\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 29.9465 - val_loss: 67.7897 - 67ms/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 55.8791 - val_loss: 11.8340 - 70ms/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 17.6040 - val_loss: 16.0264 - 75ms/epoch - 8ms/step\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 18.4623 - val_loss: 29.1145 - 67ms/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 20.3347 - val_loss: 38.3329 - 87ms/epoch - 10ms/step\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 49.4726 - val_loss: 114.7535 - 76ms/epoch - 8ms/step\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 91.8438 - val_loss: 75.4737 - 71ms/epoch - 8ms/step\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 42.0845 - val_loss: 36.1096 - 73ms/epoch - 8ms/step\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 36.6106 - val_loss: 19.0706 - 70ms/epoch - 8ms/step\n",
      "Epoch 44/100\n",
      "9/9 - 0s - loss: 28.2720 - val_loss: 4.2670 - 73ms/epoch - 8ms/step\n",
      "Epoch 45/100\n",
      "9/9 - 0s - loss: 17.9850 - val_loss: 43.4785 - 67ms/epoch - 7ms/step\n",
      "Epoch 46/100\n",
      "9/9 - 0s - loss: 37.2763 - val_loss: 30.2308 - 60ms/epoch - 7ms/step\n",
      "Epoch 47/100\n",
      "9/9 - 0s - loss: 36.0434 - val_loss: 14.8247 - 74ms/epoch - 8ms/step\n",
      "Epoch 48/100\n",
      "9/9 - 0s - loss: 32.7237 - val_loss: 23.0098 - 68ms/epoch - 8ms/step\n",
      "Epoch 49/100\n",
      "9/9 - 0s - loss: 34.7945 - val_loss: 17.1758 - 72ms/epoch - 8ms/step\n",
      "Epoch 50/100\n",
      "9/9 - 0s - loss: 18.3651 - val_loss: 21.8865 - 71ms/epoch - 8ms/step\n",
      "Epoch 51/100\n",
      "9/9 - 0s - loss: 17.2057 - val_loss: 17.7007 - 79ms/epoch - 9ms/step\n",
      "Epoch 52/100\n",
      "9/9 - 0s - loss: 18.9188 - val_loss: 5.7622 - 76ms/epoch - 8ms/step\n",
      "Epoch 53/100\n",
      "9/9 - 0s - loss: 15.0085 - val_loss: 31.2784 - 73ms/epoch - 8ms/step\n",
      "Epoch 54/100\n",
      "9/9 - 0s - loss: 17.7970 - val_loss: 30.5717 - 75ms/epoch - 8ms/step\n",
      "Epoch 55/100\n",
      "9/9 - 0s - loss: 20.1554 - val_loss: 30.6217 - 70ms/epoch - 8ms/step\n",
      "Epoch 56/100\n",
      "9/9 - 0s - loss: 19.6728 - val_loss: 25.1946 - 68ms/epoch - 8ms/step\n",
      "Epoch 57/100\n",
      "9/9 - 0s - loss: 21.2872 - val_loss: 67.1470 - 66ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "9/9 - 0s - loss: 23.7287 - val_loss: 20.5163 - 61ms/epoch - 7ms/step\n",
      "Epoch 59/100\n",
      "9/9 - 0s - loss: 19.4008 - val_loss: 9.2367 - 71ms/epoch - 8ms/step\n",
      "Epoch 60/100\n",
      "9/9 - 0s - loss: 24.7478 - val_loss: 51.1190 - 58ms/epoch - 6ms/step\n",
      "Epoch 61/100\n",
      "9/9 - 0s - loss: 21.2939 - val_loss: 3.1549 - 57ms/epoch - 6ms/step\n",
      "Epoch 62/100\n",
      "9/9 - 0s - loss: 10.3272 - val_loss: 46.9247 - 53ms/epoch - 6ms/step\n",
      "Epoch 63/100\n",
      "9/9 - 0s - loss: 36.5932 - val_loss: 52.1956 - 55ms/epoch - 6ms/step\n",
      "Epoch 64/100\n",
      "9/9 - 0s - loss: 47.7434 - val_loss: 102.8872 - 62ms/epoch - 7ms/step\n",
      "Epoch 65/100\n",
      "9/9 - 0s - loss: 44.3543 - val_loss: 4.7107 - 58ms/epoch - 6ms/step\n",
      "Epoch 66/100\n",
      "9/9 - 0s - loss: 29.5823 - val_loss: 16.4534 - 59ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "9/9 - 0s - loss: 32.4985 - val_loss: 28.8167 - 66ms/epoch - 7ms/step\n",
      "Epoch 68/100\n",
      "9/9 - 0s - loss: 33.7966 - val_loss: 32.8490 - 70ms/epoch - 8ms/step\n",
      "Epoch 69/100\n",
      "9/9 - 0s - loss: 31.6030 - val_loss: 9.2226 - 87ms/epoch - 10ms/step\n",
      "Epoch 70/100\n",
      "9/9 - 0s - loss: 17.6679 - val_loss: 21.2345 - 76ms/epoch - 8ms/step\n",
      "Epoch 71/100\n",
      "9/9 - 0s - loss: 18.8371 - val_loss: 49.8182 - 59ms/epoch - 7ms/step\n",
      "Epoch 72/100\n",
      "9/9 - 0s - loss: 37.4482 - val_loss: 38.2868 - 69ms/epoch - 8ms/step\n",
      "Epoch 73/100\n",
      "9/9 - 0s - loss: 36.8873 - val_loss: 17.8581 - 68ms/epoch - 8ms/step\n",
      "Epoch 74/100\n",
      "9/9 - 0s - loss: 27.7476 - val_loss: 49.6260 - 70ms/epoch - 8ms/step\n",
      "Epoch 75/100\n",
      "9/9 - 0s - loss: 35.7549 - val_loss: 40.4830 - 71ms/epoch - 8ms/step\n",
      "Epoch 76/100\n",
      "9/9 - 0s - loss: 26.9945 - val_loss: 54.6800 - 68ms/epoch - 8ms/step\n",
      "Epoch 77/100\n",
      "9/9 - 0s - loss: 34.7609 - val_loss: 41.9603 - 64ms/epoch - 7ms/step\n",
      "Epoch 78/100\n",
      "9/9 - 0s - loss: 49.8772 - val_loss: 38.5500 - 67ms/epoch - 7ms/step\n",
      "Epoch 79/100\n",
      "9/9 - 0s - loss: 50.0076 - val_loss: 60.0998 - 62ms/epoch - 7ms/step\n",
      "Epoch 80/100\n",
      "9/9 - 0s - loss: 55.3288 - val_loss: 83.0530 - 75ms/epoch - 8ms/step\n",
      "Epoch 81/100\n",
      "9/9 - 0s - loss: 56.8725 - val_loss: 94.6463 - 123ms/epoch - 14ms/step\n",
      "Epoch 82/100\n",
      "9/9 - 0s - loss: 54.1468 - val_loss: 29.2230 - 71ms/epoch - 8ms/step\n",
      "Epoch 83/100\n",
      "9/9 - 0s - loss: 50.8579 - val_loss: 58.0240 - 64ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "9/9 - 0s - loss: 64.8713 - val_loss: 45.5247 - 58ms/epoch - 6ms/step\n",
      "Epoch 85/100\n",
      "9/9 - 0s - loss: 48.6255 - val_loss: 97.5019 - 58ms/epoch - 6ms/step\n",
      "Epoch 86/100\n",
      "9/9 - 0s - loss: 55.0300 - val_loss: 34.4709 - 58ms/epoch - 6ms/step\n",
      "Epoch 87/100\n",
      "9/9 - 0s - loss: 54.3597 - val_loss: 41.7874 - 71ms/epoch - 8ms/step\n",
      "Epoch 88/100\n",
      "9/9 - 0s - loss: 57.3993 - val_loss: 31.8896 - 59ms/epoch - 7ms/step\n",
      "Epoch 89/100\n",
      "9/9 - 0s - loss: 32.6342 - val_loss: 36.6163 - 57ms/epoch - 6ms/step\n",
      "Epoch 90/100\n",
      "9/9 - 0s - loss: 32.3820 - val_loss: 51.7041 - 57ms/epoch - 6ms/step\n",
      "Epoch 91/100\n",
      "9/9 - 0s - loss: 18.7867 - val_loss: 37.6886 - 55ms/epoch - 6ms/step\n",
      "Epoch 92/100\n",
      "9/9 - 0s - loss: 37.7455 - val_loss: 3.1943 - 58ms/epoch - 6ms/step\n",
      "Epoch 93/100\n",
      "9/9 - 0s - loss: 15.5021 - val_loss: 35.7002 - 70ms/epoch - 8ms/step\n",
      "Epoch 94/100\n",
      "9/9 - 0s - loss: 33.9451 - val_loss: 44.7141 - 72ms/epoch - 8ms/step\n",
      "Epoch 95/100\n",
      "9/9 - 0s - loss: 38.5440 - val_loss: 17.0859 - 57ms/epoch - 6ms/step\n",
      "Epoch 96/100\n",
      "9/9 - 0s - loss: 34.6955 - val_loss: 2.6448 - 72ms/epoch - 8ms/step\n",
      "Epoch 97/100\n",
      "9/9 - 0s - loss: 19.3012 - val_loss: 4.6197 - 55ms/epoch - 6ms/step\n",
      "Epoch 98/100\n",
      "9/9 - 0s - loss: 19.8220 - val_loss: 25.7089 - 54ms/epoch - 6ms/step\n",
      "Epoch 99/100\n",
      "9/9 - 0s - loss: 33.4909 - val_loss: 21.9347 - 55ms/epoch - 6ms/step\n",
      "Epoch 100/100\n",
      "9/9 - 0s - loss: 29.5453 - val_loss: 39.8524 - 58ms/epoch - 6ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.14522398],\n",
       "       [ -9.3601055 ],\n",
       "       [-29.113241  ],\n",
       "       [ -8.973978  ],\n",
       "       [-41.372612  ],\n",
       "       [-15.517832  ],\n",
       "       [-34.509792  ],\n",
       "       [  2.6275582 ],\n",
       "       [ -4.1425376 ],\n",
       "       [ -8.209433  ],\n",
       "       [-46.33242   ],\n",
       "       [-13.186731  ],\n",
       "       [-46.559914  ],\n",
       "       [-14.423948  ],\n",
       "       [-11.954599  ],\n",
       "       [ -6.548349  ],\n",
       "       [-20.98937   ],\n",
       "       [-43.63129   ],\n",
       "       [-43.856377  ],\n",
       "       [ -3.3977714 ],\n",
       "       [-40.002396  ],\n",
       "       [-38.01322   ],\n",
       "       [-13.034712  ],\n",
       "       [  5.133372  ],\n",
       "       [-57.685776  ],\n",
       "       [ -1.5658737 ],\n",
       "       [-34.57497   ],\n",
       "       [-12.189146  ],\n",
       "       [-29.044123  ],\n",
       "       [ -8.435602  ],\n",
       "       [ -5.9060683 ],\n",
       "       [-39.161034  ],\n",
       "       [  6.5103173 ],\n",
       "       [  3.2081475 ],\n",
       "       [-32.611668  ],\n",
       "       [  3.7958436 ],\n",
       "       [ 10.678658  ],\n",
       "       [ -1.5126663 ],\n",
       "       [-24.098246  ],\n",
       "       [-28.153078  ],\n",
       "       [-10.764357  ],\n",
       "       [-17.608385  ],\n",
       "       [-39.674133  ],\n",
       "       [-23.9512    ],\n",
       "       [  1.5416664 ],\n",
       "       [-21.733248  ],\n",
       "       [-19.90724   ],\n",
       "       [-53.103874  ],\n",
       "       [-16.30805   ],\n",
       "       [  0.44710386],\n",
       "       [  4.7703843 ],\n",
       "       [-29.641771  ],\n",
       "       [ -4.957956  ],\n",
       "       [-62.437706  ],\n",
       "       [-44.249187  ],\n",
       "       [ 10.274672  ],\n",
       "       [-53.97055   ],\n",
       "       [-27.23535   ],\n",
       "       [-20.673437  ],\n",
       "       [  8.1887    ],\n",
       "       [-58.843735  ],\n",
       "       [ -8.035505  ],\n",
       "       [-23.762934  ],\n",
       "       [-21.656034  ],\n",
       "       [-18.579794  ],\n",
       "       [ -0.30193865],\n",
       "       [-29.164145  ],\n",
       "       [ -2.6411338 ],\n",
       "       [-39.754353  ],\n",
       "       [-25.030016  ],\n",
       "       [  4.1873956 ],\n",
       "       [ -0.43158114],\n",
       "       [-18.349173  ],\n",
       "       [-48.129128  ],\n",
       "       [-15.291765  ],\n",
       "       [-11.119848  ],\n",
       "       [-22.607473  ],\n",
       "       [-30.895334  ],\n",
       "       [  6.461619  ],\n",
       "       [ -7.1851707 ],\n",
       "       [-21.714388  ],\n",
       "       [-15.534365  ],\n",
       "       [  0.11648405],\n",
       "       [-23.479406  ],\n",
       "       [-18.912592  ],\n",
       "       [  2.7277932 ],\n",
       "       [-28.445875  ],\n",
       "       [  0.58064497],\n",
       "       [-39.565384  ],\n",
       "       [  8.498163  ],\n",
       "       [-25.76406   ],\n",
       "       [-39.3816    ],\n",
       "       [-25.396112  ],\n",
       "       [  9.164114  ],\n",
       "       [ -5.7145514 ],\n",
       "       [-19.669909  ],\n",
       "       [-35.892937  ],\n",
       "       [-15.418055  ],\n",
       "       [ -2.28301   ],\n",
       "       [ -3.6840148 ],\n",
       "       [-16.858072  ],\n",
       "       [-41.179844  ],\n",
       "       [  7.5403047 ],\n",
       "       [-11.092619  ],\n",
       "       [ -4.1008277 ],\n",
       "       [-14.176073  ],\n",
       "       [  7.090861  ],\n",
       "       [-40.387028  ],\n",
       "       [  8.564764  ],\n",
       "       [-32.780968  ],\n",
       "       [ -0.93877566],\n",
       "       [  7.516318  ],\n",
       "       [-11.508898  ],\n",
       "       [  0.60615766],\n",
       "       [-24.61508   ],\n",
       "       [  9.656111  ],\n",
       "       [ -4.8267264 ],\n",
       "       [  3.267119  ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "eval_regression_data(raw_dataset, 'MPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744cd7c-1f08-4746-81fe-312a3550b2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
